{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a120589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Repeat 1/3\n",
      "\n",
      "🔄 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.6442\n",
      "MLP: 0.6358\n",
      "RF: 0.6640\n",
      "XGBT: 0.6841\n",
      "\n",
      "🔬 JSD: 0.5872 | WD: 0.0711\n",
      "\n",
      "🔄 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.6456\n",
      "MLP: 0.6537\n",
      "RF: 0.6668\n",
      "XGBT: 0.6781\n",
      "\n",
      "🔬 JSD: 0.6349 | WD: 0.0718\n",
      "\n",
      "🔁 Repeat 2/3\n",
      "\n",
      "🔄 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.6345\n",
      "MLP: 0.6171\n",
      "RF: 0.6585\n",
      "XGBT: 0.6822\n",
      "\n",
      "🔬 JSD: 0.6488 | WD: 0.0724\n",
      "\n",
      "🔄 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.6347\n",
      "MLP: 0.6228\n",
      "RF: 0.6745\n",
      "XGBT: 0.6940\n",
      "\n",
      "🔬 JSD: 0.6113 | WD: 0.0725\n",
      "\n",
      "🔁 Repeat 3/3\n",
      "\n",
      "🔄 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.6474\n",
      "MLP: 0.6527\n",
      "RF: 0.6837\n",
      "XGBT: 0.7049\n",
      "\n",
      "🔬 JSD: 0.6350 | WD: 0.0729\n",
      "\n",
      "🔄 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.6411\n",
      "MLP: 0.5909\n",
      "RF: 0.6912\n",
      "XGBT: 0.6981\n",
      "\n",
      "🔬 JSD: 0.6352 | WD: 0.0728\n",
      "\n",
      "📈 FINAL AVERAGE RESULTS ACROSS 3x2 CV:\n",
      "LogReg TSTR Accuracy: 0.6412\n",
      "MLP TSTR Accuracy: 0.6289\n",
      "RF TSTR Accuracy: 0.6731\n",
      "XGBT TSTR Accuracy: 0.6902\n",
      "\n",
      "JSD: 0.6254\n",
      "Wasserstein Distance: 0.0723\n",
      "\n",
      "🚀 Training generator on full dataset for final synthetic data...\n",
      "\n",
      "💾 Generating final synthetic dataset (50% size of original)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'synthetic_connect4_half_improved.csv'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full updated CW-GAN-GP code with improved architecture and training strategy for connect-4.arff\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import entropy, wasserstein_distance\n",
    "from scipy.io import arff\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# ===== Load + Preprocess ARFF =====\n",
    "data, meta = arff.loadarff(\"connect-4.arff\")\n",
    "df = pd.DataFrame(data)\n",
    "for col in df.select_dtypes([object]).columns:\n",
    "    df[col] = df[col].str.decode(\"utf-8\").str.replace(r\"[\\\\'\\\"]\", \"\", regex=True)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "encoders = {}\n",
    "for col in df.columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    encoders[col] = le\n",
    "\n",
    "X = df.drop(columns=[\"class\"])\n",
    "y = df[\"class\"]\n",
    "input_dim = X.shape[1]\n",
    "num_classes = y.nunique()\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# ===== Metrics =====\n",
    "def compute_jsd(p, q):\n",
    "    p, q = np.array(p) + 1e-10, np.array(q) + 1e-10\n",
    "    p, q = p / p.sum(), q / q.sum()\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * (entropy(p, m) + entropy(q, m))\n",
    "\n",
    "def evaluate_jsd_wd(real_df, synth_df):\n",
    "    jsd_scores, wd_scores = [], []\n",
    "    for col in real_df.columns:\n",
    "        real, synth = real_df[col].values, synth_df[col].values\n",
    "        jsd = compute_jsd(np.histogram(real, bins=20)[0], np.histogram(synth, bins=20)[0])\n",
    "        wd = wasserstein_distance(real, synth)\n",
    "        jsd_scores.append(jsd)\n",
    "        wd_scores.append(wd)\n",
    "    return np.mean(jsd_scores), np.mean(wd_scores)\n",
    "\n",
    "# ===== Improved CW-GAN Models =====\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, 32)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(32 + 32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "    def forward(self, z, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat((z, c), dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, 32)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim + 32, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        d_in = torch.cat((x, c), dim=1)\n",
    "        return self.model(d_in)\n",
    "\n",
    "def compute_gp(critic, real_samples, fake_samples, labels, device):\n",
    "    alpha = torch.rand(real_samples.size(0), 1).to(device)\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolates = critic(interpolates, labels)\n",
    "    fake = torch.ones_like(d_interpolates)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates, inputs=interpolates, grad_outputs=fake,\n",
    "        create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    return ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "# ===== Train + Evaluate =====\n",
    "skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "all_results = defaultdict(list)\n",
    "\n",
    "for repeat in range(3):\n",
    "    print(f\"\\n🔁 Repeat {repeat+1}/3\")\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"\\n🔄 Fold {fold+1}/2\")\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        X_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "        loader = DataLoader(TensorDataset(X_tensor, y_tensor), batch_size=128, shuffle=True)\n",
    "\n",
    "        generator = Generator().to(device)\n",
    "        critic = Critic().to(device)\n",
    "        opt_G = torch.optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.9))\n",
    "        opt_C = torch.optim.Adam(critic.parameters(), lr=2e-4, betas=(0.5, 0.9))\n",
    "\n",
    "        for epoch in range(200):\n",
    "            for i, (real_samples, labels) in enumerate(loader):\n",
    "                real_samples, labels = real_samples.to(device), labels.to(device)\n",
    "                for _ in range(5):  # more critic updates\n",
    "                    z = torch.randn(real_samples.size(0), 32).to(device)\n",
    "                    fake_samples = generator(z, labels)\n",
    "                    real_validity = critic(real_samples, labels)\n",
    "                    fake_validity = critic(fake_samples.detach(), labels)\n",
    "                    gp = compute_gp(critic, real_samples, fake_samples, labels, device)\n",
    "                    c_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + 10 * gp\n",
    "                    opt_C.zero_grad()\n",
    "                    c_loss.backward()\n",
    "                    opt_C.step()\n",
    "\n",
    "                if i % 5 == 0:\n",
    "                    z = torch.randn(real_samples.size(0), 32).to(device)\n",
    "                    opt_G.zero_grad()\n",
    "                    gen_samples = generator(z, labels)\n",
    "                    g_loss = -torch.mean(critic(gen_samples, labels))\n",
    "                    g_loss.backward()\n",
    "                    opt_G.step()\n",
    "\n",
    "        synth_size = len(X_train) // 2\n",
    "        real_dist = y_train.value_counts(normalize=True).sort_index().values\n",
    "        synth_labels = torch.tensor(np.random.choice(num_classes, size=synth_size, p=real_dist), dtype=torch.long).to(device)\n",
    "        z = torch.randn(synth_size, 32).to(device)\n",
    "        gen_data = generator(z, synth_labels).detach().cpu().numpy()\n",
    "        synth_df = pd.DataFrame(gen_data, columns=X.columns)\n",
    "        synth_df[\"class\"] = synth_labels.cpu().numpy()\n",
    "\n",
    "        print(\"\\n📊 TSTR Accuracy:\")\n",
    "        models = {\n",
    "            \"LogReg\": LogisticRegression(max_iter=300),\n",
    "            \"MLP\": MLPClassifier(max_iter=300),\n",
    "            \"RF\": RandomForestClassifier(),\n",
    "            \"XGBT\": XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "        }\n",
    "        for name, model in models.items():\n",
    "            model.fit(synth_df.drop(columns=[\"class\"]), synth_df[\"class\"])\n",
    "            y_pred = model.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            all_results[f\"{name}_acc\"].append(acc)\n",
    "            print(f\"{name}: {acc:.4f}\")\n",
    "\n",
    "        jsd, wd = evaluate_jsd_wd(X_train, synth_df.drop(columns=[\"class\"]))\n",
    "        all_results[\"jsd\"].append(jsd)\n",
    "        all_results[\"wd\"].append(wd)\n",
    "        print(f\"\\n🔬 JSD: {jsd:.4f} | WD: {wd:.4f}\")\n",
    "\n",
    "# ===== Summary =====\n",
    "print(\"\\n📈 FINAL AVERAGE RESULTS ACROSS 3x2 CV:\")\n",
    "for name in [\"LogReg\", \"MLP\", \"RF\", \"XGBT\"]:\n",
    "    avg = np.mean(all_results[f\"{name}_acc\"])\n",
    "    print(f\"{name} TSTR Accuracy: {avg:.4f}\")\n",
    "print(f\"\\nJSD: {np.mean(all_results['jsd']):.4f}\")\n",
    "print(f\"Wasserstein Distance: {np.mean(all_results['wd']):.4f}\")\n",
    "\n",
    "# ===== Final Generator Training =====\n",
    "print(\"\\n🚀 Training generator on full dataset for final synthetic data...\")\n",
    "X_tensor_full = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_tensor_full = torch.tensor(y.values, dtype=torch.long)\n",
    "loader_full = DataLoader(TensorDataset(X_tensor_full, y_tensor_full), batch_size=128, shuffle=True)\n",
    "\n",
    "generator = Generator().to(device)\n",
    "critic = Critic().to(device)\n",
    "opt_G = torch.optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.9))\n",
    "opt_C = torch.optim.Adam(critic.parameters(), lr=2e-4, betas=(0.5, 0.9))\n",
    "\n",
    "for epoch in range(200):\n",
    "    for i, (real_samples, labels) in enumerate(loader_full):\n",
    "        real_samples, labels = real_samples.to(device), labels.to(device)\n",
    "        for _ in range(5):\n",
    "            z = torch.randn(real_samples.size(0), 32).to(device)\n",
    "            fake_samples = generator(z, labels)\n",
    "            real_validity = critic(real_samples, labels)\n",
    "            fake_validity = critic(fake_samples.detach(), labels)\n",
    "            gp = compute_gp(critic, real_samples, fake_samples, labels, device)\n",
    "            opt_C.zero_grad()\n",
    "            c_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + 10 * gp\n",
    "            c_loss.backward()\n",
    "            opt_C.step()\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            z = torch.randn(real_samples.size(0), 32).to(device)\n",
    "            opt_G.zero_grad()\n",
    "            gen_samples = generator(z, labels)\n",
    "            g_loss = -torch.mean(critic(gen_samples, labels))\n",
    "            g_loss.backward()\n",
    "            opt_G.step()\n",
    "\n",
    "# ===== Generate Final Dataset =====\n",
    "print(\"\\n💾 Generating final synthetic dataset (50% size of original)\")\n",
    "synth_size = len(X) // 2\n",
    "z = torch.randn(synth_size, 32).to(device)\n",
    "real_dist = y.value_counts(normalize=True).sort_index().values\n",
    "synth_labels = torch.tensor(np.random.choice(num_classes, size=synth_size, p=real_dist), dtype=torch.long).to(device)\n",
    "gen_data = generator(z, synth_labels).detach().cpu().numpy()\n",
    "synth_df_final = pd.DataFrame(gen_data, columns=X.columns)\n",
    "synth_df_final[\"class\"] = synth_labels.cpu().numpy()\n",
    "\n",
    "# ===== Postprocess =====\n",
    "for col in X.columns:\n",
    "    synth_df_final[col] = synth_df_final[col].round().astype(int)\n",
    "    synth_df_final[col] = synth_df_final[col].clip(0, df[col].max())\n",
    "synth_df_final[\"class\"] = synth_df_final[\"class\"].clip(0, df[\"class\"].max())\n",
    "\n",
    "# Save\n",
    "final_path = \"synthetic_connect4_half_improved.csv\"\n",
    "synth_df_final.to_csv(final_path, index=False)\n",
    "final_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6ddbff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 FINAL AVERAGE RESULTS ACROSS 3x2 CV:\n",
      "LogReg TSTR Accuracy: 0.6412\n",
      "MLP TSTR Accuracy: 0.6289\n",
      "RF TSTR Accuracy: 0.6731\n",
      "XGBT TSTR Accuracy: 0.6902\n",
      "\n",
      "⭐ Average Accuracy (All Models): 0.6584\n",
      "\n",
      "🔬 Average JSD: 0.6254\n",
      "🔬 Average Wasserstein Distance: 0.0723\n"
     ]
    }
   ],
   "source": [
    "# ===== Final Average Summary =====\n",
    "print(\"\\n📈 FINAL AVERAGE RESULTS ACROSS 3x2 CV:\")\n",
    "avg_accuracies = {}\n",
    "\n",
    "for name in [\"LogReg\", \"MLP\", \"RF\", \"XGBT\"]:\n",
    "    acc_list = all_results[f\"{name}_acc\"]\n",
    "    avg_acc = np.mean(acc_list)\n",
    "    avg_accuracies[name] = avg_acc\n",
    "    print(f\"{name} TSTR Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "# Calculate overall average accuracy across all classifiers\n",
    "overall_avg_acc = np.mean(list(avg_accuracies.values()))\n",
    "print(f\"\\n⭐ Average Accuracy (All Models): {overall_avg_acc:.4f}\")\n",
    "\n",
    "# Print divergence scores\n",
    "avg_jsd = np.mean(all_results['jsd'])\n",
    "avg_wd = np.mean(all_results['wd'])\n",
    "print(f\"\\n🔬 Average JSD: {avg_jsd:.4f}\")\n",
    "print(f\"🔬 Average Wasserstein Distance: {avg_wd:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
