{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNBgO3BBe41d/8loMOCH7i1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3hR9kzgzyWyh","executionInfo":{"status":"ok","timestamp":1743898233405,"user_tz":-600,"elapsed":46446,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}},"outputId":"e61c7c0e-ed16-48b9-9693-21653b3810f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.14.1)\n","Mounted at /content/drive/\n","/content/drive/MyDrive/Colab Notebooks/Katabatic/CrGAN/adult\n","Using device: cuda:0\n"]}],"source":["%pip install xgboost\n","\n","import numpy as np\n","import pandas as pd\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torch.autograd import Variable\n","from torch.autograd import grad as torch_grad\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","from scipy.stats import entropy\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","import time\n","from tqdm import tqdm\n","from google.colab import drive\n","\n","drive.mount('/content/drive/')\n","%cd /content/drive/MyDrive/Colab Notebooks/Katabatic/CrGAN/adult/\n","\n","# Suppress warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Setting random seeds for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","# Check if CUDA is available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"]},{"cell_type":"code","source":["# Load and preprocess data\n","def load_data(train_path, test_path):\n","    # Load data\n","    df_train = pd.read_csv(train_path)\n","    df_test = pd.read_csv(test_path)\n","\n","    # Replace spaces in column names\n","    df_train.columns = [col.strip().replace(' ', '') for col in df_train.columns]\n","    df_test.columns = [col.strip().replace(' ', '') for col in df_test.columns]\n","\n","    # Define categorical and numerical columns\n","    categorical_cols = ['workclass', 'education', 'marital.status', 'occupation',\n","                       'relationship', 'race', 'sex', 'native.country']\n","    numerical_cols = ['age', 'fnlwgt', 'education.num', 'capital.gain',\n","                     'capital.loss', 'hours.per.week']\n","\n","    # Process target variable\n","    df_train['income'] = df_train['income'].map({' <=50K': 0, ' >50K': 1})\n","    df_test['income'] = df_test['income'].map({' <=50K': 0, ' >50K': 1})\n","\n","    # Create preprocessing pipeline\n","    numerical_transformer = Pipeline(steps=[\n","        ('scaler', StandardScaler())\n","    ])\n","\n","    categorical_transformer = Pipeline(steps=[\n","        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n","    ])\n","\n","    preprocessor = ColumnTransformer(\n","        transformers=[\n","            ('num', numerical_transformer, numerical_cols),\n","            ('cat', categorical_transformer, categorical_cols)\n","        ])\n","\n","    # Split features and target\n","    X_train = df_train.drop('income', axis=1)\n","    y_train = df_train['income']\n","    X_test = df_test.drop('income', axis=1)\n","    y_test = df_test['income']\n","\n","    # Fit and transform the data\n","    X_train_transformed = preprocessor.fit_transform(X_train)\n","    X_test_transformed = preprocessor.transform(X_test)\n","\n","    # Get output feature names\n","    cat_feature_names = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_cols)\n","    all_feature_names = list(numerical_cols) + list(cat_feature_names)\n","\n","    print(f\"Training data shape: {X_train_transformed.shape}\")\n","    print(f\"Testing data shape: {X_test_transformed.shape}\")\n","\n","    return X_train_transformed, y_train, X_test_transformed, y_test, preprocessor, all_feature_names\n","\n","# Custom dataset class\n","class AdultDataset(Dataset):\n","    def __init__(self, features, labels=None):\n","        self.features = torch.tensor(features, dtype=torch.float32)\n","        if labels is not None:\n","            self.labels = torch.tensor(labels, dtype=torch.float32).view(-1, 1)\n","        else:\n","            self.labels = None\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, idx):\n","        if self.labels is not None:\n","            return self.features[idx], self.labels[idx]\n","        else:\n","            return self.features[idx]\n","\n","# Generator Network\n","class Generator(nn.Module):\n","    def __init__(self, latent_dim, output_dim):\n","        super(Generator, self).__init__()\n","        self.latent_dim = latent_dim\n","        self.output_dim = output_dim\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(latent_dim, 256),\n","            nn.BatchNorm1d(256),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Linear(256, 512),\n","            nn.BatchNorm1d(512),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Linear(512, 1024),\n","            nn.BatchNorm1d(1024),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Linear(1024, output_dim),\n","            nn.Tanh()  # Output layer - maps to (-1, 1) range\n","        )\n","\n","    def forward(self, z):\n","        return self.model(z)\n","\n","# Critic Network (Discriminator)\n","class Critic(nn.Module):\n","    def __init__(self, input_dim):\n","        super(Critic, self).__init__()\n","        self.input_dim = input_dim\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(input_dim, 512),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(512, 256),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(256, 128),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(128, 1)\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Cramer GAN Implementation\n","class CramerGAN:\n","    def __init__(self, data_dim, latent_dim=100, critic_iterations=5, lambda_gp=10):\n","        self.data_dim = data_dim\n","        self.latent_dim = latent_dim\n","        self.critic_iterations = critic_iterations\n","        self.lambda_gp = lambda_gp\n","\n","        # Initialize networks\n","        self.generator = Generator(latent_dim, data_dim).to(device)\n","        self.critic = Critic(data_dim).to(device)\n","\n","        # Setup optimizers\n","        self.g_optimizer = optim.Adam(self.generator.parameters(), lr=0.0002, betas=(0.5, 0.9))\n","        self.c_optimizer = optim.Adam(self.critic.parameters(), lr=0.0002, betas=(0.5, 0.9))\n","\n","        # Initialize loss tracking\n","        self.g_losses = []\n","        self.c_losses = []\n","\n","    def _critic_train_iteration(self, real_data, batch_size):\n","        # Generate random noise\n","        noise = torch.randn(batch_size, self.latent_dim).to(device)\n","\n","        # Generate fake data\n","        fake_data = self.generator(noise)\n","\n","        # Get critic outputs\n","        critic_real = self.critic(real_data)\n","        critic_fake = self.critic(fake_data)\n","\n","        # Calculate Cramer distance\n","        critic_real2 = self.critic(torch.roll(real_data, shifts=1, dims=0))\n","        critic_fake2 = self.critic(torch.roll(fake_data, shifts=1, dims=0))\n","\n","        # Cramer GAN loss function\n","        c_loss = torch.mean(critic_real - critic_fake) - 0.5 * torch.mean(torch.pow(critic_real - critic_real2, 2)) + 0.5 * torch.mean(torch.pow(critic_fake - critic_fake2, 2))\n","\n","        # Calculate gradient penalty\n","        alpha = torch.rand(batch_size, 1).to(device)\n","        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n","        interpolates.requires_grad_(True)\n","\n","        critic_interpolates = self.critic(interpolates)\n","        gradients = torch_grad(outputs=critic_interpolates, inputs=interpolates,\n","                              grad_outputs=torch.ones_like(critic_interpolates).to(device),\n","                              create_graph=True, retain_graph=True)[0]\n","\n","        gradients = gradients.view(batch_size, -1)\n","        gradient_penalty = self.lambda_gp * ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n","\n","        # Update critic\n","        self.c_optimizer.zero_grad()\n","        c_loss_total = c_loss + gradient_penalty\n","        c_loss_total.backward()\n","        self.c_optimizer.step()\n","\n","        return c_loss_total.item()\n","\n","    def _generator_train_iteration(self, batch_size):\n","        # Generate random noise\n","        noise = torch.randn(batch_size, self.latent_dim).to(device)\n","\n","        # Generate fake data\n","        fake_data = self.generator(noise)\n","\n","        # Calculate critic outputs\n","        critic_fake = self.critic(fake_data)\n","        critic_fake2 = self.critic(torch.roll(fake_data, shifts=1, dims=0))\n","\n","        # Generator loss is negative of critic loss\n","        g_loss = -torch.mean(critic_fake) + 0.5 * torch.mean(torch.pow(critic_fake - critic_fake2, 2))\n","\n","        # Update generator\n","        self.g_optimizer.zero_grad()\n","        g_loss.backward()\n","        self.g_optimizer.step()\n","\n","        return g_loss.item()\n","\n","    def train(self, data_loader, epochs, save_interval=10, verbose=True):\n","        for epoch in range(epochs):\n","            epoch_start_time = time.time()\n","            c_loss_total = 0\n","            g_loss_total = 0\n","            num_batches = 0\n","\n","            for i, (real_data, _) in enumerate(data_loader):\n","                batch_size = real_data.size(0)\n","                real_data = real_data.to(device)\n","\n","                # Train critic\n","                for _ in range(self.critic_iterations):\n","                    c_loss = self._critic_train_iteration(real_data, batch_size)\n","                c_loss_total += c_loss\n","\n","                # Train generator\n","                g_loss = self._generator_train_iteration(batch_size)\n","                g_loss_total += g_loss\n","\n","                num_batches += 1\n","\n","            # Calculate average loss for the epoch\n","            c_loss_avg = c_loss_total / num_batches\n","            g_loss_avg = g_loss_total / num_batches\n","\n","            self.c_losses.append(c_loss_avg)\n","            self.g_losses.append(g_loss_avg)\n","\n","            epoch_time = time.time() - epoch_start_time\n","\n","            if verbose and (epoch % save_interval == 0 or epoch == epochs - 1):\n","                print(f\"Epoch [{epoch+1}/{epochs}] | Critic Loss: {c_loss_avg:.4f} | Generator Loss: {g_loss_avg:.4f} | Time: {epoch_time:.2f}s\")\n","\n","    def generate_samples(self, num_samples):\n","        self.generator.eval()\n","        noise = torch.randn(num_samples, self.latent_dim).to(device)\n","        with torch.no_grad():\n","            generated_data = self.generator(noise).cpu().numpy()\n","        self.generator.train()\n","        return generated_data\n","\n","    def save_model(self, path):\n","        torch.save({\n","            'generator_state_dict': self.generator.state_dict(),\n","            'critic_state_dict': self.critic.state_dict(),\n","            'g_optimizer_state_dict': self.g_optimizer.state_dict(),\n","            'c_optimizer_state_dict': self.c_optimizer.state_dict(),\n","        }, path)\n","\n","    def load_model(self, path):\n","        checkpoint = torch.load(path)\n","        self.generator.load_state_dict(checkpoint['generator_state_dict'])\n","        self.critic.load_state_dict(checkpoint['critic_state_dict'])\n","        self.g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n","        self.c_optimizer.load_state_dict(checkpoint['c_optimizer_state_dict'])\n","\n"],"metadata":{"id":"gOXsxIkuy-dt","executionInfo":{"status":"ok","timestamp":1743898233546,"user_tz":-600,"elapsed":132,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# 1. Machine Learning Utility (TSTR)\n","def evaluate_tstr(real_data, synthetic_data, real_labels, random_state=42):\n","    \"\"\"\n","    Train classifiers on synthetic data and test on real data (TSTR)\n","    Returns accuracy for each classifier\n","    \"\"\"\n","    # Train-test split for real data\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        real_data, real_labels, test_size=0.2, random_state=random_state\n","    )\n","\n","    # Synthetic data (all used for training)\n","    X_synth = synthetic_data\n","\n","    # Ensure proper dimensions for labels\n","    if isinstance(y_train, pd.Series):\n","        y_train = y_train.values\n","\n","    # Create synthetic labels based on real distribution\n","    np.random.seed(random_state)\n","    y_synth = np.random.choice([0, 1], size=len(X_synth), p=[\n","                               1-y_train.mean(), y_train.mean()])\n","\n","    # Define classifiers\n","    classifiers = {\n","        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=random_state),\n","        'MLP': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=random_state),\n","        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=random_state),\n","        'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=random_state)\n","    }\n","\n","    results = {}\n","\n","    for name, clf in classifiers.items():\n","        # Train on synthetic data\n","        clf.fit(X_synth, y_synth)\n","\n","        # Test on real data\n","        y_pred = clf.predict(X_test)\n","\n","        # Calculate metrics\n","        accuracy = accuracy_score(y_test, y_pred)\n","        f1 = f1_score(y_test, y_pred)\n","\n","        results[name] = {\n","            'accuracy': accuracy,\n","            'f1_score': f1\n","        }\n","\n","    return results\n","\n","# 2. Statistical Similarity\n","\n","\n","def jensen_shannon_divergence(p, q):\n","    \"\"\"\n","    Calculate Jensen-Shannon Divergence between distributions p and q\n","    \"\"\"\n","    # Ensure p and q are normalized\n","    p = p / np.sum(p)\n","    q = q / np.sum(q)\n","\n","    m = 0.5 * (p + q)\n","\n","    # Calculate JSD\n","    jsd = 0.5 * (entropy(p, m) + entropy(q, m))\n","\n","    return jsd\n","\n","\n","def wasserstein_distance(p, q):\n","    \"\"\"\n","    Calculate 1D Wasserstein distance (Earth Mover's Distance)\n","    \"\"\"\n","    from scipy.stats import wasserstein_distance\n","\n","    return wasserstein_distance(p, q)\n","\n","\n","def evaluate_statistical_similarity(real_data, synthetic_data, feature_names):\n","    \"\"\"\n","    Calculate statistical similarity metrics between real and synthetic data\n","    \"\"\"\n","    results = {'JSD': {}, 'WD': {}}\n","\n","    # Calculate metrics for each feature\n","    for i in range(real_data.shape[1]):\n","        feature_name = feature_names[i] if i < len(\n","            feature_names) else f\"feature_{i}\"\n","\n","        # Get feature values\n","        real_values = real_data[:, i]\n","        synth_values = synthetic_data[:, i]\n","\n","        # Calculate histogram (discrete distribution)\n","        hist_bins = min(50, len(np.unique(real_values)))\n","\n","        hist_real, bin_edges = np.histogram(\n","            real_values, bins=hist_bins, density=True)\n","        hist_synth, _ = np.histogram(\n","            synth_values, bins=bin_edges, density=True)\n","\n","        # Add a small epsilon to avoid division by zero\n","        epsilon = 1e-10\n","        hist_real = hist_real + epsilon\n","        hist_synth = hist_synth + epsilon\n","\n","        # Calculate JSD\n","        jsd = jensen_shannon_divergence(hist_real, hist_synth)\n","        results['JSD'][feature_name] = jsd\n","\n","        # Calculate Wasserstein Distance\n","        wd = wasserstein_distance(real_values, synth_values)\n","        results['WD'][feature_name] = wd\n","\n","    # Calculate average metrics\n","    results['JSD_avg'] = np.mean(list(results['JSD'].values()))\n","    results['WD_avg'] = np.mean(list(results['WD'].values()))\n","\n","    return results\n","\n","\n","def plot_loss_curves(model):\n","    \"\"\"\n","    Plot the loss curves for the generator and critic\n","    \"\"\"\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(model.g_losses, label='Generator Loss')\n","    plt.plot(model.c_losses, label='Critic Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.title('CramerGAN Training Loss')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig('crgan_loss_curves.png')\n","    plt.close()\n","\n","\n","def plot_feature_distributions(real_data, synthetic_data, feature_names, n_features=5):\n","    \"\"\"\n","    Plot distributions of real vs synthetic data for selected features\n","    \"\"\"\n","    if n_features > len(feature_names):\n","        n_features = len(feature_names)\n","\n","    # Select a subset of features to visualize\n","    selected_indices = np.random.choice(\n","        range(len(feature_names)), size=n_features, replace=False)\n","\n","    plt.figure(figsize=(15, 10))\n","    for i, idx in enumerate(selected_indices):\n","        feature_name = feature_names[idx]\n","\n","        plt.subplot(n_features, 1, i+1)\n","\n","        # Get feature values\n","        real_values = real_data[:, idx]\n","        synth_values = synthetic_data[:, idx]\n","\n","        # Plot histograms\n","        sns.histplot(real_values, kde=True, stat=\"density\",\n","                     label=\"Real\", alpha=0.6, color=\"blue\")\n","        sns.histplot(synth_values, kde=True, stat=\"density\",\n","                     label=\"Synthetic\", alpha=0.6, color=\"red\")\n","\n","        plt.title(f\"Distribution for {feature_name}\")\n","        plt.legend()\n","\n","    plt.tight_layout()\n","    plt.savefig('feature_distributions.png')\n","    plt.close()\n","\n","# Main function\n","\n","\n","def main():\n","    # File paths\n","    train_path = \"data/adult-train.csv\"\n","    test_path = \"data/adult-test.csv\"\n","\n","    # Load and preprocess data\n","    X_train, y_train, X_test, y_test, preprocessor, feature_names = load_data(\n","        train_path, test_path)\n","\n","    # Create dataset and dataloader\n","    train_dataset = AdultDataset(X_train, y_train)\n","    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","\n","    # Initialize and train the model\n","    data_dim = X_train.shape[1]\n","    latent_dim = 100\n","\n","    print(f\"Data dimension: {data_dim}\")\n","    print(f\"Latent dimension: {latent_dim}\")\n","\n","    crgan = CramerGAN(data_dim, latent_dim)\n","\n","    # Train the model\n","    epochs = 300\n","    print(f\"Training CramerGAN for {epochs} epochs...\")\n","    crgan.train(train_loader, epochs, save_interval=10)\n","\n","    # Save the model\n","    crgan.save_model('crgan_model.pt')\n","\n","    # Plot loss curves\n","    plot_loss_curves(crgan)\n","\n","    # Generate synthetic data\n","    num_samples = 1000\n","    print(f\"Generating {num_samples} synthetic samples...\")\n","    synthetic_data = crgan.generate_samples(num_samples)\n","\n","    # Statistical similarity evaluation\n","    print(\"Evaluating statistical similarity...\")\n","    stat_results = evaluate_statistical_similarity(\n","        X_train, synthetic_data, feature_names)\n","\n","    print(\"\\nJensen-Shannon Divergence (average):\", stat_results['JSD_avg'])\n","    print(\"Wasserstein Distance (average):\", stat_results['WD_avg'])\n","\n","    # Machine Learning Utility (TSTR) evaluation\n","    print(\"\\nEvaluating Machine Learning Utility (TSTR)...\")\n","    tstr_results = evaluate_tstr(X_train, synthetic_data, y_train)\n","\n","    print(\"\\nTSTR Results:\")\n","    for clf_name, metrics in tstr_results.items():\n","        print(\n","            f\"{clf_name}: Accuracy = {metrics['accuracy']:.4f}, F1 Score = {metrics['f1_score']:.4f}\")\n","\n","    # Plot feature distributions\n","    plot_feature_distributions(X_train, synthetic_data, feature_names)\n","\n","    print(\"\\nEvaluation complete! Check the output directory for plots and saved model.\")\n","\n"],"metadata":{"id":"NkDUQ1ZEzAG2","executionInfo":{"status":"ok","timestamp":1743898233854,"user_tz":-600,"elapsed":305,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"tDGjV1eezBhC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743902031845,"user_tz":-600,"elapsed":1194717,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}},"outputId":"9fcec759-9b33-465f-b13c-cdae656045eb"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Training data shape: (32561, 108)\n","Testing data shape: (16281, 108)\n","Data dimension: 108\n","Latent dimension: 100\n","Training CramerGAN for 300 epochs...\n","Epoch [1/300] | Critic Loss: -20886.9372 | Generator Loss: -0.7912 | Time: 26.98s\n","Epoch [11/300] | Critic Loss: -107412639743100.4844 | Generator Loss: 7836976108.9255 | Time: 12.70s\n","Epoch [21/300] | Critic Loss: -23363338636110892.0000 | Generator Loss: 1318491068054.5881 | Time: 12.91s\n","Epoch [31/300] | Critic Loss: -578347327770891392.0000 | Generator Loss: 40657577346180.5156 | Time: 12.70s\n","Epoch [41/300] | Critic Loss: -5402991783680845824.0000 | Generator Loss: 384980910632044.4375 | Time: 12.75s\n","Epoch [51/300] | Critic Loss: -33389915447510511616.0000 | Generator Loss: 2716727609663520.0000 | Time: 13.13s\n","Epoch [61/300] | Critic Loss: -141076770875941306368.0000 | Generator Loss: 11323803380350462.0000 | Time: 12.72s\n","Epoch [71/300] | Critic Loss: -473359144968169586688.0000 | Generator Loss: 45362624376342536.0000 | Time: 12.62s\n","Epoch [81/300] | Critic Loss: -1105749923190355787776.0000 | Generator Loss: 104166193040311744.0000 | Time: 12.51s\n","Epoch [91/300] | Critic Loss: -2180264270734713683968.0000 | Generator Loss: 149184793163754080.0000 | Time: 12.50s\n","Epoch [101/300] | Critic Loss: -4252000410207026413568.0000 | Generator Loss: 284327754657177632.0000 | Time: 12.62s\n","Epoch [111/300] | Critic Loss: -7106888314719600377856.0000 | Generator Loss: 547111957736801600.0000 | Time: 12.30s\n","Epoch [121/300] | Critic Loss: -12374211158469060329472.0000 | Generator Loss: 960672885781190272.0000 | Time: 12.54s\n","Epoch [131/300] | Critic Loss: -19063062380331617222656.0000 | Generator Loss: 1519690572765738496.0000 | Time: 12.51s\n","Epoch [141/300] | Critic Loss: -30773759748845349109760.0000 | Generator Loss: 2481337811795957760.0000 | Time: 12.34s\n","Epoch [151/300] | Critic Loss: -46786960654257751064576.0000 | Generator Loss: 4050901496025494016.0000 | Time: 12.54s\n","Epoch [161/300] | Critic Loss: -69027351673540725178368.0000 | Generator Loss: 5582280147152123904.0000 | Time: 12.50s\n","Epoch [171/300] | Critic Loss: -100181656389426700353536.0000 | Generator Loss: 7494265968641781760.0000 | Time: 12.25s\n","Epoch [181/300] | Critic Loss: -142654839840111511732224.0000 | Generator Loss: 12127636212603248640.0000 | Time: 12.43s\n","Epoch [191/300] | Critic Loss: -179736571210872960057344.0000 | Generator Loss: 15633278268851994624.0000 | Time: 12.49s\n","Epoch [201/300] | Critic Loss: -215980772503581328670720.0000 | Generator Loss: 21181521944834211840.0000 | Time: 12.30s\n","Epoch [211/300] | Critic Loss: -240698172785588575404032.0000 | Generator Loss: 17792190859455877120.0000 | Time: 12.44s\n","Epoch [221/300] | Critic Loss: -269336179992408059019264.0000 | Generator Loss: 13902307942023718912.0000 | Time: 12.45s\n","Epoch [231/300] | Critic Loss: -297632550560162939666432.0000 | Generator Loss: 13053558391198330880.0000 | Time: 12.54s\n","Epoch [241/300] | Critic Loss: -329662315288304772186112.0000 | Generator Loss: 13245499610018330624.0000 | Time: 12.56s\n","Epoch [251/300] | Critic Loss: -352364935153919226544128.0000 | Generator Loss: 15297222403088332800.0000 | Time: 12.48s\n","Epoch [261/300] | Critic Loss: -387366459755630696595456.0000 | Generator Loss: 15666066999135367168.0000 | Time: 12.25s\n","Epoch [271/300] | Critic Loss: -423231375482027193139200.0000 | Generator Loss: 17922884018044276736.0000 | Time: 12.48s\n","Epoch [281/300] | Critic Loss: -452265669078834263097344.0000 | Generator Loss: 17586765128705292288.0000 | Time: 12.48s\n","Epoch [291/300] | Critic Loss: -475824163755628643221504.0000 | Generator Loss: 18466519758336516096.0000 | Time: 12.40s\n","Epoch [300/300] | Critic Loss: -513691724198622055628800.0000 | Generator Loss: 14352391028329748480.0000 | Time: 12.46s\n","Generating 1000 synthetic samples...\n","Evaluating statistical similarity...\n","\n","Jensen-Shannon Divergence (average): nan\n","Wasserstein Distance (average): 1.0403740187824302\n","\n","Evaluating Machine Learning Utility (TSTR)...\n","\n","TSTR Results:\n","Logistic Regression: Accuracy = 0.3981, F1 Score = 0.4087\n","MLP: Accuracy = 0.7437, F1 Score = 0.1790\n","Random Forest: Accuracy = 0.6908, F1 Score = 0.1205\n","XGBoost: Accuracy = 0.7588, F1 Score = 0.0000\n","\n","Evaluation complete! Check the output directory for plots and saved model.\n"]}]}]}