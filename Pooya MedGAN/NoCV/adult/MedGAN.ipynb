{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPvIJ2Fct2SYbsSZT/xpRAD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["%pip install xgboost\n","\n","import numpy as np\n","import pandas as pd\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torch.autograd import Variable\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","from scipy.stats import entropy\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","import time\n","from tqdm import tqdm\n","from google.colab import drive\n","\n","drive.mount('/content/drive/')\n","%cd /content/drive/MyDrive/Colab Notebooks/Katabatic/MedGAN/adult/\n","\n","# Suppress warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Setting random seeds for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","# Check if CUDA is available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hubtSm5Ic2uZ","executionInfo":{"status":"ok","timestamp":1743836035146,"user_tz":-660,"elapsed":76230,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}},"outputId":"e41d1690-f26f-404c-f5e4-2d3b3a26e13b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.14.1)\n","Mounted at /content/drive/\n","/content/drive/MyDrive/Colab Notebooks/Katabatic/MedGAN/adult\n","Using device: cpu\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"H2yErrWVb07a","executionInfo":{"status":"ok","timestamp":1743836035447,"user_tz":-660,"elapsed":281,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}}},"outputs":[],"source":["# Load and preprocess data\n","def load_data(train_path, test_path):\n","    # Load data\n","    df_train = pd.read_csv(train_path)\n","    df_test = pd.read_csv(test_path)\n","\n","    # Replace spaces in column names\n","    df_train.columns = [col.strip().replace(' ', '') for col in df_train.columns]\n","    df_test.columns = [col.strip().replace(' ', '') for col in df_test.columns]\n","\n","    # Define categorical and numerical columns\n","    categorical_cols = ['workclass', 'education', 'marital.status', 'occupation',\n","                       'relationship', 'race', 'sex', 'native.country']\n","    numerical_cols = ['age', 'fnlwgt', 'education.num', 'capital.gain',\n","                     'capital.loss', 'hours.per.week']\n","\n","    # Process target variable\n","    df_train['income'] = df_train['income'].map({' <=50K': 0, ' >50K': 1})\n","    df_test['income'] = df_test['income'].map({' <=50K': 0, ' >50K': 1})\n","\n","    # Create preprocessing pipeline\n","    numerical_transformer = Pipeline(steps=[\n","        ('scaler', StandardScaler())\n","    ])\n","\n","    categorical_transformer = Pipeline(steps=[\n","        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n","    ])\n","\n","    preprocessor = ColumnTransformer(\n","        transformers=[\n","            ('num', numerical_transformer, numerical_cols),\n","            ('cat', categorical_transformer, categorical_cols)\n","        ])\n","\n","    # Split features and target\n","    X_train = df_train.drop('income', axis=1)\n","    y_train = df_train['income']\n","    X_test = df_test.drop('income', axis=1)\n","    y_test = df_test['income']\n","\n","    # Fit and transform the data\n","    X_train_transformed = preprocessor.fit_transform(X_train)\n","    X_test_transformed = preprocessor.transform(X_test)\n","\n","    # Get output feature names\n","    cat_feature_names = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_cols)\n","    all_feature_names = list(numerical_cols) + list(cat_feature_names)\n","\n","    print(f\"Training data shape: {X_train_transformed.shape}\")\n","    print(f\"Testing data shape: {X_test_transformed.shape}\")\n","\n","    return X_train_transformed, y_train, X_test_transformed, y_test, preprocessor, all_feature_names\n","\n","# Custom dataset class\n","class AdultDataset(Dataset):\n","    def __init__(self, features, labels=None):\n","        self.features = torch.tensor(features, dtype=torch.float32)\n","        if labels is not None:\n","            self.labels = torch.tensor(labels, dtype=torch.float32).view(-1, 1)\n","        else:\n","            self.labels = None\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, idx):\n","        if self.labels is not None:\n","            return self.features[idx], self.labels[idx]\n","        else:\n","            return self.features[idx]\n","\n","# Autoencoder for MedGAN\n","class Autoencoder(nn.Module):\n","    def __init__(self, input_dim, hidden_dim=128, latent_dim=64):\n","        super(Autoencoder, self).__init__()\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.latent_dim = latent_dim\n","\n","        # Encoder\n","        self.encoder = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, latent_dim),\n","            nn.BatchNorm1d(latent_dim),\n","            nn.ReLU()\n","        )\n","\n","        # Decoder\n","        self.decoder = nn.Sequential(\n","            nn.Linear(latent_dim, hidden_dim),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, input_dim),\n","            nn.Sigmoid()  # Output activation for normalized data\n","        )\n","\n","    def encode(self, x):\n","        return self.encoder(x)\n","\n","    def decode(self, z):\n","        return self.decoder(z)\n","\n","    def forward(self, x):\n","        z = self.encode(x)\n","        return self.decode(z)\n","\n","# Generator for MedGAN\n","class Generator(nn.Module):\n","    def __init__(self, latent_dim, hidden_dim=128, output_dim=64):\n","        super(Generator, self).__init__()\n","        self.latent_dim = latent_dim\n","        self.hidden_dim = hidden_dim\n","        self.output_dim = output_dim\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(latent_dim, hidden_dim),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.ReLU(),\n","\n","            nn.Linear(hidden_dim, hidden_dim*2),\n","            nn.BatchNorm1d(hidden_dim*2),\n","            nn.ReLU(),\n","\n","            nn.Linear(hidden_dim*2, output_dim),\n","            nn.Tanh()  # Output in [-1,1] range\n","        )\n","\n","    def forward(self, z):\n","        return self.model(z)\n","\n","# Discriminator for MedGAN\n","class Discriminator(nn.Module):\n","    def __init__(self, input_dim, hidden_dim=128):\n","        super(Discriminator, self).__init__()\n","        self.input_dim = input_dim\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(hidden_dim, hidden_dim//2),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(hidden_dim//2, 1),\n","            nn.Sigmoid()  # Binary classification output\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# MedGAN Implementation\n","class MedGAN:\n","    def __init__(self, input_dim, hidden_dim=128, latent_dim=64, z_dim=128,\n","                 ae_pretrain_epochs=100, discriminator_steps=1, lambda_rec=0.2):\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.latent_dim = latent_dim  # Autoencoder latent dimension\n","        self.z_dim = z_dim  # Generator input noise dimension\n","        self.ae_pretrain_epochs = ae_pretrain_epochs\n","        self.discriminator_steps = discriminator_steps\n","        self.lambda_rec = lambda_rec  # Weight for reconstruction loss\n","\n","        # Initialize networks\n","        self.autoencoder = Autoencoder(input_dim, hidden_dim, latent_dim).to(device)\n","        self.generator = Generator(z_dim, hidden_dim, latent_dim).to(device)\n","        self.discriminator = Discriminator(input_dim).to(device)\n","\n","        # Setup optimizers\n","        self.ae_optimizer = optim.Adam(self.autoencoder.parameters(), lr=0.001)\n","        self.g_optimizer = optim.Adam(self.generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","        self.d_optimizer = optim.Adam(self.discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","        # Loss functions\n","        self.criterion_reconstruct = nn.MSELoss()\n","        self.criterion_gan = nn.BCELoss()\n","\n","        # Initialize loss tracking\n","        self.ae_losses = []\n","        self.g_losses = []\n","        self.d_losses = []\n","\n","    def pretrain_autoencoder(self, data_loader, epochs=None):\n","        \"\"\"Pretrain the autoencoder\"\"\"\n","        if epochs is None:\n","            epochs = self.ae_pretrain_epochs\n","\n","        print(f\"Pretraining autoencoder for {epochs} epochs...\")\n","        self.autoencoder.train()\n","\n","        for epoch in range(epochs):\n","            epoch_loss = 0\n","            for batch_data, _ in data_loader:\n","                batch_data = batch_data.to(device)\n","\n","                # Forward pass\n","                reconstructed = self.autoencoder(batch_data)\n","                loss = self.criterion_reconstruct(reconstructed, batch_data)\n","\n","                # Backward pass and optimize\n","                self.ae_optimizer.zero_grad()\n","                loss.backward()\n","                self.ae_optimizer.step()\n","\n","                epoch_loss += loss.item()\n","\n","            # Record average epoch loss\n","            avg_loss = epoch_loss / len(data_loader)\n","            self.ae_losses.append(avg_loss)\n","\n","            if (epoch+1) % 10 == 0:\n","                print(f\"Epoch [{epoch+1}/{epochs}], Reconstruction Loss: {avg_loss:.6f}\")\n","\n","    def train_gan(self, data_loader, epochs, save_interval=10):\n","        \"\"\"Train the GAN after autoencoder pretraining\"\"\"\n","        print(f\"Training MedGAN for {epochs} epochs...\")\n","\n","        # Set networks to training mode\n","        self.autoencoder.eval()  # Freeze autoencoder after pretraining\n","        self.generator.train()\n","        self.discriminator.train()\n","\n","        # Initialize labels for real and fake data\n","        real_label = 1.0\n","        fake_label = 0.0\n","\n","        for epoch in range(epochs):\n","            d_loss_sum = 0\n","            g_loss_sum = 0\n","            batch_count = 0\n","\n","            for batch_data, _ in data_loader:\n","                batch_size = batch_data.size(0)\n","                batch_data = batch_data.to(device)\n","\n","                # -----------------------\n","                # Train Discriminator\n","                # -----------------------\n","                for _ in range(self.discriminator_steps):\n","                    self.d_optimizer.zero_grad()\n","\n","                    # Real data\n","                    output_real = self.discriminator(batch_data)\n","                    labels_real = torch.full((batch_size, 1), real_label, device=device)\n","                    d_loss_real = self.criterion_gan(output_real, labels_real)\n","\n","                    # Fake data: Generator -> Decoder -> Discriminator\n","                    z = torch.randn(batch_size, self.z_dim, device=device)\n","                    latent_fake = self.generator(z)\n","                    fake_data = self.autoencoder.decode(latent_fake)\n","                    output_fake = self.discriminator(fake_data.detach())\n","                    labels_fake = torch.full((batch_size, 1), fake_label, device=device)\n","                    d_loss_fake = self.criterion_gan(output_fake, labels_fake)\n","\n","                    # Total discriminator loss\n","                    d_loss = d_loss_real + d_loss_fake\n","                    d_loss.backward()\n","                    self.d_optimizer.step()\n","\n","                # -----------------------\n","                # Train Generator\n","                # -----------------------\n","                self.g_optimizer.zero_grad()\n","\n","                # Generate fake data\n","                z = torch.randn(batch_size, self.z_dim, device=device)\n","                latent_fake = self.generator(z)\n","                fake_data = self.autoencoder.decode(latent_fake)\n","\n","                # Discriminator on fake data\n","                output_fake = self.discriminator(fake_data)\n","                labels_real = torch.full((batch_size, 1), real_label, device=device)\n","\n","                # Adversarial loss\n","                g_loss_adv = self.criterion_gan(output_fake, labels_real)\n","\n","                # Optional: Add reconstruction loss\n","                if self.lambda_rec > 0:\n","                    # Reconstruct random real samples through AE\n","                    real_latent = self.autoencoder.encode(batch_data)\n","                    real_reconstructed = self.autoencoder.decode(real_latent)\n","                    rec_loss = self.criterion_reconstruct(real_reconstructed, batch_data)\n","                    g_loss = g_loss_adv + self.lambda_rec * rec_loss\n","                else:\n","                    g_loss = g_loss_adv\n","\n","                g_loss.backward()\n","                self.g_optimizer.step()\n","\n","                # Record statistics\n","                d_loss_sum += d_loss.item()\n","                g_loss_sum += g_loss.item()\n","                batch_count += 1\n","\n","            # Calculate average losses\n","            avg_d_loss = d_loss_sum / batch_count\n","            avg_g_loss = g_loss_sum / batch_count\n","\n","            self.d_losses.append(avg_d_loss)\n","            self.g_losses.append(avg_g_loss)\n","\n","            if (epoch+1) % save_interval == 0 or epoch == epochs-1:\n","                print(f\"Epoch [{epoch+1}/{epochs}], D Loss: {avg_d_loss:.4f}, G Loss: {avg_g_loss:.4f}\")\n","\n","    def generate_samples(self, num_samples):\n","        \"\"\"Generate synthetic samples\"\"\"\n","        self.generator.eval()\n","        self.autoencoder.eval()\n","\n","        with torch.no_grad():\n","            # Generate random noise\n","            z = torch.randn(num_samples, self.z_dim).to(device)\n","\n","            # Generate latent representations\n","            latent_fake = self.generator(z)\n","\n","            # Decode to data space\n","            fake_data = self.autoencoder.decode(latent_fake).cpu().numpy()\n","\n","        return fake_data\n","\n","    def save_model(self, path):\n","        \"\"\"Save the model states\"\"\"\n","        torch.save({\n","            'autoencoder_state_dict': self.autoencoder.state_dict(),\n","            'generator_state_dict': self.generator.state_dict(),\n","            'discriminator_state_dict': self.discriminator.state_dict(),\n","            'ae_optimizer_state_dict': self.ae_optimizer.state_dict(),\n","            'g_optimizer_state_dict': self.g_optimizer.state_dict(),\n","            'd_optimizer_state_dict': self.d_optimizer.state_dict(),\n","        }, path)\n","\n","    def load_model(self, path):\n","        \"\"\"Load model states\"\"\"\n","        checkpoint = torch.load(path)\n","        self.autoencoder.load_state_dict(checkpoint['autoencoder_state_dict'])\n","        self.generator.load_state_dict(checkpoint['generator_state_dict'])\n","        self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n","        self.ae_optimizer.load_state_dict(checkpoint['ae_optimizer_state_dict'])\n","        self.g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n","        self.d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n"]},{"cell_type":"code","source":["# 1. Machine Learning Utility (TSTR)\n","def evaluate_tstr(real_data, synthetic_data, real_labels, random_state=42):\n","    \"\"\"\n","    Train classifiers on synthetic data and test on real data (TSTR)\n","    Returns accuracy for each classifier\n","    \"\"\"\n","    # Train-test split for real data\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        real_data, real_labels, test_size=0.2, random_state=random_state\n","    )\n","\n","    # Synthetic data (all used for training)\n","    X_synth = synthetic_data\n","\n","    # Ensure proper dimensions for labels\n","    if isinstance(y_train, pd.Series):\n","        y_train = y_train.values\n","\n","    # Create synthetic labels based on real distribution\n","    np.random.seed(random_state)\n","    y_synth = np.random.choice([0, 1], size=len(X_synth), p=[\n","                               1-y_train.mean(), y_train.mean()])\n","\n","    # Define classifiers\n","    classifiers = {\n","        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=random_state),\n","        'MLP': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=random_state),\n","        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=random_state),\n","        'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=random_state)\n","    }\n","\n","    results = {}\n","\n","    for name, clf in classifiers.items():\n","        # Train on synthetic data\n","        clf.fit(X_synth, y_synth)\n","\n","        # Test on real data\n","        y_pred = clf.predict(X_test)\n","\n","        # Calculate metrics\n","        accuracy = accuracy_score(y_test, y_pred)\n","        f1 = f1_score(y_test, y_pred)\n","\n","        results[name] = {\n","            'accuracy': accuracy,\n","            'f1_score': f1\n","        }\n","\n","    return results\n","\n","# 2. Statistical Similarity\n","def jensen_shannon_divergence(p, q):\n","    \"\"\"\n","    Calculate Jensen-Shannon Divergence between distributions p and q\n","    \"\"\"\n","    # Ensure p and q are normalized\n","    p = p / np.sum(p)\n","    q = q / np.sum(q)\n","\n","    m = 0.5 * (p + q)\n","\n","    # Calculate JSD\n","    jsd = 0.5 * (entropy(p, m) + entropy(q, m))\n","\n","    return jsd\n","\n","\n","def wasserstein_distance(p, q):\n","    \"\"\"\n","    Calculate 1D Wasserstein distance (Earth Mover's Distance)\n","    \"\"\"\n","    from scipy.stats import wasserstein_distance\n","\n","    return wasserstein_distance(p, q)\n","\n","\n","def evaluate_statistical_similarity(real_data, synthetic_data, feature_names):\n","    \"\"\"\n","    Calculate statistical similarity metrics between real and synthetic data\n","    \"\"\"\n","    results = {'JSD': {}, 'WD': {}}\n","\n","    # Calculate metrics for each feature\n","    for i in range(real_data.shape[1]):\n","        feature_name = feature_names[i] if i < len(\n","            feature_names) else f\"feature_{i}\"\n","\n","        # Get feature values\n","        real_values = real_data[:, i]\n","        synth_values = synthetic_data[:, i]\n","\n","        # Calculate histogram (discrete distribution)\n","        hist_bins = min(50, len(np.unique(real_values)))\n","\n","        hist_real, bin_edges = np.histogram(\n","            real_values, bins=hist_bins, density=True)\n","        hist_synth, _ = np.histogram(\n","            synth_values, bins=bin_edges, density=True)\n","\n","        # Add a small epsilon to avoid division by zero\n","        epsilon = 1e-10\n","        hist_real = hist_real + epsilon\n","        hist_synth = hist_synth + epsilon\n","\n","        # Calculate JSD\n","        jsd = jensen_shannon_divergence(hist_real, hist_synth)\n","        results['JSD'][feature_name] = jsd\n","\n","        # Calculate Wasserstein Distance\n","        wd = wasserstein_distance(real_values, synth_values)\n","        results['WD'][feature_name] = wd\n","\n","    # Calculate average metrics\n","    results['JSD_avg'] = np.mean(list(results['JSD'].values()))\n","    results['WD_avg'] = np.mean(list(results['WD'].values()))\n","\n","    return results\n","\n","\n","def plot_loss_curves(model):\n","    \"\"\"\n","    Plot the loss curves for the autoencoder, generator and discriminator\n","    \"\"\"\n","    plt.figure(figsize=(10, 8))\n","\n","    # Plot autoencoder pretraining loss\n","    plt.subplot(2, 1, 1)\n","    plt.plot(model.ae_losses, label='Autoencoder Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.title('MedGAN Autoencoder Pretraining Loss')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    # Plot GAN training loss\n","    plt.subplot(2, 1, 2)\n","    plt.plot(model.g_losses, label='Generator Loss')\n","    plt.plot(model.d_losses, label='Discriminator Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.title('MedGAN Training Loss')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    plt.tight_layout()\n","    plt.savefig('medgan_loss_curves.png')\n","    plt.close()\n","\n","\n","def plot_feature_distributions(real_data, synthetic_data, feature_names, n_features=5):\n","    \"\"\"\n","    Plot distributions of real vs synthetic data for selected features\n","    \"\"\"\n","    if n_features > len(feature_names):\n","        n_features = len(feature_names)\n","\n","    # Select a subset of features to visualize\n","    selected_indices = np.random.choice(\n","        range(len(feature_names)), size=n_features, replace=False)\n","\n","    plt.figure(figsize=(15, 10))\n","    for i, idx in enumerate(selected_indices):\n","        feature_name = feature_names[idx]\n","\n","        plt.subplot(n_features, 1, i+1)\n","\n","        # Get feature values\n","        real_values = real_data[:, idx]\n","        synth_values = synthetic_data[:, idx]\n","\n","        # Plot histograms\n","        sns.histplot(real_values, kde=True, stat=\"density\",\n","                     label=\"Real\", alpha=0.6, color=\"blue\")\n","        sns.histplot(synth_values, kde=True, stat=\"density\",\n","                     label=\"Synthetic\", alpha=0.6, color=\"red\")\n","\n","        plt.title(f\"Distribution for {feature_name}\")\n","        plt.legend()\n","\n","    plt.tight_layout()\n","    plt.savefig('feature_distributions.png')\n","    plt.close()\n","\n","# Main function\n","def main():\n","    # File paths\n","    train_path = \"data/adult-train.csv\"\n","    test_path = \"data/adult-test.csv\"\n","\n","    # Load and preprocess data\n","    X_train, y_train, X_test, y_test, preprocessor, feature_names = load_data(\n","        train_path, test_path)\n","\n","    # Create dataset and dataloader\n","    train_dataset = AdultDataset(X_train, y_train)\n","    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","\n","    # Initialize model\n","    data_dim = X_train.shape[1]\n","    hidden_dim = 128\n","    latent_dim = 64\n","    z_dim = 128\n","\n","    print(f\"Data dimension: {data_dim}\")\n","    print(f\"Autoencoder latent dimension: {latent_dim}\")\n","    print(f\"Generator input dimension: {z_dim}\")\n","\n","    # Initialize MedGAN\n","    medgan = MedGAN(\n","        input_dim=data_dim,\n","        hidden_dim=hidden_dim,\n","        latent_dim=latent_dim,\n","        z_dim=z_dim,\n","        ae_pretrain_epochs=50,  # Reduced for demonstration\n","        discriminator_steps=1,\n","        lambda_rec=0.2\n","    )\n","\n","    # 1. Pretrain autoencoder\n","    medgan.pretrain_autoencoder(train_loader)\n","\n","    # 2. Train GAN\n","    print(\"\\nTraining MedGAN...\")\n","    gan_epochs = 300\n","    medgan.train_gan(train_loader, gan_epochs, save_interval=10)\n","\n","    # Save the model\n","    medgan.save_model('medgan_model.pt')\n","\n","    # Plot loss curves\n","    plot_loss_curves(medgan)\n","\n","    # Generate synthetic data\n","    num_samples = 1000\n","    print(f\"Generating {num_samples} synthetic samples...\")\n","    synthetic_data = medgan.generate_samples(num_samples)\n","\n","    # Statistical similarity evaluation\n","    print(\"Evaluating statistical similarity...\")\n","    stat_results = evaluate_statistical_similarity(\n","        X_train, synthetic_data, feature_names)\n","\n","    print(\"\\nJensen-Shannon Divergence (average):\", stat_results['JSD_avg'])\n","    print(\"Wasserstein Distance (average):\", stat_results['WD_avg'])\n","\n","    # Machine Learning Utility (TSTR) evaluation\n","    print(\"\\nEvaluating Machine Learning Utility (TSTR)...\")\n","    tstr_results = evaluate_tstr(X_train, synthetic_data, y_train)\n","\n","    print(\"\\nTSTR Results:\")\n","    for clf_name, metrics in tstr_results.items():\n","        print(\n","            f\"{clf_name}: Accuracy = {metrics['accuracy']:.4f}, F1 Score = {metrics['f1_score']:.4f}\")\n","\n","    # Plot feature distributions\n","    plot_feature_distributions(X_train, synthetic_data, feature_names)\n","\n","    print(\"\\nEvaluation complete! Check the output directory for plots and saved model.\")"],"metadata":{"id":"0JszRDx0dhyI","executionInfo":{"status":"ok","timestamp":1743836035515,"user_tz":-660,"elapsed":56,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ghoPO4axdWw9","executionInfo":{"status":"ok","timestamp":1743837497384,"user_tz":-660,"elapsed":1461864,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}},"outputId":"3080454e-8481-4af8-a9df-26da01881613"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Training data shape: (32561, 108)\n","Testing data shape: (16281, 108)\n","Data dimension: 108\n","Autoencoder latent dimension: 64\n","Generator input dimension: 128\n","Pretraining autoencoder for 50 epochs...\n","Epoch [10/50], Reconstruction Loss: 0.035789\n","Epoch [20/50], Reconstruction Loss: 0.035351\n","Epoch [30/50], Reconstruction Loss: 0.035337\n","Epoch [40/50], Reconstruction Loss: 0.035069\n","Epoch [50/50], Reconstruction Loss: 0.034908\n","\n","Training MedGAN...\n","Training MedGAN for 300 epochs...\n","Epoch [10/300], D Loss: 0.0954, G Loss: 4.0298\n","Epoch [20/300], D Loss: 0.0245, G Loss: 6.2375\n","Epoch [30/300], D Loss: 0.0044, G Loss: 8.0276\n","Epoch [40/300], D Loss: 0.0017, G Loss: 9.0998\n","Epoch [50/300], D Loss: 0.0008, G Loss: 10.4458\n","Epoch [60/300], D Loss: 0.0007, G Loss: 10.6637\n","Epoch [70/300], D Loss: 0.0005, G Loss: 10.9912\n","Epoch [80/300], D Loss: 0.0003, G Loss: 10.9004\n","Epoch [90/300], D Loss: 0.0002, G Loss: 12.0342\n","Epoch [100/300], D Loss: 0.0002, G Loss: 12.4445\n","Epoch [110/300], D Loss: 0.0004, G Loss: 12.5494\n","Epoch [120/300], D Loss: 0.0000, G Loss: 13.5623\n","Epoch [130/300], D Loss: 0.0000, G Loss: 14.1763\n","Epoch [140/300], D Loss: 0.0001, G Loss: 12.9233\n","Epoch [150/300], D Loss: 0.0000, G Loss: 12.2658\n","Epoch [160/300], D Loss: 0.0000, G Loss: 13.7558\n","Epoch [170/300], D Loss: 0.0001, G Loss: 13.7576\n","Epoch [180/300], D Loss: 0.0000, G Loss: 13.2534\n","Epoch [190/300], D Loss: 0.0001, G Loss: 13.3337\n","Epoch [200/300], D Loss: 0.0001, G Loss: 12.4218\n","Epoch [210/300], D Loss: 0.0000, G Loss: 13.5996\n","Epoch [220/300], D Loss: 0.0000, G Loss: 12.9479\n","Epoch [230/300], D Loss: 0.0000, G Loss: 13.1251\n","Epoch [240/300], D Loss: 0.0000, G Loss: 14.8992\n","Epoch [250/300], D Loss: 0.0000, G Loss: 12.7134\n","Epoch [260/300], D Loss: 0.0000, G Loss: 13.1956\n","Epoch [270/300], D Loss: 0.0003, G Loss: 13.6250\n","Epoch [280/300], D Loss: 0.0000, G Loss: 13.8823\n","Epoch [290/300], D Loss: 0.0000, G Loss: 15.0803\n","Epoch [300/300], D Loss: 0.0000, G Loss: 14.7340\n","Generating 1000 synthetic samples...\n","Evaluating statistical similarity...\n","\n","Jensen-Shannon Divergence (average): 0.05063342232390692\n","Wasserstein Distance (average): 0.09283492667001678\n","\n","Evaluating Machine Learning Utility (TSTR)...\n","\n","TSTR Results:\n","Logistic Regression: Accuracy = 0.7588, F1 Score = 0.0000\n","MLP: Accuracy = 0.7588, F1 Score = 0.0000\n","Random Forest: Accuracy = 0.3871, F1 Score = 0.4273\n","XGBoost: Accuracy = 0.7588, F1 Score = 0.0000\n","\n","Evaluation complete! Check the output directory for plots and saved model.\n"]}]}]}