{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaje\\AppData\\Local\\Temp\\ipykernel_55564\\1496364867.py:24: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.decode('utf-8').replace(\"\\\\\", \"\").replace(\"'\", \"\").strip() if isinstance(x, bytes) else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Repeat 1 ---\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch 0/150 | D Loss: 9.6523 | G Loss: -0.0149\n",
      "Epoch 50/150 | D Loss: 8653.6406 | G Loss: -53.7885\n",
      "Epoch 100/150 | D Loss: 380491.5000 | G Loss: -852.5272\n",
      "Saved synthetic samples for Repeat 1, Fold 1 to Adult - generated data\\synthetic_samples_repeat_1_fold_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\shaje\\AppData\\Local\\Temp\\ipykernel_55564\\1496364867.py:195: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(X_balanced, y_balanced)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 2 ---\n",
      "Epoch 0/150 | D Loss: 9.6407 | G Loss: -0.0285\n",
      "Epoch 50/150 | D Loss: 15193.4482 | G Loss: -42.0088\n",
      "Epoch 100/150 | D Loss: 1548128.7500 | G Loss: -1044.2056\n",
      "Saved synthetic samples for Repeat 1, Fold 2 to Adult - generated data\\synthetic_samples_repeat_1_fold_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shaje\\AppData\\Local\\Temp\\ipykernel_55564\\1496364867.py:195: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(X_balanced, y_balanced)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Repeat 2 ---\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch 0/150 | D Loss: 9.6421 | G Loss: -0.0185\n",
      "Epoch 50/150 | D Loss: 9826.2441 | G Loss: -65.3757\n",
      "Epoch 100/150 | D Loss: 502604.5625 | G Loss: -1109.1370\n",
      "Saved synthetic samples for Repeat 2, Fold 1 to Adult - generated data\\synthetic_samples_repeat_2_fold_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\shaje\\AppData\\Local\\Temp\\ipykernel_55564\\1496364867.py:195: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(X_balanced, y_balanced)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 2 ---\n",
      "Epoch 0/150 | D Loss: 9.6736 | G Loss: -0.0134\n",
      "Epoch 50/150 | D Loss: 11246.0918 | G Loss: -62.3670\n",
      "Epoch 100/150 | D Loss: 1526327.1250 | G Loss: -1166.4490\n",
      "Saved synthetic samples for Repeat 2, Fold 2 to Adult - generated data\\synthetic_samples_repeat_2_fold_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\shaje\\AppData\\Local\\Temp\\ipykernel_55564\\1496364867.py:195: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(X_balanced, y_balanced)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Repeat 3 ---\n",
      "\n",
      "--- Fold 1 ---\n",
      "Epoch 0/150 | D Loss: 9.6782 | G Loss: -0.0208\n",
      "Epoch 50/150 | D Loss: 11025.9092 | G Loss: -66.6659\n",
      "Epoch 100/150 | D Loss: 732354.0625 | G Loss: -1170.7943\n",
      "Saved synthetic samples for Repeat 3, Fold 1 to Adult - generated data\\synthetic_samples_repeat_3_fold_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\numpy\\lib\\histograms.py:906: RuntimeWarning: invalid value encountered in divide\n",
      "  return n/db/n.sum(), bin_edges\n",
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\numpy\\lib\\histograms.py:906: RuntimeWarning: divide by zero encountered in divide\n",
      "  return n/db/n.sum(), bin_edges\n",
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\numpy\\lib\\histograms.py:906: RuntimeWarning: invalid value encountered in divide\n",
      "  return n/db/n.sum(), bin_edges\n",
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\numpy\\lib\\histograms.py:906: RuntimeWarning: divide by zero encountered in divide\n",
      "  return n/db/n.sum(), bin_edges\n",
      "C:\\Users\\shaje\\AppData\\Local\\Temp\\ipykernel_55564\\1496364867.py:195: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(X_balanced, y_balanced)\n",
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\numpy\\lib\\histograms.py:906: RuntimeWarning: invalid value encountered in divide\n",
      "  return n/db/n.sum(), bin_edges\n",
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\numpy\\lib\\histograms.py:906: RuntimeWarning: divide by zero encountered in divide\n",
      "  return n/db/n.sum(), bin_edges\n",
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\numpy\\lib\\histograms.py:906: RuntimeWarning: invalid value encountered in divide\n",
      "  return n/db/n.sum(), bin_edges\n",
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\numpy\\lib\\histograms.py:906: RuntimeWarning: divide by zero encountered in divide\n",
      "  return n/db/n.sum(), bin_edges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 2 ---\n",
      "Epoch 0/150 | D Loss: 9.7140 | G Loss: -0.0052\n",
      "Epoch 50/150 | D Loss: 6827.8765 | G Loss: -31.5947\n",
      "Epoch 100/150 | D Loss: 391324.0938 | G Loss: -774.8862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaje\\AppData\\Local\\Temp\\ipykernel_55564\\1496364867.py:186: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  fake_df = fake_df.clip(lower=X.min(), upper=X.max(), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved synthetic samples for Repeat 3, Fold 2 to Adult - generated data\\synthetic_samples_repeat_3_fold_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\shaje\\Downloads\\AI-masters\\Team project A\\WCGAN-GP\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shaje\\AppData\\Local\\Temp\\ipykernel_55564\\1496364867.py:195: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(X_balanced, y_balanced)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Evaluation Summary ---\n",
      "                   TSTR Accuracy       JSD  Wasserstein\n",
      "Repeat Fold Model                                      \n",
      "1      1    LR          0.835429  0.999038     0.508194\n",
      "            MLP         0.850538  0.999038     0.508194\n",
      "            RF          0.831416  0.999038     0.508194\n",
      "            XGB         0.832972  0.999038     0.508194\n",
      "       2    LR          0.837967  0.998517     0.419534\n",
      "            MLP         0.836862  0.998517     0.419534\n",
      "            RF          0.825765  0.998517     0.419534\n",
      "            XGB         0.833586  0.998517     0.419534\n",
      "2      1    LR          0.835429  0.998620     0.422201\n",
      "            MLP         0.824782  0.998620     0.422201\n",
      "            RF          0.831334  0.998620     0.422201\n",
      "            XGB         0.832972  0.998620     0.422201\n",
      "       2    LR          0.837967  0.998052     0.563770\n",
      "            MLP         0.825028  0.998052     0.563770\n",
      "            RF          0.826092  0.998052     0.563770\n",
      "            XGB         0.833586  0.998052     0.563770\n",
      "3      1    LR          0.835429       NaN     0.201274\n",
      "            MLP         0.808239       NaN     0.201274\n",
      "            RF          0.831497       NaN     0.201274\n",
      "            XGB         0.832972       NaN     0.201274\n",
      "       2    LR          0.837967  0.970945     0.572747\n",
      "            MLP         0.811965  0.970945     0.572747\n",
      "            RF          0.824454  0.970945     0.572747\n",
      "            XGB         0.833586  0.970945     0.572747\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.stats import wasserstein_distance\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Embedding, multiply, LeakyReLU, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# ---------------------- Load and Preprocess ARFF ----------------------\n",
    "data, meta = arff.loadarff(\"data/adult 1.arff\")\n",
    "df = pd.DataFrame(data)\n",
    "df = df.applymap(lambda x: x.decode('utf-8').replace(\"\\\\\", \"\").replace(\"'\", \"\").strip() if isinstance(x, bytes) else x)\n",
    "df['class'] = df['class'].map({'<=50K': 0, '>50K': 1})\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "X = df.drop('class', axis=1)\n",
    "y = df['class']\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ---------------------- JSD Utility ----------------------\n",
    "def compute_jsd_columnwise(real, fake, bins=50):\n",
    "    jsd_scores = []\n",
    "    for i in range(real.shape[1]):\n",
    "        try:\n",
    "            r_hist, _ = np.histogram(real[:, i], bins=bins, density=True)\n",
    "            f_hist, _ = np.histogram(fake[:, i], bins=bins, density=True)\n",
    "            r_hist += 1e-8\n",
    "            f_hist += 1e-8\n",
    "            jsd = jensenshannon(r_hist, f_hist, base=2)\n",
    "            jsd_scores.append(jsd)\n",
    "        except:\n",
    "            continue\n",
    "    return np.mean(jsd_scores)\n",
    "\n",
    "# ---------------------- Improved WCGAN-GP ----------------------\n",
    "class ImprovedWCGANGP:\n",
    "    def __init__(self, data_dim, num_classes, latent_dim=32, gp_weight=10):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.data_dim = data_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.gp_weight = gp_weight\n",
    "        optimizer = Adam(0.0001, beta_1=0.5, beta_2=0.9)\n",
    "\n",
    "        self.generator = self.build_generator()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "        self.critic.trainable = True\n",
    "        self.critic.compile(loss=self.wasserstein_loss, optimizer=optimizer)\n",
    "\n",
    "        self.critic.trainable = False\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        fake_data = self.generator([noise, label])\n",
    "        validity = self.critic([fake_data, label])\n",
    "        self.combined = Model([noise, label], validity)\n",
    "        self.combined.compile(loss=self.wasserstein_loss, optimizer=optimizer)\n",
    "\n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def gradient_penalty(self, real_samples, fake_samples, labels):\n",
    "        alpha = tf.random.uniform([real_samples.shape[0], 1], 0.0, 1.0)\n",
    "        interpolated = alpha * real_samples + (1 - alpha) * fake_samples\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            validity_interpolated = self.critic([interpolated, labels])\n",
    "        grads = gp_tape.gradient(validity_interpolated, interpolated)\n",
    "        grad_l2 = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=1))\n",
    "        gp = tf.reduce_mean((grad_l2 - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def build_generator(self):\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
    "        model_input = multiply([noise, label_embedding])\n",
    "        x = Dense(128)(model_input)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(256)(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(512)(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        output = Dense(self.data_dim, activation='tanh')(x)\n",
    "        return Model([noise, label], output)\n",
    "\n",
    "    def build_critic(self):\n",
    "        data_input = Input(shape=(self.data_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.data_dim)(label))\n",
    "        model_input = multiply([data_input, label_embedding])\n",
    "        x = Dense(512)(model_input)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        x = Dense(256)(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        x = Dense(128)(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        output = Dense(1)(x)\n",
    "        return Model([data_input, label], output)\n",
    "\n",
    "    def train(self, X_train, y_train, epochs=150, batch_size=64, n_critic=5):\n",
    "        valid = -np.ones((batch_size, 1))\n",
    "        fake = np.ones((batch_size, 1))\n",
    "        for epoch in range(epochs):\n",
    "            for _ in range(n_critic):\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                real_samples = X_train[idx]\n",
    "                labels = y_train[idx].reshape(-1, 1)\n",
    "\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                fake_samples = self.generator.predict([noise, labels], verbose=0)\n",
    "\n",
    "                d_loss_real = self.critic.train_on_batch([real_samples, labels], valid)\n",
    "                d_loss_fake = self.critic.train_on_batch([fake_samples, labels], fake)\n",
    "                gp = self.gradient_penalty(real_samples, fake_samples, labels)\n",
    "                d_loss = 0.5 * (d_loss_real + d_loss_fake) + self.gp_weight * gp\n",
    "\n",
    "            g_loss = self.combined.train_on_batch([noise, labels], valid)\n",
    "\n",
    "            if epoch % 50 == 0:\n",
    "                print(f\"Epoch {epoch}/{epochs} | D Loss: {d_loss:.4f} | G Loss: {g_loss:.4f}\")\n",
    "\n",
    "# ---------------------- Create Folder for Synthetic Data ----------------------\n",
    "folder_path = \"Adult - generated data\"\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)  # Create the folder if it doesn't exist\n",
    "\n",
    "# ---------------------- Evaluation Setup ----------------------\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "repeats = 3\n",
    "sample_fraction = 0.5\n",
    "models = {\n",
    "    'LR': LogisticRegression(max_iter=200),\n",
    "    'MLP': MLPClassifier(max_iter=200),\n",
    "    'RF': RandomForestClassifier(),\n",
    "    'XGB': XGBClassifier(eval_metric='logloss')\n",
    "}\n",
    "results = []\n",
    "\n",
    "for repeat in range(repeats):\n",
    "    print(f\"\\n--- Repeat {repeat + 1} ---\")\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X_scaled)):\n",
    "        print(f\"\\n--- Fold {fold + 1} ---\")\n",
    "        X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "        y_train, y_test = y.iloc[train_index].values, y.iloc[test_index].values\n",
    "\n",
    "        wcgan = ImprovedWCGANGP(data_dim=X_train.shape[1], num_classes=2)\n",
    "        wcgan.train(X_train, y_train, epochs=150)\n",
    "\n",
    "        train_df = pd.DataFrame(X_train)\n",
    "        train_df['income'] = y_train\n",
    "        majority = train_df[train_df['income'] == 0]\n",
    "        minority = train_df[train_df['income'] == 1]\n",
    "        minority_upsampled = resample(minority, replace=True, n_samples=len(majority), random_state=42)\n",
    "        balanced_train = pd.concat([majority, minority_upsampled])\n",
    "\n",
    "        X_balanced = balanced_train.drop('income', axis=1).values\n",
    "        y_balanced = balanced_train['income'].values.reshape(-1, 1)\n",
    "\n",
    "        # Ensure sample size is 50% of the training set size\n",
    "        sample_size = int(0.5 * len(X_balanced))\n",
    "\n",
    "        noise = np.random.normal(0, 1, (sample_size, wcgan.latent_dim))\n",
    "        y_synthetic_labels = y_balanced[:sample_size]\n",
    "        fake_samples = wcgan.generator.predict([noise, y_synthetic_labels], verbose=0)\n",
    "\n",
    "        fake_df = pd.DataFrame(fake_samples, columns=X.columns)\n",
    "        fake_df = pd.DataFrame(scaler.inverse_transform(fake_df), columns=X.columns)\n",
    "        fake_df = fake_df.clip(lower=X.min(), upper=X.max(), axis=1)\n",
    "        fake_df = pd.DataFrame(scaler.fit_transform(fake_df), columns=X.columns)\n",
    "\n",
    "        # Save only the synthetic samples (fake data) to the \"Adult - generated data\" folder\n",
    "        synthetic_file_path = os.path.join(folder_path, f'synthetic_samples_repeat_{repeat + 1}_fold_{fold + 1}.csv')\n",
    "        fake_df.to_csv(synthetic_file_path, index=False)\n",
    "        print(f\"Saved synthetic samples for Repeat {repeat + 1}, Fold {fold + 1} to {synthetic_file_path}\")\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            model.fit(X_balanced, y_balanced)\n",
    "            preds = model.predict(X_test)\n",
    "            acc = accuracy_score(y_test, preds)\n",
    "\n",
    "            jsd = compute_jsd_columnwise(X_train, fake_df.values)\n",
    "            wd = np.mean([\n",
    "                wasserstein_distance(X_train[:, i], fake_df.values[:, i])\n",
    "                for i in range(X_train.shape[1])\n",
    "            ])\n",
    "\n",
    "            results.append({\n",
    "                \"Repeat\": repeat + 1,\n",
    "                \"Fold\": fold + 1,\n",
    "                \"Model\": model_name,\n",
    "                \"TSTR Accuracy\": acc,\n",
    "                \"JSD\": jsd,\n",
    "                \"Wasserstein\": wd\n",
    "            })\n",
    "\n",
    "# ---------------------- Results Summary ----------------------\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n--- Final Evaluation Summary ---\")\n",
    "print(results_df.groupby([\"Repeat\", \"Fold\", \"Model\"]).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
