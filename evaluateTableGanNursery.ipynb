{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from katabatic.models.TableGAN import TableGANAdapter, TableGAN, preprocess_data, postprocess_data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the adapter with a specific privacy setting\n",
    "tablegan_adapter = TableGANAdapter(type='continuous', privacy_setting='high')\n",
    "data_path = 'data/Nursery/Nursery.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "labelencoder=preprocessing.LabelEncoder()\n",
    "df= df.apply(lambda col: labelencoder.fit_transform(col) if col.dtype =='object' else col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12960 entries, 0 to 12959\n",
      "Data columns (total 9 columns):\n",
      " #   Column    Non-Null Count  Dtype\n",
      "---  ------    --------------  -----\n",
      " 0    parents  12960 non-null  int32\n",
      " 1   has_nurs  12960 non-null  int32\n",
      " 2   form      12960 non-null  int32\n",
      " 3   children  12960 non-null  int32\n",
      " 4   housing   12960 non-null  int32\n",
      " 5   finance   12960 non-null  int32\n",
      " 6   social    12960 non-null  int32\n",
      " 7   health    12960 non-null  int32\n",
      " 8   Target    12960 non-null  int32\n",
      "dtypes: int32(9)\n",
      "memory usage: 455.8 KB\n",
      "None\n",
      " parents    0\n",
      "has_nurs    0\n",
      "form        0\n",
      "children    0\n",
      "housing     0\n",
      "finance     0\n",
      "social      0\n",
      "health      0\n",
      "Target      0\n",
      "dtype: int64\n",
      " parents    0\n",
      "has_nurs    0\n",
      "form        0\n",
      "children    0\n",
      "housing     0\n",
      "finance     0\n",
      "social      0\n",
      "health      0\n",
      "Target      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n",
    "print(df.isnull().sum())\n",
    "print(df.isin(['?']).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.copy().drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---FIT TableGAN Model with high privacy setting\n",
      "---Initialise TableGAN Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\py39\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "d:\\Anaconda3\\envs\\py39\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: [D loss: -1.6309] [G loss: 1.0728] [C loss: 0.2399]\n",
      "Epoch 20/50: [D loss: -1.5167] [G loss: 1.5420] [C loss: 0.1495]\n",
      "Epoch 30/50: [D loss: -1.5107] [G loss: 2.3895] [C loss: 0.0849]\n",
      "Epoch 40/50: [D loss: -1.1561] [G loss: 3.0408] [C loss: 0.0544]\n",
      "Epoch 50/50: [D loss: -1.1526] [G loss: 3.1096] [C loss: 0.0345]\n"
     ]
    }
   ],
   "source": [
    "tablegan_adapter.fit(x_train, y_train, epochs=50, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Generate from TableGAN Model\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "synthetic_data = tablegan_adapter.generate(size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_df = pd.DataFrame(synthetic_data)\n",
    "\n",
    "x_sync_train = synthetic_df.drop(synthetic_df.columns[-1],axis=1).values\n",
    "y_sync_train = synthetic_df.iloc[ :, -1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\py39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Anaconda3\\envs\\py39\\lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Anaconda3\\envs\\py39\\lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Anaconda3\\envs\\py39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda3\\envs\\py39\\lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\Anaconda3\\envs\\py39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Anaconda3\\envs\\py39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluated Item        LR        RF       MLP      XGBT\n",
      "0           TSTR  0.288194  0.325231  0.303241  0.374614\n",
      "1           TRTR  0.762346  0.983025  0.985340  0.672454\n"
     ]
    }
   ],
   "source": [
    "# TSTR (train synthetic test real)\n",
    "tstr_score_lr  = LogisticRegression().fit(x_sync_train, y_sync_train).score(x_test, y_test)\n",
    "tstr_score_rf  = RandomForestClassifier().fit(x_sync_train, y_sync_train).score(x_test, y_test)\n",
    "tstr_score_mlp = MLPClassifier().fit(x_sync_train, y_sync_train).score(x_test, y_test)\n",
    "y_sync_train = LabelEncoder().fit_transform(y_sync_train)\n",
    "xgbt_classifier = XGBClassifier(eval_metric='logloss' )\n",
    "tstr_score_xgbt = xgbt_classifier.fit(x_sync_train, y_sync_train).score(x_test, y_test)\n",
    "\n",
    "\n",
    "# TRTR (train real test real)\n",
    "trtr_score_lr  = LogisticRegression().fit(x_train, y_train).score(x_test, y_test)\n",
    "trtr_score_rf  = RandomForestClassifier().fit(x_train, y_train).score(x_test, y_test)\n",
    "trtr_score_mlp = MLPClassifier().fit(x_train, y_train).score(x_test, y_test)\n",
    "y_train = LabelEncoder().fit_transform(y_train)\n",
    "xgbt_classifier = XGBClassifier(eval_metric='logloss')\n",
    "trtr_score_xgbt = xgbt_classifier.fit(x_train, y_train).score(x_test, y_test)\n",
    "df_evaluate = pd.DataFrame([\n",
    "    ['TSTR', tstr_score_lr, tstr_score_rf, tstr_score_mlp, tstr_score_xgbt],\n",
    "    ['TRTR', trtr_score_lr,trtr_score_rf,trtr_score_mlp, trtr_score_xgbt]\n",
    "], columns=['Evaluated Item', 'LR', 'RF', 'MLP', 'XGBT'])\n",
    "print(df_evaluate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
