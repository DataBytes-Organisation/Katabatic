import numpy as np
import pandas as pd
from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, f1_score, r2_score
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.neural_network import MLPClassifier, MLPRegressor
from xgboost import XGBClassifier, XGBRegressor
import logging
from scipy.stats import wasserstein_distance
from scipy.spatial.distance import jensenshannon

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def evaluate_likelihood_fitness(real_data, synthetic_data):
    """
    Calculate Likelihood fitness metrics (Lsyn, Ltest) on simulated data.
    
    Args:
        real_data (pd.DataFrame): The original real dataset.
        synthetic_data (pd.DataFrame): The synthetic dataset generated by TabDDPM.
    
    Returns:
        dict: Dictionary containing Lsyn and Ltest values.
    """
    from sklearn.mixture import GaussianMixture

    # Select only numerical columns for likelihood evaluation
    numeric_columns = real_data.select_dtypes(include=['int64', 'float64']).columns
    
    # Check if there are numerical columns to process
    if len(numeric_columns) == 0:
        logging.warning("No numerical columns found for likelihood fitness evaluation")
        return {
            "Lsyn": None,
            "Ltest": None
        }
        
    synthetic_numeric = synthetic_data[numeric_columns].dropna()
    real_numeric = real_data[numeric_columns].dropna()

    # Fit a Gaussian Mixture Model on the synthetic data
    gmm = GaussianMixture(n_components=min(5, len(synthetic_numeric)), covariance_type='full', random_state=42)
    gmm.fit(synthetic_numeric)

    # Evaluate log-likelihoods
    Lsyn = gmm.score(synthetic_numeric)  # Log-likelihood of synthetic data
    Ltest = gmm.score(real_numeric)      # Log-likelihood of real data under the synthetic model

    return {
        "Lsyn": Lsyn,
        "Ltest": Ltest
    }

def evaluate_statistical_similarity(real_data, synthetic_data):
    """
    Calculate Statistical similarity metrics (Jensen-Shannon Divergence, Wasserstein Distance).
    
    Args:
        real_data (pd.DataFrame): The original real dataset.
        synthetic_data (pd.DataFrame): The synthetic dataset.
    
    Returns:
        dict: Dictionary containing mean Jensen-Shannon Divergence and mean Wasserstein Distance.
    """
    js_divergences = []
    wasserstein_distances = []

    # Iterate over each column to compute similarity metrics
    for column in real_data.columns:
        if real_data[column].dtype in ['int64', 'float64']:
            # For numerical columns, compute Wasserstein Distance
            real_values = real_data[column].dropna().values
            synth_values = synthetic_data[column].dropna().values

            wd = wasserstein_distance(real_values, synth_values)
            wasserstein_distances.append(wd)
        else:
            # For categorical columns, compute Jensen-Shannon Divergence
            real_counts = real_data[column].value_counts(normalize=True)
            synth_counts = synthetic_data[column].value_counts(normalize=True)

            # Ensure both distributions have the same categories
            all_categories = set(real_counts.index) | set(synth_counts.index)
            real_probs = real_counts.reindex(all_categories, fill_value=0)
            synth_probs = synth_counts.reindex(all_categories, fill_value=0)

            # Compute Jensen-Shannon Divergence
            js_div = jensenshannon(real_probs, synth_probs)
            js_divergences.append(js_div)

    return {
        "JSD_mean": np.mean(js_divergences) if js_divergences else None,
        "Wasserstein_mean": np.mean(wasserstein_distances) if wasserstein_distances else None
    }

def evaluate_ml_efficacy(X_real, y_real):
    """
    Calculate Machine Learning efficacy metrics on real data.
    
    Args:
        X_real (pd.DataFrame): Feature data from the real dataset.
        y_real (pd.Series): Target labels from the real dataset.
    
    Returns:
        dict: Dictionary containing performance metrics for each model.
    """
    # Convert categorical target to numeric for models that expect numeric labels
    if y_real.dtype == 'object' or y_real.dtype.name == 'category':
        label_encoder = LabelEncoder()
        y_real_encoded = label_encoder.fit_transform(y_real)
        # Store mapping for reference
        label_mapping = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))
        logging.info(f"Encoded categorical target with mapping: {label_mapping}")
    else:
        y_real_encoded = y_real

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X_real, y_real_encoded, test_size=0.2, random_state=42)

    # Identify numerical and categorical features for preprocessing
    numeric_features = X_real.select_dtypes(include=['int64', 'float64']).columns
    categorical_features = X_real.select_dtypes(include=['object', 'category']).columns

    # Create preprocessor steps
    preprocessor_steps = []
    if len(numeric_features) > 0:
        preprocessor_steps.append(('num', StandardScaler(), numeric_features))
    if len(categorical_features) > 0:
        try:
            # For newer scikit-learn versions
            preprocessor_steps.append(('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features))
        except TypeError:
            # For older scikit-learn versions
            preprocessor_steps.append(('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), categorical_features))

    # Define a preprocessing pipeline
    if preprocessor_steps:
        preprocessor = ColumnTransformer(transformers=preprocessor_steps)
    else:
        # If no features need transformation, use a passthrough
        preprocessor = 'passthrough'

    # Check if the target variable is categorical
    if y_real.dtype == 'object' or y_real.dtype.name == 'category':
        # Define classifiers for categorical targets
        classifiers = {
            "LogisticRegression": LogisticRegression(max_iter=1000),
            "RandomForest": RandomForestClassifier(),
            "MLP": MLPClassifier(max_iter=1000),
            "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss')
        }

        results = {}
        for name, clf in classifiers.items():
            try:
                # Create a pipeline with preprocessing and the classifier
                pipeline = Pipeline([
                    ('preprocessor', preprocessor),
                    ('classifier', clf)
                ])

                # Train the classifier
                pipeline.fit(X_train, y_train)
                # Predict on the test set
                y_pred = pipeline.predict(X_test)

                # Calculate performance metrics
                accuracy = accuracy_score(y_test, y_pred)
                f1 = f1_score(y_test, y_pred, average='weighted')

                results[name] = {
                    "Accuracy": accuracy,
                    "F1": f1
                }
            except Exception as e:
                logging.error(f"Error training {name} classifier: {str(e)}")
                results[name] = {
                    "Accuracy": None,
                    "F1": None,
                    "Error": str(e)
                }

    else:
        # Define regressors for numerical targets
        regressors = {
            "LinearRegression": LinearRegression(),
            "RandomForest": RandomForestRegressor(),
            "MLP": MLPRegressor(max_iter=1000),
            "XGBoost": XGBRegressor()
        }

        results = {}
        for name, reg in regressors.items():
            try:
                # Create a pipeline with preprocessing and the regressor
                pipeline = Pipeline([
                    ('preprocessor', preprocessor),
                    ('regressor', reg)
                ])

                # Train the regressor
                pipeline.fit(X_train, y_train)
                # Predict on the test set
                y_pred = pipeline.predict(X_test)

                # Calculate R-squared score
                r2 = r2_score(y_test, y_pred)

                results[name] = {
                    "R2": r2
                }
            except Exception as e:
                logging.error(f"Error training {name} regressor: {str(e)}")
                results[name] = {
                    "R2": None,
                    "Error": str(e)
                }

    return results

def evaluate_tstr(X_real, y_real, X_synthetic, y_synthetic):
    """
    Evaluate Machine Learning utility using the TSTR (Train on Synthetic, Test on Real) approach.
    
    Args:
        X_real (pd.DataFrame): Feature data from the real dataset.
        y_real (pd.Series): Target labels from the real dataset.
        X_synthetic (pd.DataFrame): Feature data from the synthetic dataset.
        y_synthetic (pd.Series): Target labels from the synthetic dataset.
    
    Returns:
        dict: Dictionary containing performance metrics for each model.
    """
    # Convert categorical targets to numeric for models that expect numeric labels
    if y_real.dtype == 'object' or y_real.dtype.name == 'category':
        label_encoder = LabelEncoder()
        # Fit on all possible categories from both real and synthetic
        all_categories = pd.concat([y_real, y_synthetic]).unique()
        label_encoder.fit(all_categories)
        y_real_encoded = label_encoder.transform(y_real)
        y_synthetic_encoded = label_encoder.transform(y_synthetic)
        # Store mapping for reference
        label_mapping = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))
        logging.info(f"Encoded categorical targets with mapping: {label_mapping}")
    else:
        y_real_encoded = y_real
        y_synthetic_encoded = y_synthetic

    # Identify numerical and categorical features for preprocessing
    numeric_features = X_real.select_dtypes(include=['int64', 'float64']).columns
    categorical_features = X_real.select_dtypes(include=['object', 'category']).columns

    # Create preprocessor steps
    preprocessor_steps = []
    if len(numeric_features) > 0:
        preprocessor_steps.append(('num', StandardScaler(), numeric_features))
    if len(categorical_features) > 0:
        try:
            # For newer scikit-learn versions
            preprocessor_steps.append(('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features))
        except TypeError:
            # For older scikit-learn versions
            preprocessor_steps.append(('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), categorical_features))

    # Define a preprocessing pipeline
    if preprocessor_steps:
        preprocessor = ColumnTransformer(transformers=preprocessor_steps)
    else:
        # If no features need transformation, use a passthrough
        preprocessor = 'passthrough'

    # Check if the target variable is categorical
    if y_real.dtype == 'object' or y_real.dtype.name == 'category':
        # Define classifiers for categorical targets
        classifiers = {
            "LogisticRegression": LogisticRegression(max_iter=1000),
            "RandomForest": RandomForestClassifier(),
            "MLP": MLPClassifier(max_iter=1000),
            "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss')
        }

        results = {}
        for name, clf in classifiers.items():
            try:
                # Create a pipeline with preprocessing and the classifier
                pipeline = Pipeline([
                    ('preprocessor', preprocessor),
                    ('classifier', clf)
                ])

                # Train the classifier on synthetic data
                pipeline.fit(X_synthetic, y_synthetic_encoded)
                # Predict on the real data
                y_pred = pipeline.predict(X_real)

                # Calculate performance metrics
                accuracy = accuracy_score(y_real_encoded, y_pred)
                f1 = f1_score(y_real_encoded, y_pred, average='weighted')

                results[name] = {
                    "Accuracy": accuracy,
                    "F1": f1
                }
            except Exception as e:
                logging.error(f"Error in TSTR with {name}: {str(e)}")
                results[name] = {
                    "Accuracy": None,
                    "F1": None,
                    "Error": str(e)
                }

    else:
        # Define regressors for numerical targets
        regressors = {
            "LinearRegression": LinearRegression(),
            "RandomForest": RandomForestRegressor(),
            "MLP": MLPRegressor(max_iter=1000),
            "XGBoost": XGBRegressor()
        }

        results = {}
        for name, reg in regressors.items():
            try:
                # Create a pipeline with preprocessing and the regressor
                pipeline = Pipeline([
                    ('preprocessor', preprocessor),
                    ('regressor', reg)
                ])

                # Train the regressor on synthetic data
                pipeline.fit(X_synthetic, y_synthetic_encoded)
                # Predict on the real data
                y_pred = pipeline.predict(X_real)

                # Calculate R-squared score
                r2 = r2_score(y_real_encoded, y_pred)

                results[name] = {
                    "R2": r2
                }
            except Exception as e:
                logging.error(f"Error in TSTR with {name}: {str(e)}")
                results[name] = {
                    "R2": None,
                    "Error": str(e)
                }

    return results

def evaluate_tabddpm(real_data, synthetic_data, target_column=None):
        """
        Evaluate the TabDDPM model using various metrics including likelihood fitness,
        statistical similarity, machine learning efficacy, and TSTR performance.

        Args:
            real_data (pd.DataFrame): The original real dataset.
            synthetic_data (pd.DataFrame): The synthetic dataset generated by TabDDPM.
            target_column (str, optional): Name of the target column to use.

        Returns:
            dict: Dictionary containing all evaluation metrics.
        """
        results = {}
        
        # Check if target column exists and is specified
        has_target = target_column and target_column in real_data.columns
        
        if has_target and target_column in synthetic_data.columns:
            X_real = real_data.drop(columns=[target_column])
            y_real = real_data[target_column]
            X_synthetic = synthetic_data.drop(columns=[target_column])
            # Use ffill() to avoid warning
            y_synthetic = synthetic_data[target_column].ffill()
        else:
            if has_target:
                print(f"Warning: Target column '{target_column}' not found in synthetic data.")
                print("Will evaluate only feature distributions without target-based metrics.")
            X_real = real_data if not has_target else real_data.drop(columns=[target_column])
            y_real = None
            X_synthetic = synthetic_data
            y_synthetic = None

        if y_synthetic is not None:
            # Handle missing values in the target
            if y_synthetic.isna().any():
                mode_val = y_synthetic.mode()[0]
                y_synthetic = y_synthetic.fillna(mode_val)
                logging.info(f"Filled {y_synthetic.isna().sum()} missing values in synthetic target with mode: {mode_val}")

        # Evaluate likelihood fitness (for numerical columns)
        try:
            results["likelihood_fitness"] = evaluate_likelihood_fitness(X_real, X_synthetic)
        except Exception as e:
            logging.error(f"Error in likelihood_fitness: {str(e)}")
            results["likelihood_fitness"] = None

        # Evaluate statistical similarity
        try:
            results["statistical_similarity"] = evaluate_statistical_similarity(X_real, X_synthetic)
        except Exception as e:
            logging.error(f"Error in statistical_similarity: {str(e)}")
            results["statistical_similarity"] = None

        # Evaluate machine learning efficacy and TSTR if target column is provided
        if y_real is not None:
            try:
                results["ml_efficacy"] = evaluate_ml_efficacy(X_real, y_real)
            except Exception as e:
                logging.error(f"Error in ml_efficacy: {str(e)}")
                results["ml_efficacy"] = None

            if y_synthetic is not None:
                try:
                    results["tstr_performance"] = evaluate_tstr(X_real, y_real, X_synthetic, y_synthetic)
                except Exception as e:
                    logging.error(f"Error in tstr_performance: {str(e)}")
                    results["tstr_performance"] = None

        return results

def print_evaluation_results(results):
    """
    Print the evaluation results in a structured and readable format using logging.
    
    Args:
        results (dict): Dictionary containing all evaluation metrics.
    """
    logging.info("TabDDPM Evaluation Results:")

    # Print Likelihood Fitness Metrics
    if results.get("likelihood_fitness") is not None:
        if results["likelihood_fitness"]["Lsyn"] is not None:
            logging.info("\nLikelihood Fitness Metrics:")
            logging.info(f"  - Lsyn (Synthetic Data Log-Likelihood): {results['likelihood_fitness']['Lsyn']:.4f}")
            logging.info(f"  - Ltest (Real Data Log-Likelihood under Synthetic Model): {results['likelihood_fitness']['Ltest']:.4f}")
        else:
            logging.info("\nLikelihood Fitness: Not applicable for fully categorical data")
    else:
        logging.info("\nLikelihood Fitness: Error occurred during calculation")

    # Print Statistical Similarity Metrics
    if results.get("statistical_similarity") is not None:
        logging.info("\nStatistical Similarity Metrics:")
        if results['statistical_similarity']['JSD_mean'] is not None:
            logging.info(f"  - Jensen-Shannon Divergence Mean (Categorical): {results['statistical_similarity']['JSD_mean']:.4f}")
        if results['statistical_similarity']['Wasserstein_mean'] is not None:
            logging.info(f"  - Wasserstein Distance Mean (Numerical): {results['statistical_similarity']['Wasserstein_mean']:.4f}")
    else:
        logging.info("\nStatistical Similarity: Error occurred during calculation")

    # Print Machine Learning Efficacy Metrics
    if results.get("ml_efficacy") is not None:
        logging.info("\nMachine Learning Efficacy Metrics on Real Data:")
        for model_name, metrics in results['ml_efficacy'].items():
            logging.info(f"  {model_name}:")
            for metric_name, value in metrics.items():
                if value is not None and metric_name != "Error":
                    logging.info(f"    - {metric_name}: {value:.4f}")
                elif metric_name == "Error":
                    logging.info(f"    - Error: {value}")
    else:
        logging.info("\nMachine Learning Efficacy: Error occurred during calculation")

    # Print TSTR Performance Metrics
    if results.get("tstr_performance") is not None:
        logging.info("\nMachine Learning Utility using TSTR Approach:")
        for model_name, metrics in results['tstr_performance'].items():
            logging.info(f"  {model_name}:")
            for metric_name, value in metrics.items():
                if value is not None and metric_name != "Error":
                    logging.info(f"    - {metric_name}: {value:.4f}")
                elif metric_name == "Error":
                    logging.info(f"    - Error: {value}")
    else:
        logging.info("\nTSTR Performance: Error occurred during calculation")