{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhACzIqjVSxt",
        "outputId": "fa75d6cf-d7b2-415b-c857-a843aa060f1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss_D=1.3917, Loss_proxy=1.8908, Loss_group=2.2425, Loss_adv=1.7723\n",
            "Epoch 10: Loss_D=1.3975, Loss_proxy=0.0743, Loss_group=0.5728, Loss_adv=0.6381\n",
            "Epoch 20: Loss_D=1.3751, Loss_proxy=0.0309, Loss_group=0.2257, Loss_adv=0.7101\n",
            "Epoch 30: Loss_D=1.3579, Loss_proxy=0.0919, Loss_group=0.4769, Loss_adv=0.7514\n",
            "Epoch 40: Loss_D=1.3293, Loss_proxy=0.0639, Loss_group=0.3966, Loss_adv=0.8977\n",
            " TSTR Results on Adult (train/test files):\n",
            "Random Forest: 0.7780\n",
            "MLP: 0.7834\n"
          ]
        }
      ],
      "source": [
        "# === All Imports ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# === Fusion Network ===\n",
        "class FusionNetwork(nn.Module):\n",
        "    def __init__(self, num_generators, feature_dim):\n",
        "        super(FusionNetwork, self).__init__()\n",
        "        self.weights = nn.Parameter(torch.ones(num_generators, feature_dim) / num_generators)\n",
        "\n",
        "    def forward(self, outputs):\n",
        "        # outputs: list of [G, B, D] tensors\n",
        "        stacked = torch.stack(outputs, dim=0)\n",
        "        gamma = torch.softmax(self.weights, dim=0)\n",
        "        fused = torch.einsum('gd,gbd->bd', gamma, stacked)\n",
        "        return fused, gamma\n",
        "\n",
        "# === Masked Generator ===\n",
        "class MaskedGenerator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(MaskedGenerator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        return self.net(x * mask)\n",
        "\n",
        "# === Discriminator ===\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, feature_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(feature_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# === MEG Adapter (Exact Paper Implementation) ===\n",
        "class MEG(nn.Module):\n",
        "    def __init__(self, input_dim, num_generators=None, alpha=0.1, beta=1.0):\n",
        "        super(MEG, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        # default one generator per original feature\n",
        "        self.num_generators = input_dim if num_generators is None else num_generators\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        # instantiate generators, fusion, discriminator\n",
        "        self.generators = nn.ModuleList([\n",
        "            MaskedGenerator(input_dim) for _ in range(self.num_generators)\n",
        "        ])\n",
        "        self.fusion = FusionNetwork(self.num_generators, input_dim)\n",
        "        self.discriminator = Discriminator(input_dim)\n",
        "        # optimizers\n",
        "        self.opt_gen = optim.Adam(\n",
        "            list(self.generators.parameters()) + list(self.fusion.parameters()), lr=0.001\n",
        "        )\n",
        "        self.opt_disc = optim.Adam(self.discriminator.parameters(), lr=0.001)\n",
        "        # losses\n",
        "        self.bce = nn.BCELoss()\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # draw masks dynamically\n",
        "        masks = [(torch.rand_like(x) < 0.8).float() for _ in range(self.num_generators)]\n",
        "        outputs = [g(x, m) for g, m in zip(self.generators, masks)]\n",
        "        fused, gamma = self.fusion(outputs)\n",
        "        return outputs, fused, gamma\n",
        "\n",
        "    def train_meg(self, data, epochs=50, batch_size=64):\n",
        "        # 50 epochs for large datasets per paper\n",
        "        for epoch in range(epochs):\n",
        "            perm = torch.randperm(data.size(0))\n",
        "            for i in range(0, data.size(0), batch_size):\n",
        "                idx = perm[i:i + batch_size]\n",
        "                real_batch = data[idx]\n",
        "                bs = real_batch.size(0)\n",
        "                real_labels = torch.ones(bs, 1).to(real_batch.device)\n",
        "                fake_labels = torch.zeros(bs, 1).to(real_batch.device)\n",
        "\n",
        "                # Discriminator update\n",
        "                self.opt_disc.zero_grad()\n",
        "                loss_real = self.bce(self.discriminator(real_batch), real_labels)\n",
        "                _, fused, _ = self.forward(real_batch)\n",
        "                loss_fake = self.bce(self.discriminator(fused.detach()), fake_labels)\n",
        "                loss_D = loss_real + loss_fake\n",
        "                loss_D.backward()\n",
        "                self.opt_disc.step()\n",
        "\n",
        "                # Generator + Fusion update\n",
        "                self.opt_gen.zero_grad()\n",
        "                outputs, fused, gamma = self.forward(real_batch)\n",
        "                loss_proxy = self.mse(fused, real_batch)\n",
        "                loss_group = sum(\n",
        "                    torch.mean(w * (out - real_batch).pow(2))\n",
        "                    for out, w in zip(outputs, gamma)\n",
        "                )\n",
        "                loss_adv = self.bce(self.discriminator(fused), real_labels)\n",
        "                loss_G = loss_proxy + self.alpha * loss_group + self.beta * loss_adv\n",
        "                loss_G.backward()\n",
        "                self.opt_gen.step()\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(\n",
        "                    f\"Epoch {epoch}: \"\n",
        "                    f\"Loss_D={loss_D.item():.4f}, \"\n",
        "                    f\"Loss_proxy={loss_proxy.item():.4f}, \"\n",
        "                    f\"Loss_group={loss_group.item():.4f}, \"\n",
        "                    f\"Loss_adv={loss_adv.item():.4f}\"\n",
        "                )\n",
        "\n",
        "    def generate(self, x):\n",
        "        _, fused, _ = self.forward(x)\n",
        "        return fused\n",
        "\n",
        "# === Load Adult Train/Test Files Directly ===\n",
        "# Column names for UCI Adult dataset\n",
        "columns = [\n",
        "    'age','workclass','fnlwgt','education','education-num',\n",
        "    'marital-status','occupation','relationship','race','sex',\n",
        "    'capital-gain','capital-loss','hours-per-week','native-country','income'\n",
        "]\n",
        "# URLs for train and test\n",
        "t_train = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "t_test  = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
        "# Load data\n",
        "df_train = pd.read_csv(\n",
        "    t_train, header=None, names=columns, sep=', ', engine='python', na_values='?'\n",
        ")\n",
        "df_test = pd.read_csv(\n",
        "    t_test, header=0, names=columns, sep=', ', engine='python', na_values='?'\n",
        ")\n",
        "# Remove trailing dot in test labels\n",
        "if df_test['income'].dtype == object:\n",
        "    df_test['income'] = df_test['income'].str.rstrip('.')\n",
        "# Drop missing values\n",
        "df_train.dropna(inplace=True)\n",
        "df_test.dropna(inplace=True)\n",
        "\n",
        "# Original feature count (excluding target)\n",
        "orig_feature_count = df_train.shape[1] - 1\n",
        "# One-hot encode features only (exclude target) per paper\n",
        "feature_cols = df_train.columns.drop('income')\n",
        "# Dummies on training features\n",
        "df_train_enc = pd.get_dummies(df_train[feature_cols], drop_first=False)\n",
        "# Dummies on test features and align columns\n",
        "df_test_enc = pd.get_dummies(df_test[feature_cols], drop_first=False)\n",
        "df_test_enc = df_test_enc.reindex(columns=df_train_enc.columns, fill_value=0)\n",
        "\n",
        "# Separate features and label\n",
        "X_train = df_train_enc.values\n",
        "y_train = LabelEncoder().fit_transform(df_train['income'].values)\n",
        "X_test  = df_test_enc.values\n",
        "y_test  = LabelEncoder().fit_transform(df_test['income'].values)\n",
        "\n",
        "# Standardize numeric\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train_t = torch.tensor(scaler.transform(X_train), dtype=torch.float32)\n",
        "X_test_t  = torch.tensor(scaler.transform(X_test), dtype=torch.float32)\n",
        "\n",
        "# Train MEG on full training set\n",
        "meg = MEG(\n",
        "    input_dim=X_train_t.shape[1],\n",
        "    num_generators=orig_feature_count,\n",
        "    alpha=0.1,\n",
        "    beta=1.0\n",
        ")\n",
        "meg.train_meg(X_train_t, epochs=50, batch_size=64)\n",
        "\n",
        "# Generate synthetic training data\n",
        "X_syn = meg.generate(X_train_t).detach().cpu().numpy()\n",
        "y_syn = y_train\n",
        "\n",
        "# Evaluate classifiers on real test set\n",
        "clfs = {\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'MLP': MLPClassifier(hidden_layer_sizes=(100,50), max_iter=500, random_state=42)\n",
        "}\n",
        "results = {}\n",
        "for name, clf in clfs.items():\n",
        "    clf.fit(X_syn, y_syn)\n",
        "    preds = clf.predict(X_test)\n",
        "    results[name] = accuracy_score(y_test, preds)\n",
        "\n",
        "print(\" TSTR Results on Adult (train/test files):\")\n",
        "for name, acc in results.items():\n",
        "    print(f\"{name}: {acc:.4f}\")\n"
      ]
    }
  ]
}