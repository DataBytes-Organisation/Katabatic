{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"elapsed":9973,"status":"ok","timestamp":1744785457548,"user":{"displayName":"Abhinav Tripathi","userId":"00702766975243664923"},"user_tz":-600},"id":"EuHCMoNEHZS9","outputId":"e8cf6fd6-c73d-439f-a575-296839354d3f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-f915d9a2-de37-4705-bc79-0ef0b644a4f1\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-f915d9a2-de37-4705-bc79-0ef0b644a4f1\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving car.csv to car.csv\n"]}],"source":["from google.colab import files\n","uploaded = files.upload()\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"INZ-ucWeHgLw","executionInfo":{"status":"ok","timestamp":1744785733188,"user_tz":-600,"elapsed":75031,"user":{"displayName":"Abhinav Tripathi","userId":"00702766975243664923"}},"outputId":"368c3dda-bf3f-4893-a857-cc2839b12b48"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded car.csv | Shape: (1728, 7)\n","KGE matrix created\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/100 | Loss: 3.0264\n","Epoch 2/100 | Loss: 0.6101\n","Epoch 3/100 | Loss: 0.4337\n","Epoch 4/100 | Loss: 0.3737\n","Epoch 5/100 | Loss: 0.3028\n","Epoch 6/100 | Loss: 0.2636\n","Epoch 7/100 | Loss: 0.2384\n","Epoch 8/100 | Loss: 0.2081\n","Epoch 9/100 | Loss: 0.2023\n","Epoch 10/100 | Loss: 0.1941\n","Epoch 11/100 | Loss: 0.1795\n","Epoch 12/100 | Loss: 0.1602\n","Epoch 13/100 | Loss: 0.1461\n","Epoch 14/100 | Loss: 0.1605\n","Epoch 15/100 | Loss: 0.1395\n","Epoch 16/100 | Loss: 0.1335\n","Epoch 17/100 | Loss: 0.1232\n","Epoch 18/100 | Loss: 0.1213\n","Epoch 19/100 | Loss: 0.1042\n","Epoch 20/100 | Loss: 0.1158\n","Epoch 21/100 | Loss: 0.0910\n","Epoch 22/100 | Loss: 0.0792\n","Epoch 23/100 | Loss: 0.0779\n","Epoch 24/100 | Loss: 0.0706\n","Epoch 25/100 | Loss: 0.0605\n","Epoch 26/100 | Loss: 0.0581\n","Epoch 27/100 | Loss: 0.0547\n","Epoch 28/100 | Loss: 0.0576\n","Epoch 29/100 | Loss: 0.0508\n","Epoch 30/100 | Loss: 0.0471\n","Epoch 31/100 | Loss: 0.0411\n","Epoch 32/100 | Loss: 0.0400\n","Epoch 33/100 | Loss: 0.0354\n","Epoch 34/100 | Loss: 0.0350\n","Epoch 35/100 | Loss: 0.0325\n","Epoch 36/100 | Loss: 0.0314\n","Epoch 37/100 | Loss: 0.0309\n","Epoch 38/100 | Loss: 0.0296\n","Epoch 39/100 | Loss: 0.0263\n","Epoch 40/100 | Loss: 0.0254\n","Epoch 41/100 | Loss: 0.0257\n","Epoch 42/100 | Loss: 0.0252\n","Epoch 43/100 | Loss: 0.0260\n","Epoch 44/100 | Loss: 0.0245\n","Epoch 45/100 | Loss: 0.0215\n","Epoch 46/100 | Loss: 0.0203\n","Epoch 47/100 | Loss: 0.0193\n","Epoch 48/100 | Loss: 0.0205\n","Epoch 49/100 | Loss: 0.0198\n","Epoch 50/100 | Loss: 0.0176\n","Epoch 51/100 | Loss: 0.0165\n","Epoch 52/100 | Loss: 0.0163\n","Epoch 53/100 | Loss: 0.0159\n","Epoch 54/100 | Loss: 0.0151\n","Epoch 55/100 | Loss: 0.0145\n","Epoch 56/100 | Loss: 0.0128\n","Epoch 57/100 | Loss: 0.0129\n","Epoch 58/100 | Loss: 0.0147\n","Epoch 59/100 | Loss: 0.0140\n","Epoch 60/100 | Loss: 0.0135\n","Epoch 61/100 | Loss: 0.0122\n","Epoch 62/100 | Loss: 0.0127\n","Epoch 63/100 | Loss: 0.0129\n","Epoch 64/100 | Loss: 0.0122\n","Epoch 65/100 | Loss: 0.0115\n","Epoch 66/100 | Loss: 0.0103\n","Epoch 67/100 | Loss: 0.0115\n","Epoch 68/100 | Loss: 0.0094\n","Epoch 69/100 | Loss: 0.0096\n","Epoch 70/100 | Loss: 0.0095\n","Epoch 71/100 | Loss: 0.0091\n","Epoch 72/100 | Loss: 0.0116\n","Epoch 73/100 | Loss: 0.0095\n","Epoch 74/100 | Loss: 0.0102\n","Epoch 75/100 | Loss: 0.0113\n","Epoch 76/100 | Loss: 0.0109\n","Epoch 77/100 | Loss: 0.0099\n","Epoch 78/100 | Loss: 0.0092\n","Epoch 79/100 | Loss: 0.0081\n","Epoch 80/100 | Loss: 0.0086\n","Epoch 81/100 | Loss: 0.0086\n","Epoch 82/100 | Loss: 0.0084\n","Epoch 83/100 | Loss: 0.0079\n","Epoch 84/100 | Loss: 0.0077\n","Epoch 85/100 | Loss: 0.0081\n","Epoch 86/100 | Loss: 0.0071\n","Epoch 87/100 | Loss: 0.0068\n","Epoch 88/100 | Loss: 0.0072\n","Epoch 89/100 | Loss: 0.0071\n","Epoch 90/100 | Loss: 0.0068\n","Epoch 91/100 | Loss: 0.0077\n","Epoch 92/100 | Loss: 0.0077\n","Epoch 93/100 | Loss: 0.0070\n","Epoch 94/100 | Loss: 0.0074\n","Epoch 95/100 | Loss: 0.0068\n","Epoch 96/100 | Loss: 0.0058\n","Epoch 97/100 | Loss: 0.0054\n","Epoch 98/100 | Loss: 0.0054\n","Epoch 99/100 | Loss: 0.0061\n","Epoch 100/100 | Loss: 0.0071\n","Synthetic data shape: (1382, 6)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["TSTR Benchmark (Train on Synthetic, Test on Real):\n","                     Accuracy  Precision    Recall  F1 Score\n","Logistic Regression  0.699422   0.489191  0.699422  0.575715\n","MLP                  0.684971   0.486109  0.684971  0.568655\n","Random Forest        0.699422   0.584441  0.699422  0.596458\n","XGBoost              0.656069   0.563622  0.656069  0.600058\n"]}],"source":["# Install required libraries\n","!pip install xgboost --quiet\n","\n","# Imports\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import networkx as nx\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neural_network import MLPClassifier\n","from xgboost import XGBClassifier\n","from torch.utils.data import DataLoader, TensorDataset\n","from random import choice\n","\n","# Load dataset\n","filename = list(uploaded.keys())[0]\n","df = pd.read_csv(filename)\n","print(f\"Loaded {filename} | Shape: {df.shape}\")\n","\n","# Target column\n","target_col = df.columns[-1]\n","\n","# Detect categorical columns\n","cat_cols = [col for col in df.select_dtypes(include='object').columns if col != target_col]\n","if len(cat_cols) == 0:\n","    cat_cols = [col for col in df.columns if col != target_col and df[col].nunique() <= 15]\n","    if len(cat_cols) > 0:\n","        print(f\"Inferred categorical columns: {cat_cols}\")\n","    else:\n","        print(\"No categorical columns found, proceeding without KGE\")\n","        cat_cols = []\n","\n","# Label encoding\n","encoders = {}\n","for col in cat_cols + [target_col]:\n","    le = LabelEncoder()\n","    df[col] = le.fit_transform(df[col].astype(str))\n","    encoders[col] = le\n","\n","# Feature/target split\n","X = df.drop(columns=[target_col])\n","y = df[target_col]\n","\n","# Split into train and test (80/20)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n","\n","# Scale data\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# KGE Matrix builder\n","def build_kge_matrix(df, cat_cols, embedding_dim=64, walk_length=10, num_walks=20, window=3):\n","    if not cat_cols:\n","        print(\"KGE skipped due to no categorical features\")\n","        return np.zeros((len(df), embedding_dim))\n","\n","    G = nx.Graph()\n","    for col in cat_cols:\n","        for val in df[col].unique():\n","            G.add_node(f\"{col}:{int(val)}\")\n","\n","    for _, row in df[cat_cols].iterrows():\n","        for i in range(len(cat_cols)):\n","            for j in range(i + 1, len(cat_cols)):\n","                G.add_edge(f\"{cat_cols[i]}:{int(row[cat_cols[i]])}\", f\"{cat_cols[j]}:{int(row[cat_cols[j]])}\")\n","\n","    def random_walk(g, start, wl):\n","        walk = [start]\n","        for _ in range(wl - 1):\n","            nbrs = list(g.neighbors(walk[-1]))\n","            walk.append(choice(nbrs) if nbrs else walk[-1])\n","        return walk\n","\n","    vocab = list(G.nodes())\n","    vocab_index = {n: i for i, n in enumerate(vocab)}\n","    co_matrix = np.zeros((len(vocab), len(vocab)))\n","\n","    for node in vocab:\n","        for _ in range(num_walks):\n","            walk = random_walk(G, node, walk_length)\n","            for i, tgt in enumerate(walk):\n","                for j in range(max(0, i - window), min(len(walk), i + window + 1)):\n","                    if i != j:\n","                        co_matrix[vocab_index[tgt], vocab_index[walk[j]]] += 1\n","\n","    safe_dim = min(embedding_dim, min(co_matrix.shape))\n","    if safe_dim == 0:\n","        print(\"PCA skipped, returning zero matrix\")\n","        return np.zeros((len(df), embedding_dim))\n","\n","    embeddings = PCA(n_components=safe_dim).fit_transform(co_matrix)\n","\n","    row_kges = []\n","    for _, row in df.iterrows():\n","        vecs = []\n","        for col in cat_cols:\n","            node = f\"{col}:{int(row[col])}\"\n","            vecs.append(embeddings[vocab_index.get(node, 0)])\n","        avg_vec = np.mean(vecs, axis=0)\n","        if safe_dim < embedding_dim:\n","            avg_vec = np.pad(avg_vec, (0, embedding_dim - safe_dim))\n","        row_kges.append(avg_vec)\n","\n","    print(\"KGE matrix created\")\n","    return np.array(row_kges)\n","\n","# Build KGE from training data\n","train_df = X_train.copy()\n","train_df[target_col] = y_train\n","kge_matrix = build_kge_matrix(train_df, cat_cols, embedding_dim=64)\n","\n","# Define MEG architecture\n","class MappingNetwork(nn.Module):\n","    def __init__(self, kge_dim, embed_dim):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(kge_dim, embed_dim),\n","            nn.ReLU(),\n","            nn.Linear(embed_dim, embed_dim)\n","        )\n","    def forward(self, kge):\n","        return self.net(kge)\n","\n","class OriginalMEG(nn.Module):\n","    def __init__(self, input_dim, kge_dim, embed_dim):\n","        super().__init__()\n","        self.token_embed = nn.Linear(input_dim, embed_dim)\n","        self.kge_mapper = MappingNetwork(kge_dim, embed_dim)\n","        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4)\n","        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n","        self.decoder = nn.Sequential(\n","            nn.Linear(embed_dim, embed_dim),\n","            nn.ReLU(),\n","            nn.Linear(embed_dim, input_dim)\n","        )\n","    def forward(self, x, kge):\n","        x_embed = self.token_embed(x)\n","        kge_embed = self.kge_mapper(kge)\n","        fused = x_embed + kge_embed\n","        encoded = self.encoder(fused.unsqueeze(0)).squeeze(0)\n","        return self.decoder(encoded)\n","\n","# Prepare tensors\n","X_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n","KGE_tensor = torch.tensor(kge_matrix, dtype=torch.float32)\n","\n","dataset = TensorDataset(X_tensor, KGE_tensor)\n","loader = DataLoader(dataset, batch_size=64, shuffle=True)\n","\n","# Train MEG\n","meg = OriginalMEG(input_dim=X_tensor.shape[1], kge_dim=KGE_tensor.shape[1], embed_dim=128)\n","opt = torch.optim.Adam(meg.parameters(), lr=0.001)\n","loss_fn = nn.MSELoss()\n","\n","meg.train()\n","for epoch in range(100):\n","    total = 0\n","    for xb, kb in loader:\n","        out = meg(xb, kb)\n","        loss = loss_fn(out, xb)\n","        opt.zero_grad()\n","        loss.backward()\n","        opt.step()\n","        total += loss.item()\n","    print(f\"Epoch {epoch+1}/100 | Loss: {total:.4f}\")\n","\n","# Generate synthetic data\n","meg.eval()\n","with torch.no_grad():\n","    noise = torch.randn((len(X_train), X_tensor.shape[1]))\n","    kge_sample = KGE_tensor[torch.randint(0, len(KGE_tensor), (len(X_train),))]\n","    synthetic = meg(noise, kge_sample)\n","\n","X_synth = synthetic.numpy()\n","y_synth = y_train.to_numpy()\n","print(\"Synthetic data shape:\", X_synth.shape)\n","\n","# TSTR Evaluation: Train classifiers on synthetic, test on real\n","models = {\n","    \"Logistic Regression\": LogisticRegression(max_iter=500),\n","    \"MLP\": MLPClassifier(max_iter=300),\n","    \"Random Forest\": RandomForestClassifier(),\n","    \"XGBoost\": XGBClassifier(eval_metric='logloss')\n","}\n","\n","results = {}\n","for name, clf in models.items():\n","    try:\n","        clf.fit(X_synth, y_synth)\n","        y_pred = clf.predict(X_test_scaled)\n","        results[name] = {\n","            \"Accuracy\": accuracy_score(y_test, y_pred),\n","            \"Precision\": precision_score(y_test, y_pred, average='weighted', zero_division=0),\n","            \"Recall\": recall_score(y_test, y_pred, average='weighted', zero_division=0),\n","            \"F1 Score\": f1_score(y_test, y_pred, average='weighted', zero_division=0)\n","        }\n","    except Exception as e:\n","        results[name] = {\"Error\": str(e)}\n","\n","# Display results\n","results_df = pd.DataFrame(results).T\n","print(\"TSTR Benchmark (Train on Synthetic, Test on Real):\")\n","print(results_df)\n","\n","# Save synthetic data\n","synth_df = pd.DataFrame(X_synth, columns=X.columns)\n","synth_df[target_col] = y_synth\n","synth_df.to_csv(\"synthetic_meg_output.csv\", index=False)\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPywewps0XZZRmzg2mm2wyw"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}