{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO2jVawk1DnYJgyLM4/HKmJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["%pip install xgboost\n","\n","import numpy as np\n","import pandas as pd\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, classification_report\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","from scipy.stats import entropy\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","import time\n","from tqdm import tqdm\n","from google.colab import drive\n","\n","drive.mount('/content/drive/')\n","%cd /content/drive/MyDrive/Colab Notebooks/Katabatic/MedGAN/poker/\n","\n","# Suppress warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Setting random seeds for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","# Check if CUDA is available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zvJ-9Q5mIEo","executionInfo":{"status":"ok","timestamp":1743855307622,"user_tz":-660,"elapsed":14744,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}},"outputId":"53339c21-7e9d-40ba-a6d9-6a1e2468fbe4"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.14.1)\n","Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n","/content/drive/MyDrive/Colab Notebooks/Katabatic/MedGAN/poker\n","Using device: cpu\n"]}]},{"cell_type":"code","execution_count":30,"metadata":{"id":"5YbcHeMdlywW","executionInfo":{"status":"ok","timestamp":1743855307765,"user_tz":-660,"elapsed":136,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}}},"outputs":[],"source":["# Load and preprocess data\n","def load_poker_data(train_path, test_path):\n","    # Column names based on the dataset description\n","    column_names = ['S1', 'C1', 'S2', 'C2', 'S3', 'C3', 'S4', 'C4', 'S5', 'C5', 'CLASS']\n","\n","    # Load data\n","    df_train = pd.read_csv(train_path, header=None, names=column_names)\n","    df_test = pd.read_csv(test_path, header=None, names=column_names)\n","\n","    print(f\"Training data shape: {df_train.shape}\")\n","    print(f\"Testing data shape: {df_test.shape}\")\n","\n","    # Split features and target\n","    X_train = df_train.drop('CLASS', axis=1)\n","    y_train = df_train['CLASS']\n","    X_test = df_test.drop('CLASS', axis=1)\n","    y_test = df_test['CLASS']\n","\n","    # Create preprocessing pipeline - for poker hands, we'll use one-hot encoding for suits\n","    # and normalize the card ranks\n","\n","    # Define suit columns and rank columns\n","    suit_cols = ['S1', 'S2', 'S3', 'S4', 'S5']\n","    rank_cols = ['C1', 'C2', 'C3', 'C4', 'C5']\n","\n","    # Create transformers\n","    suit_transformer = Pipeline(steps=[\n","        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n","    ])\n","\n","    rank_transformer = Pipeline(steps=[\n","        ('scaler', StandardScaler())\n","    ])\n","\n","    preprocessor = ColumnTransformer(\n","        transformers=[\n","            ('suit', suit_transformer, suit_cols),\n","            ('rank', rank_transformer, rank_cols)\n","        ])\n","\n","    # Fit and transform the data\n","    X_train_transformed = preprocessor.fit_transform(X_train)\n","    X_test_transformed = preprocessor.transform(X_test)\n","\n","    # Get feature names\n","    suit_feature_names = preprocessor.named_transformers_['suit'].named_steps['onehot'].get_feature_names_out(suit_cols)\n","    all_feature_names = list(suit_feature_names) + list(rank_cols)\n","\n","    print(f\"Processed training data shape: {X_train_transformed.shape}\")\n","    print(f\"Processed testing data shape: {X_test_transformed.shape}\")\n","\n","    return X_train_transformed, y_train, X_test_transformed, y_test, preprocessor, all_feature_names\n","\n","# Custom dataset class\n","class PokerDataset(Dataset):\n","    def __init__(self, features, labels=None):\n","        self.features = torch.tensor(features, dtype=torch.float32)\n","        if labels is not None:\n","            self.labels = torch.tensor(labels, dtype=torch.float32).view(-1, 1)\n","        else:\n","            self.labels = None\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, idx):\n","        if self.labels is not None:\n","            return self.features[idx], self.labels[idx]\n","        else:\n","            return self.features[idx]\n","\n","# Autoencoder for MedGAN\n","class Autoencoder(nn.Module):\n","    def __init__(self, input_dim, encoding_dim=128):\n","        super(Autoencoder, self).__init__()\n","\n","        # Encoder\n","        self.encoder = nn.Sequential(\n","            nn.Linear(input_dim, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(),\n","            nn.Linear(512, 256),\n","            nn.BatchNorm1d(256),\n","            nn.ReLU(),\n","            nn.Linear(256, encoding_dim),\n","            nn.BatchNorm1d(encoding_dim),\n","            nn.ReLU()\n","        )\n","\n","        # Decoder\n","        self.decoder = nn.Sequential(\n","            nn.Linear(encoding_dim, 256),\n","            nn.BatchNorm1d(256),\n","            nn.ReLU(),\n","            nn.Linear(256, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(),\n","            nn.Linear(512, input_dim),\n","            nn.Tanh()  # Output range (-1, 1)\n","        )\n","\n","    def forward(self, x):\n","        encoded = self.encoder(x)\n","        decoded = self.decoder(encoded)\n","        return decoded\n","\n","    def encode(self, x):\n","        return self.encoder(x)\n","\n","# Generator for MedGAN\n","class Generator(nn.Module):\n","    def __init__(self, latent_dim, hidden_dim=128, output_dim=128):\n","        super(Generator, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(latent_dim, hidden_dim),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.ReLU(),\n","\n","            nn.Linear(hidden_dim, hidden_dim*2),\n","            nn.BatchNorm1d(hidden_dim*2),\n","            nn.ReLU(),\n","\n","            nn.Linear(hidden_dim*2, output_dim),\n","            nn.Tanh()  # Output in range (-1, 1)\n","        )\n","\n","    def forward(self, z):\n","        return self.model(z)\n","\n","# Discriminator for MedGAN\n","class Discriminator(nn.Module):\n","    def __init__(self, input_dim):\n","        super(Discriminator, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(input_dim, 256),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(256, 128),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(128, 64),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(64, 1),\n","            nn.Sigmoid()  # Output probability\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# MedGAN Implementation\n","class MedGAN:\n","    def __init__(self, data_dim, encoding_dim=128, latent_dim=128,\n","                 batch_size=128, autoencoder_epochs=100):\n","        self.data_dim = data_dim\n","        self.encoding_dim = encoding_dim\n","        self.latent_dim = latent_dim\n","        self.batch_size = batch_size\n","\n","        # Initialize networks\n","        self.autoencoder = Autoencoder(data_dim, encoding_dim).to(device)\n","        self.generator = Generator(latent_dim, hidden_dim=256, output_dim=encoding_dim).to(device)\n","        self.discriminator = Discriminator(data_dim).to(device)\n","\n","        # Setup optimizers\n","        self.ae_optimizer = optim.Adam(self.autoencoder.parameters(), lr=0.001)\n","        self.g_optimizer = optim.Adam(self.generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","        self.d_optimizer = optim.Adam(self.discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","        # Loss functions\n","        self.ae_criterion = nn.MSELoss()\n","        self.gan_criterion = nn.BCELoss()\n","\n","        # For tracking progress\n","        self.ae_losses = []\n","        self.g_losses = []\n","        self.d_losses = []\n","        self.autoencoder_epochs = autoencoder_epochs\n","\n","    def pretrain_autoencoder(self, data_loader, epochs=None):\n","        \"\"\"Pretrain the autoencoder\"\"\"\n","        if epochs is None:\n","            epochs = self.autoencoder_epochs\n","\n","        print(f\"Pretraining autoencoder for {epochs} epochs...\")\n","\n","        self.autoencoder.train()\n","        for epoch in range(epochs):\n","            epoch_loss = 0\n","            num_batches = 0\n","\n","            for real_data, _ in data_loader:\n","                real_data = real_data.to(device)\n","\n","                # Forward pass\n","                reconstructed = self.autoencoder(real_data)\n","                loss = self.ae_criterion(reconstructed, real_data)\n","\n","                # Backward pass\n","                self.ae_optimizer.zero_grad()\n","                loss.backward()\n","                self.ae_optimizer.step()\n","\n","                epoch_loss += loss.item()\n","                num_batches += 1\n","\n","            avg_loss = epoch_loss / num_batches\n","            self.ae_losses.append(avg_loss)\n","\n","            if (epoch + 1) % 10 == 0 or epoch == 0:\n","                print(f\"Autoencoder Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.6f}\")\n","\n","        print(\"Autoencoder pretraining complete!\")\n","\n","    def train_gan(self, data_loader, epochs=100, save_interval=10):\n","        \"\"\"Train the GAN after pretraining the autoencoder\"\"\"\n","        print(f\"Training MedGAN for {epochs} epochs...\")\n","\n","        # Prepare labels for real and fake data\n","        real_label = 1.0\n","        fake_label = 0.0\n","\n","        for epoch in range(epochs):\n","            d_loss_total = 0\n","            g_loss_total = 0\n","            num_batches = 0\n","\n","            for real_data, _ in data_loader:\n","                batch_size = real_data.size(0)\n","                real_data = real_data.to(device)\n","\n","                # ---------------------\n","                # Train Discriminator\n","                # ---------------------\n","                self.d_optimizer.zero_grad()\n","\n","                # Real data\n","                real_output = self.discriminator(real_data)\n","                d_real_loss = self.gan_criterion(\n","                    real_output,\n","                    torch.full((batch_size, 1), real_label, device=device)\n","                )\n","\n","                # Fake data\n","                noise = torch.randn(batch_size, self.latent_dim, device=device)\n","                fake_encoded = self.generator(noise)\n","                fake_data = self.autoencoder.decoder(fake_encoded)\n","                fake_output = self.discriminator(fake_data.detach())\n","                d_fake_loss = self.gan_criterion(\n","                    fake_output,\n","                    torch.full((batch_size, 1), fake_label, device=device)\n","                )\n","\n","                # Combined loss\n","                d_loss = d_real_loss + d_fake_loss\n","                d_loss.backward()\n","                self.d_optimizer.step()\n","\n","                # ---------------------\n","                # Train Generator\n","                # ---------------------\n","                self.g_optimizer.zero_grad()\n","\n","                # Generate new fake data\n","                noise = torch.randn(batch_size, self.latent_dim, device=device)\n","                fake_encoded = self.generator(noise)\n","                fake_data = self.autoencoder.decoder(fake_encoded)\n","                fake_output = self.discriminator(fake_data)\n","\n","                # Generator loss\n","                g_loss = self.gan_criterion(\n","                    fake_output,\n","                    torch.full((batch_size, 1), real_label, device=device)\n","                )\n","                g_loss.backward()\n","                self.g_optimizer.step()\n","\n","                d_loss_total += d_loss.item()\n","                g_loss_total += g_loss.item()\n","                num_batches += 1\n","\n","            # Calculate average losses\n","            avg_d_loss = d_loss_total / num_batches\n","            avg_g_loss = g_loss_total / num_batches\n","\n","            self.d_losses.append(avg_d_loss)\n","            self.g_losses.append(avg_g_loss)\n","\n","            if (epoch + 1) % save_interval == 0 or epoch == 0:\n","                print(f\"GAN Epoch {epoch+1}/{epochs} | D Loss: {avg_d_loss:.6f} | G Loss: {avg_g_loss:.6f}\")\n","\n","        print(\"MedGAN training complete!\")\n","\n","    def generate_samples(self, num_samples):\n","        \"\"\"Generate synthetic samples\"\"\"\n","        self.generator.eval()\n","        self.autoencoder.eval()\n","\n","        batches = []\n","        remaining = num_samples\n","\n","        while remaining > 0:\n","            batch_size = min(remaining, self.batch_size)\n","            noise = torch.randn(batch_size, self.latent_dim, device=device)\n","\n","            with torch.no_grad():\n","                fake_encoded = self.generator(noise)\n","                fake_data = self.autoencoder.decoder(fake_encoded)\n","\n","            batches.append(fake_data.cpu().numpy())\n","            remaining -= batch_size\n","\n","        synthetic_data = np.vstack(batches)\n","\n","        return synthetic_data\n","\n","    def save_model(self, path):\n","        \"\"\"Save trained models\"\"\"\n","        torch.save({\n","            'autoencoder_state_dict': self.autoencoder.state_dict(),\n","            'generator_state_dict': self.generator.state_dict(),\n","            'discriminator_state_dict': self.discriminator.state_dict(),\n","            'ae_optimizer_state_dict': self.ae_optimizer.state_dict(),\n","            'g_optimizer_state_dict': self.g_optimizer.state_dict(),\n","            'd_optimizer_state_dict': self.d_optimizer.state_dict(),\n","        }, path)\n","\n","    def load_model(self, path):\n","        \"\"\"Load trained models\"\"\"\n","        checkpoint = torch.load(path)\n","        self.autoencoder.load_state_dict(checkpoint['autoencoder_state_dict'])\n","        self.generator.load_state_dict(checkpoint['generator_state_dict'])\n","        self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n","        self.ae_optimizer.load_state_dict(checkpoint['ae_optimizer_state_dict'])\n","        self.g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n","        self.d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n","\n","# Post-process generated data to make it valid poker hands\n","def post_process_poker_data(synthetic_data, preprocessor):\n","    \"\"\"\n","    Post-process generated data to ensure it can be translated back into valid poker hands\n","    \"\"\"\n","    # Get the number of suit features (one-hot encoded)\n","    n_suit_features = len(preprocessor.named_transformers_['suit'].named_steps['onehot'].get_feature_names_out(['S1', 'S2', 'S3', 'S4', 'S5']))\n","\n","    # Split the synthetic data into suits and ranks\n","    synthetic_suits = synthetic_data[:, :n_suit_features]\n","    synthetic_ranks = synthetic_data[:, n_suit_features:]\n","\n","    # For each card's suit (one-hot encoded), take the max value to get the most likely suit\n","    num_cards = 5\n","    n_suits = 4  # Hearts, Spades, Diamonds, Clubs\n","\n","    processed_suits = np.zeros((len(synthetic_data), num_cards))\n","\n","    for i in range(num_cards):\n","        one_hot_indices = np.argmax(synthetic_suits[:, i*n_suits:(i+1)*n_suits], axis=1)\n","        processed_suits[:, i] = one_hot_indices + 1  # Add 1 to match original encoding (1-4)\n","\n","    # Inverse transform the rank data\n","    processed_ranks = preprocessor.named_transformers_['rank'].named_steps['scaler'].inverse_transform(synthetic_ranks)\n","\n","    # Clip and round ranks to valid values (1-13)\n","    processed_ranks = np.clip(np.round(processed_ranks), 1, 13)\n","\n","    # Combine suits and ranks\n","    processed_data = np.zeros((len(synthetic_data), num_cards * 2))\n","\n","    for i in range(num_cards):\n","        processed_data[:, 2*i] = processed_suits[:, i]       # Suit\n","        processed_data[:, 2*i+1] = processed_ranks[:, i]     # Rank\n","\n","    return processed_data\n","\n"]},{"cell_type":"code","source":["# Evaluation Metrics\n","\n","# 1. Machine Learning Utility (TSTR)\n","def evaluate_tstr(real_data, synthetic_data, real_labels, random_state=42):\n","    \"\"\"\n","    Train classifiers on synthetic data and test on real data (TSTR)\n","    Returns accuracy for each classifier\n","    \"\"\"\n","    # Train-test split for real data\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        real_data, real_labels, test_size=0.2, random_state=random_state\n","    )\n","\n","    # Synthetic data (all used for training)\n","    X_synth = synthetic_data\n","\n","    # Ensure proper dimensions for labels\n","    if isinstance(y_train, pd.Series):\n","        y_train = y_train.values\n","\n","    # Create synthetic labels based on real distribution\n","    # For poker hands, we need to represent the distribution of classes (0-9)\n","    class_distribution = np.bincount(y_test.astype(int), minlength=10).astype(float)\n","    class_distribution += 1e-6  # Add small value to ensure all classes have a non-zero probability\n","    class_distribution /= class_distribution.sum()  # Normalize\n","\n","    # Sample synthetic labels using this adjusted distribution\n","    np.random.seed(random_state)\n","    y_synth = np.random.choice(range(10), size=len(X_synth), p=class_distribution)\n","\n","    # Define classifiers\n","    classifiers = {\n","        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=random_state),\n","        'MLP': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=random_state),\n","        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=random_state),\n","        'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=random_state)\n","    }\n","\n","    results = {}\n","\n","    for name, clf in classifiers.items():\n","        # Train on synthetic data\n","        clf.fit(X_synth, y_synth)\n","\n","        # Test on real data\n","        y_pred = clf.predict(X_test)\n","\n","        # Calculate metrics\n","        accuracy = accuracy_score(y_test, y_pred)\n","\n","        # We use weighted f1 since poker hands dataset is heavily imbalanced\n","        f1 = f1_score(y_test, y_pred, average='weighted')\n","\n","        results[name] = {\n","            'accuracy': accuracy,\n","            'f1_score': f1\n","        }\n","\n","    return results\n","\n","# 2. Statistical Similarity\n","def jensen_shannon_divergence(p, q):\n","    \"\"\"\n","    Calculate Jensen-Shannon Divergence between distributions p and q\n","    \"\"\"\n","    # Ensure p and q are normalized\n","    p = p / np.sum(p)\n","    q = q / np.sum(q)\n","\n","    m = 0.5 * (p + q)\n","\n","    # Calculate JSD\n","    jsd = 0.5 * (entropy(p, m) + entropy(q, m))\n","\n","    return jsd\n","\n","def wasserstein_distance(p, q):\n","    \"\"\"\n","    Calculate 1D Wasserstein distance (Earth Mover's Distance)\n","    \"\"\"\n","    from scipy.stats import wasserstein_distance\n","\n","    return wasserstein_distance(p, q)\n","\n","def evaluate_statistical_similarity(real_data, synthetic_data, feature_names):\n","    \"\"\"\n","    Calculate statistical similarity metrics between real and synthetic data\n","    \"\"\"\n","    results = {'JSD': {}, 'WD': {}}\n","\n","    # Calculate metrics for each feature\n","    for i in range(real_data.shape[1]):\n","        feature_name = feature_names[i] if i < len(feature_names) else f\"feature_{i}\"\n","\n","        # Get feature values\n","        real_values = real_data[:, i]\n","        synth_values = synthetic_data[:, i]\n","\n","        # Calculate histogram (discrete distribution)\n","        hist_bins = min(50, len(np.unique(real_values)))\n","\n","        hist_real, bin_edges = np.histogram(real_values, bins=hist_bins, density=True)\n","        hist_synth, _ = np.histogram(synth_values, bins=bin_edges, density=True)\n","\n","        # Add a small epsilon to avoid division by zero\n","        epsilon = 1e-10\n","        hist_real = hist_real + epsilon\n","        hist_synth = hist_synth + epsilon\n","\n","        # Calculate JSD\n","        jsd = jensen_shannon_divergence(hist_real, hist_synth)\n","        results['JSD'][feature_name] = jsd\n","\n","        # Calculate Wasserstein Distance\n","        wd = wasserstein_distance(real_values, synth_values)\n","        results['WD'][feature_name] = wd\n","\n","    # Calculate average metrics\n","    results['JSD_avg'] = np.mean(list(results['JSD'].values()))\n","    results['WD_avg'] = np.mean(list(results['WD'].values()))\n","\n","    return results\n","\n","def plot_loss_curves(model):\n","    \"\"\"\n","    Plot the loss curves for the autoencoder, generator, and discriminator\n","    \"\"\"\n","    plt.figure(figsize=(12, 8))\n","\n","    # Plot autoencoder loss\n","    plt.subplot(2, 1, 1)\n","    plt.plot(model.ae_losses, label='Autoencoder Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.title('MedGAN Autoencoder Training Loss')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    # Plot GAN losses\n","    plt.subplot(2, 1, 2)\n","    plt.plot(model.g_losses, label='Generator Loss')\n","    plt.plot(model.d_losses, label='Discriminator Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.title('MedGAN Adversarial Training Loss')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    plt.tight_layout()\n","    plt.savefig('poker_medgan_loss_curves.png')\n","    plt.close()\n","\n","def plot_feature_distributions(real_data, synthetic_data, feature_names, n_features=10):\n","    \"\"\"\n","    Plot distributions of real vs synthetic data for selected features\n","    \"\"\"\n","    if n_features > len(feature_names):\n","        n_features = len(feature_names)\n","\n","    # Select a subset of features to visualize\n","    selected_indices = np.random.choice(range(len(feature_names)), size=n_features, replace=False)\n","\n","    plt.figure(figsize=(15, 20))\n","    for i, idx in enumerate(selected_indices):\n","        feature_name = feature_names[idx]\n","\n","        plt.subplot(n_features, 1, i+1)\n","\n","        # Get feature values\n","        real_values = real_data[:, idx]\n","        synth_values = synthetic_data[:, idx]\n","\n","        # Plot histograms\n","        sns.histplot(real_values, kde=True, stat=\"density\", label=\"Real\", alpha=0.6, color=\"blue\")\n","        sns.histplot(synth_values, kde=True, stat=\"density\", label=\"Synthetic\", alpha=0.6, color=\"red\")\n","\n","        plt.title(f\"Distribution for {feature_name}\")\n","        plt.legend()\n","\n","    plt.tight_layout()\n","    plt.savefig('poker_feature_distributions.png')\n","    plt.close()\n","\n","def plot_class_distribution(real_labels, synthetic_labels):\n","    \"\"\"\n","    Plot the class distribution of real vs synthetic data\n","    \"\"\"\n","    plt.figure(figsize=(12, 6))\n","\n","    # Get class counts\n","    real_class_counts = np.bincount(real_labels.astype(int), minlength=10)\n","    synth_class_counts = np.bincount(synthetic_labels.astype(int), minlength=10)\n","\n","    # Normalize\n","    real_class_dist = real_class_counts / np.sum(real_class_counts)\n","    synth_class_dist = synth_class_counts / np.sum(synth_class_counts)\n","\n","    # Define class names\n","    class_names = [\n","        'Nothing in hand',\n","        'One pair',\n","        'Two pairs',\n","        'Three of a kind',\n","        'Straight',\n","        'Flush',\n","        'Full house',\n","        'Four of a kind',\n","        'Straight flush',\n","        'Royal flush'\n","    ]\n","\n","    # Plot\n","    bar_width = 0.35\n","    x = np.arange(10)\n","\n","    plt.bar(x - bar_width/2, real_class_dist, bar_width, label='Real Data', color='blue', alpha=0.7)\n","    plt.bar(x + bar_width/2, synth_class_dist, bar_width, label='Synthetic Data', color='red', alpha=0.7)\n","\n","    plt.xlabel('Poker Hand')\n","    plt.ylabel('Proportion')\n","    plt.title('Class Distribution: Real vs Synthetic Poker Hands')\n","    plt.xticks(x, class_names, rotation=45, ha='right')\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.savefig('poker_class_distribution.png')\n","    plt.close()\n","\n","# Main function\n","def main():\n","    # File paths\n","    train_path = 'data/poker-hand-training-true.csv'\n","    test_path = 'data/poker-hand-testing.csv'\n","\n","    # Load and preprocess data\n","    X_train, y_train, X_test, y_test, preprocessor, feature_names = load_poker_data(train_path, test_path)\n","\n","    # Create dataset and dataloader\n","    train_dataset = PokerDataset(X_train, y_train)\n","    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","\n","    # Initialize MedGAN\n","    data_dim = X_train.shape[1]\n","    encoding_dim = 128\n","    latent_dim = 100\n","\n","    print(f\"Data dimension: {data_dim}\")\n","    print(f\"Encoding dimension: {encoding_dim}\")\n","    print(f\"Latent dimension: {latent_dim}\")\n","\n","    medgan = MedGAN(data_dim, encoding_dim, latent_dim, batch_size=128, autoencoder_epochs=50)\n","\n","    # Step 1: Pretrain the autoencoder\n","    medgan.pretrain_autoencoder(train_loader)\n","\n","    # Step 2: Train the GAN\n","    medgan.train_gan(train_loader, epochs=100, save_interval=10)\n","\n","    # Save the model\n","    medgan.save_model('poker_medgan_model.pt')\n","\n","    # Plot loss curves\n","    plot_loss_curves(medgan)\n","\n","    # Generate synthetic data\n","    num_samples = 1000\n","    print(f\"Generating {num_samples} synthetic samples...\")\n","    synthetic_data_raw = medgan.generate_samples(num_samples)\n","\n","    # Post-process the synthetic data to make it valid poker hands\n","    synthetic_data_processed = post_process_poker_data(synthetic_data_raw, preprocessor)\n","\n","    # Generate synthetic labels using a classifier trained on real data\n","    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n","    clf.fit(X_train, y_train)\n","    synthetic_labels = clf.predict(synthetic_data_raw)\n","\n","    # Plot class distribution\n","    plot_class_distribution(y_train, synthetic_labels)\n","\n","    # Statistical similarity evaluation\n","    print(\"Evaluating statistical similarity...\")\n","    stat_results = evaluate_statistical_similarity(X_train, synthetic_data_raw, feature_names)\n","\n","    print(\"\\nJensen-Shannon Divergence (average):\", stat_results['JSD_avg'])\n","    print(\"Wasserstein Distance (average):\", stat_results['WD_avg'])\n","\n","    # Machine Learning Utility (TSTR) evaluation\n","    print(\"\\nEvaluating Machine Learning Utility (TSTR)...\")\n","    tstr_results = evaluate_tstr(X_train, synthetic_data_raw, y_train)\n","\n","    print(\"\\nTSTR Results:\")\n","    for clf_name, metrics in tstr_results.items():\n","        print(f\"{clf_name}: Accuracy = {metrics['accuracy']:.4f}, F1 Score = {metrics['f1_score']:.4f}\")\n","\n","    # Plot feature distributions\n","    plot_feature_distributions(X_train, synthetic_data_raw, feature_names)\n","\n","    print(\"\\nEvaluation complete! Check the output directory for plots and saved model.\")\n"],"metadata":{"id":"P4zL8cbWmBfJ","executionInfo":{"status":"ok","timestamp":1743855307812,"user_tz":-660,"elapsed":50,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U7Tkm6fhl5jf","executionInfo":{"status":"ok","timestamp":1743856046197,"user_tz":-660,"elapsed":738378,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}},"outputId":"053836ef-29be-412e-a38c-5868c8893767"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Training data shape: (25010, 11)\n","Testing data shape: (1000000, 11)\n","Processed training data shape: (25010, 25)\n","Processed testing data shape: (1000000, 25)\n","Data dimension: 25\n","Encoding dimension: 128\n","Latent dimension: 100\n","Pretraining autoencoder for 50 epochs...\n","Autoencoder Epoch 1/50 | Loss: 0.056696\n","Autoencoder Epoch 10/50 | Loss: 0.017806\n","Autoencoder Epoch 20/50 | Loss: 0.016816\n","Autoencoder Epoch 30/50 | Loss: 0.016778\n","Autoencoder Epoch 40/50 | Loss: 0.016382\n","Autoencoder Epoch 50/50 | Loss: 0.016206\n","Autoencoder pretraining complete!\n","Training MedGAN for 100 epochs...\n","GAN Epoch 1/100 | D Loss: 1.273534 | G Loss: 0.781497\n","GAN Epoch 10/100 | D Loss: 1.163367 | G Loss: 0.949234\n","GAN Epoch 20/100 | D Loss: 0.949228 | G Loss: 1.291191\n","GAN Epoch 30/100 | D Loss: 0.889354 | G Loss: 1.410466\n","GAN Epoch 40/100 | D Loss: 0.852112 | G Loss: 1.472878\n","GAN Epoch 50/100 | D Loss: 0.804739 | G Loss: 1.570688\n","GAN Epoch 60/100 | D Loss: 0.714435 | G Loss: 1.774080\n","GAN Epoch 70/100 | D Loss: 0.594836 | G Loss: 2.085980\n","GAN Epoch 80/100 | D Loss: 0.496927 | G Loss: 2.492481\n","GAN Epoch 90/100 | D Loss: 0.424075 | G Loss: 2.820711\n","GAN Epoch 100/100 | D Loss: 0.379862 | G Loss: 3.100892\n","MedGAN training complete!\n","Generating 1000 synthetic samples...\n","Evaluating statistical similarity...\n","\n","Jensen-Shannon Divergence (average): 0.03880418909877127\n","Wasserstein Distance (average): 0.08313660325098503\n","\n","Evaluating Machine Learning Utility (TSTR)...\n","\n","TSTR Results:\n","Logistic Regression: Accuracy = 0.4936, F1 Score = 0.4349\n","MLP: Accuracy = 0.4450, F1 Score = 0.4335\n","Random Forest: Accuracy = 0.4846, F1 Score = 0.4534\n","XGBoost: Accuracy = 0.4828, F1 Score = 0.4496\n","\n","Evaluation complete! Check the output directory for plots and saved model.\n"]}]}]}