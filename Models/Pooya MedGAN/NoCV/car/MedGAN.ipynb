{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN0qH3QexzKhXCfEOKqeCIE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torch.autograd import Variable\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, classification_report\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","from scipy.stats import entropy\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","import time\n","from tqdm import tqdm\n","from google.colab import drive\n","\n","drive.mount('/content/drive/')\n","%cd /content/drive/MyDrive/Colab Notebooks/Katabatic/MedGAN/car/\n","\n","# Suppress warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Setting random seeds for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","# Check if CUDA is available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HZ2OJh2pJRYS","executionInfo":{"status":"ok","timestamp":1743765050174,"user_tz":-660,"elapsed":8991,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}},"outputId":"abf4f290-ca9f-4c27-da10-6a25d6cb452a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n","/content/drive/MyDrive/Colab Notebooks/Katabatic/MedGAN/car\n","Using device: cpu\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"89jAxJ4BJJPP","executionInfo":{"status":"ok","timestamp":1743765050297,"user_tz":-660,"elapsed":104,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}}},"outputs":[],"source":["def load_car_data(data_path):\n","    # Column names for the car dataset\n","    column_names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n","\n","    # Load data\n","    df = pd.read_csv(data_path, header=None, names=column_names)\n","\n","    print(f\"Dataset shape: {df.shape}\")\n","\n","    # Split into train and test (80% train, 20% test)\n","    df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n","\n","    print(f\"Training data shape: {df_train.shape}\")\n","    print(f\"Testing data shape: {df_test.shape}\")\n","\n","    # Split features and target\n","    X_train = df_train.drop('class', axis=1)\n","    y_train = df_train['class']\n","    X_test = df_test.drop('class', axis=1)\n","    y_test = df_test['class']\n","\n","    # Identify categorical columns\n","    categorical_cols = X_train.columns.tolist()  # All columns are categorical\n","\n","    # Create preprocessing pipeline for categorical data using one-hot encoding\n","    categorical_transformer = Pipeline(steps=[\n","        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n","    ])\n","\n","    preprocessor = ColumnTransformer(\n","        transformers=[\n","            ('cat', categorical_transformer, categorical_cols)\n","        ])\n","\n","    # Fit and transform the data\n","    X_train_transformed = preprocessor.fit_transform(X_train)\n","    X_test_transformed = preprocessor.transform(X_test)\n","\n","    print(f\"Processed training data shape: {X_train_transformed.shape}\")\n","    print(f\"Processed testing data shape: {X_test_transformed.shape}\")\n","\n","    # Get one-hot encoding feature names for later use\n","    cat_encoder = preprocessor.named_transformers_['cat'].named_steps['onehot']\n","    feature_names = cat_encoder.get_feature_names_out(categorical_cols)\n","\n","    # Create label encoder for the target classes\n","    unique_classes = sorted(df['class'].unique())\n","    label_encoder = {cls: i for i, cls in enumerate(unique_classes)}\n","    inverse_label_encoder = {i: cls for cls, i in label_encoder.items()}\n","\n","    # Encode targets\n","    y_train_encoded = y_train.map(label_encoder)\n","    y_test_encoded = y_test.map(label_encoder)\n","\n","    # Store original categorical values\n","    cat_values = {}\n","    for col in categorical_cols:\n","        cat_values[col] = sorted(df[col].unique())\n","\n","    return (X_train_transformed, y_train_encoded, X_test_transformed, y_test_encoded,\n","            preprocessor, feature_names, label_encoder, inverse_label_encoder, cat_values,\n","            X_train, y_train)\n","\n","# Custom dataset class\n","class CarDataset(Dataset):\n","    def __init__(self, features, labels=None):\n","        self.features = torch.tensor(features, dtype=torch.float32)\n","        if labels is not None:\n","            self.labels = torch.tensor(labels.values, dtype=torch.long)\n","        else:\n","            self.labels = None\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, idx):\n","        if self.labels is not None:\n","            return self.features[idx], self.labels[idx]\n","        else:\n","            return self.features[idx]\n","\n","# Autoencoder for MedGAN\n","class Autoencoder(nn.Module):\n","    def __init__(self, input_dim, hidden_dim=128, latent_dim=64):\n","        super(Autoencoder, self).__init__()\n","\n","        # Encoder layers\n","        self.encoder = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm1d(hidden_dim),\n","\n","            nn.Linear(hidden_dim, latent_dim),\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm1d(latent_dim)\n","        )\n","\n","        # Decoder layers\n","        self.decoder = nn.Sequential(\n","            nn.Linear(latent_dim, hidden_dim),\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm1d(hidden_dim),\n","\n","            nn.Linear(hidden_dim, input_dim),\n","            nn.Sigmoid()  # Use sigmoid for binary features (one-hot encoded data)\n","        )\n","\n","    def forward(self, x):\n","        encoded = self.encoder(x)\n","        decoded = self.decoder(encoded)\n","        return decoded\n","\n","    def encode(self, x):\n","        return self.encoder(x)\n","\n","# Generator Network for MedGAN\n","class Generator(nn.Module):\n","    def __init__(self, latent_dim, hidden_dim=128, output_dim=64):\n","        super(Generator, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(latent_dim, hidden_dim),\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm1d(hidden_dim),\n","\n","            nn.Linear(hidden_dim, hidden_dim * 2),\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm1d(hidden_dim * 2),\n","\n","            nn.Linear(hidden_dim * 2, output_dim),\n","            nn.Tanh()  # Output is a latent representation for the decoder\n","        )\n","\n","    def forward(self, z):\n","        return self.model(z)\n","\n","# Discriminator Network for MedGAN\n","class Discriminator(nn.Module):\n","    def __init__(self, input_dim, hidden_dim=128):\n","        super(Discriminator, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim * 2),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(hidden_dim * 2, hidden_dim),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(hidden_dim, 1),\n","            nn.Sigmoid()  # Output probability of being real\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# MedGAN Implementation\n","class MedGAN:\n","    def __init__(self, data_dim, latent_dim=100, hidden_dim=128, autoencoder_latent_dim=64):\n","        self.data_dim = data_dim\n","        self.latent_dim = latent_dim\n","        self.hidden_dim = hidden_dim\n","        self.autoencoder_latent_dim = autoencoder_latent_dim\n","\n","        # Initialize networks\n","        self.autoencoder = Autoencoder(data_dim, hidden_dim, autoencoder_latent_dim).to(device)\n","        self.generator = Generator(latent_dim, hidden_dim, autoencoder_latent_dim).to(device)\n","        self.discriminator = Discriminator(data_dim, hidden_dim).to(device)\n","\n","        # Setup optimizers\n","        self.ae_optimizer = optim.Adam(self.autoencoder.parameters(), lr=0.001)\n","        self.g_optimizer = optim.Adam(self.generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","        self.d_optimizer = optim.Adam(self.discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","        # Loss functions\n","        self.bce_loss = nn.BCELoss()\n","        self.mse_loss = nn.MSELoss()\n","\n","        # Initialize loss tracking\n","        self.ae_losses = []\n","        self.g_losses = []\n","        self.d_losses = []\n","\n","    def pretrain_autoencoder(self, data_loader, epochs, verbose=True):\n","        \"\"\"Pretrain the autoencoder\"\"\"\n","        print(\"Pretraining autoencoder...\")\n","        self.autoencoder.train()\n","\n","        for epoch in range(epochs):\n","            epoch_loss = 0\n","            num_batches = 0\n","\n","            for real_data, _ in data_loader:\n","                real_data = real_data.to(device)\n","\n","                # Forward pass\n","                reconstructed = self.autoencoder(real_data)\n","                loss = self.mse_loss(reconstructed, real_data)\n","\n","                # Backward pass and optimize\n","                self.ae_optimizer.zero_grad()\n","                loss.backward()\n","                self.ae_optimizer.step()\n","\n","                epoch_loss += loss.item()\n","                num_batches += 1\n","\n","            avg_loss = epoch_loss / num_batches\n","            self.ae_losses.append(avg_loss)\n","\n","            if verbose and (epoch % 10 == 0 or epoch == epochs - 1):\n","                print(f\"Autoencoder Epoch [{epoch+1}/{epochs}] | Loss: {avg_loss:.6f}\")\n","\n","    def train_gan(self, data_loader, epochs, d_steps=1, verbose=True):\n","      \"\"\"Train the GAN after pretraining the autoencoder\"\"\"\n","      print(\"Training MedGAN...\")\n","      self.autoencoder.eval()  # Freeze autoencoder weights\n","\n","      for epoch in range(epochs):\n","          epoch_g_loss = 0\n","          epoch_d_loss = 0\n","          num_batches = 0\n","\n","          for real_data, _ in data_loader:\n","              batch_size = real_data.size(0)  # Get the actual batch size\n","              real_data = real_data.to(device)\n","\n","              # Create labels dynamically based on the actual batch size\n","              ones = torch.ones(batch_size, 1).to(device)\n","              zeros = torch.zeros(batch_size, 1).to(device)\n","\n","              # Train Discriminator\n","              for _ in range(d_steps):\n","                  self.d_optimizer.zero_grad()\n","\n","                  # Real data\n","                  d_real = self.discriminator(real_data)\n","                  d_real_loss = self.bce_loss(d_real, ones)\n","\n","                  # Fake data\n","                  z = torch.randn(batch_size, self.latent_dim).to(device)\n","                  fake_latent = self.generator(z)\n","                  fake_data = self.autoencoder.decoder(fake_latent)\n","                  d_fake = self.discriminator(fake_data.detach())\n","                  d_fake_loss = self.bce_loss(d_fake, zeros)\n","\n","                  # Total discriminator loss\n","                  d_loss = d_real_loss + d_fake_loss\n","                  d_loss.backward()\n","                  self.d_optimizer.step()\n","\n","              # Train Generator\n","              self.g_optimizer.zero_grad()\n","\n","              z = torch.randn(batch_size, self.latent_dim).to(device)\n","              fake_latent = self.generator(z)\n","              fake_data = self.autoencoder.decoder(fake_latent)\n","              g_fake = self.discriminator(fake_data)\n","              g_loss = self.bce_loss(g_fake, ones)\n","\n","              g_loss.backward()\n","              self.g_optimizer.step()\n","\n","              # Record losses\n","              epoch_d_loss += d_loss.item()\n","              epoch_g_loss += g_loss.item()\n","              num_batches += 1\n","\n","          # Calculate average losses\n","          avg_d_loss = epoch_d_loss / num_batches\n","          avg_g_loss = epoch_g_loss / num_batches\n","\n","          self.d_losses.append(avg_d_loss)\n","          self.g_losses.append(avg_g_loss)\n","\n","          if verbose and (epoch % 10 == 0 or epoch == epochs - 1):\n","              print(f\"GAN Epoch [{epoch+1}/{epochs}] | D Loss: {avg_d_loss:.6f} | G Loss: {avg_g_loss:.6f}\")\n","\n","    def generate_samples(self, num_samples):\n","        \"\"\"Generate synthetic data samples\"\"\"\n","        self.generator.eval()\n","        self.autoencoder.eval()\n","\n","        with torch.no_grad():\n","            z = torch.randn(num_samples, self.latent_dim).to(device)\n","            latent_codes = self.generator(z)\n","            samples = self.autoencoder.decoder(latent_codes).cpu().numpy()\n","\n","        self.generator.train()\n","        self.autoencoder.train()\n","\n","        return samples\n","\n","    def save_model(self, path):\n","        \"\"\"Save the model\"\"\"\n","        torch.save({\n","            'autoencoder_state_dict': self.autoencoder.state_dict(),\n","            'generator_state_dict': self.generator.state_dict(),\n","            'discriminator_state_dict': self.discriminator.state_dict(),\n","            'ae_optimizer_state_dict': self.ae_optimizer.state_dict(),\n","            'g_optimizer_state_dict': self.g_optimizer.state_dict(),\n","            'd_optimizer_state_dict': self.d_optimizer.state_dict(),\n","        }, path)\n","\n","    def load_model(self, path):\n","        \"\"\"Load the model\"\"\"\n","        checkpoint = torch.load(path)\n","        self.autoencoder.load_state_dict(checkpoint['autoencoder_state_dict'])\n","        self.generator.load_state_dict(checkpoint['generator_state_dict'])\n","        self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n","        self.ae_optimizer.load_state_dict(checkpoint['ae_optimizer_state_dict'])\n","        self.g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n","        self.d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n","\n","# Post-process generated data for categorical features\n","def post_process_car_data(synthetic_data, preprocessor, cat_values, feature_names):\n","    \"\"\"\n","    Post-process the synthetic data to convert one-hot encoded features back to categorical values\n","    \"\"\"\n","    # Create a DataFrame with one-hot encoded columns\n","    synthetic_df = pd.DataFrame(synthetic_data, columns=feature_names)\n","\n","    # Extract categorical feature groups\n","    result_df = pd.DataFrame()\n","\n","    # Process each categorical column\n","    for col_name, values in cat_values.items():\n","        # Get one-hot columns for this feature\n","        col_pattern = f\"{col_name}_\"\n","        category_cols = [c for c in feature_names if c.startswith(col_pattern)]\n","\n","        # Get the most likely category for each sample\n","        category_probs = synthetic_df[category_cols].values\n","        category_indices = np.argmax(category_probs, axis=1)\n","\n","        # Map indices back to original categories\n","        # Extract the original category from the one-hot column name\n","        categories = [c.split('_', 1)[1] for c in category_cols]\n","        result_df[col_name] = [categories[idx] for idx in category_indices]\n","\n","    return result_df\n"]},{"cell_type":"code","source":["# Evaluation Metrics\n","\n","# 1. Machine Learning Utility (TSTR)\n","def evaluate_tstr(real_data, synthetic_data, real_labels, random_state=42):\n","    \"\"\"\n","    Train classifiers on synthetic data and test on real data (TSTR)\n","    Returns accuracy for each classifier\n","    \"\"\"\n","    # Train-test split for real data\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        real_data, real_labels, test_size=0.2, random_state=random_state\n","    )\n","\n","    # Synthetic data (all used for training)\n","    X_synth = synthetic_data\n","\n","    # Ensure proper dimensions for labels\n","    if isinstance(y_train, pd.Series):\n","        y_train = y_train.values\n","\n","    # Create synthetic labels based on real distribution\n","    num_classes = len(np.unique(y_train))\n","    class_distribution = np.bincount(y_train.astype(int), minlength=num_classes) / len(y_train)\n","    np.random.seed(random_state)\n","    y_synth = np.random.choice(range(num_classes), size=len(X_synth), p=class_distribution)\n","\n","    # Define classifiers\n","    classifiers = {\n","        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=random_state),\n","        'MLP': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=random_state),\n","        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=random_state),\n","        'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=random_state)\n","    }\n","\n","    results = {}\n","\n","    for name, clf in classifiers.items():\n","        # Train on synthetic data\n","        clf.fit(X_synth, y_synth)\n","\n","        # Test on real data\n","        y_pred = clf.predict(X_test)\n","\n","        # Calculate metrics\n","        accuracy = accuracy_score(y_test, y_pred)\n","\n","        # We use weighted f1 since we have multiple classes\n","        f1 = f1_score(y_test, y_pred, average='weighted')\n","\n","        results[name] = {\n","            'accuracy': accuracy,\n","            'f1_score': f1\n","        }\n","\n","    return results\n","\n","# 2. Statistical Similarity for categorical data\n","def evaluate_categorical_similarity(real_df, synthetic_df):\n","    \"\"\"\n","    Calculate statistical similarity for categorical features\n","    \"\"\"\n","    results = {'JSD': {}}\n","\n","    # For each categorical column, calculate JSD\n","    for col in real_df.columns:\n","        # Get value counts\n","        real_counts = real_df[col].value_counts(normalize=True).sort_index()\n","        synth_counts = synthetic_df[col].value_counts(normalize=True).sort_index()\n","\n","        # Align the distributions\n","        all_categories = sorted(set(real_counts.index) | set(synth_counts.index))\n","        real_dist = np.array([real_counts.get(cat, 0) for cat in all_categories])\n","        synth_dist = np.array([synth_counts.get(cat, 0) for cat in all_categories])\n","\n","        # Add small epsilon to avoid zeros\n","        epsilon = 1e-10\n","        real_dist = real_dist + epsilon\n","        synth_dist = synth_dist + epsilon\n","\n","        # Normalize\n","        real_dist = real_dist / real_dist.sum()\n","        synth_dist = synth_dist / synth_dist.sum()\n","\n","        # Calculate JSD\n","        jsd = jensen_shannon_divergence(real_dist, synth_dist)\n","        results['JSD'][col] = jsd\n","\n","    # Average JSD across all features\n","    results['JSD_avg'] = np.mean(list(results['JSD'].values()))\n","\n","    return results\n","\n","def jensen_shannon_divergence(p, q):\n","    \"\"\"\n","    Calculate Jensen-Shannon Divergence between distributions p and q\n","    \"\"\"\n","    # Ensure p and q are normalized\n","    p = p / np.sum(p)\n","    q = q / np.sum(q)\n","\n","    m = 0.5 * (p + q)\n","\n","    # Calculate JSD\n","    jsd = 0.5 * (entropy(p, m) + entropy(q, m))\n","\n","    return jsd\n","\n","def plot_loss_curves(model):\n","    \"\"\"\n","    Plot the loss curves for the autoencoder, generator, and discriminator\n","    \"\"\"\n","    # Plot autoencoder pretraining loss\n","    plt.figure(figsize=(12, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(model.ae_losses)\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.title('Autoencoder Pretraining Loss')\n","    plt.grid(True)\n","\n","    # Plot GAN losses\n","    plt.subplot(1, 2, 2)\n","    plt.plot(model.g_losses, label='Generator Loss')\n","    plt.plot(model.d_losses, label='Discriminator Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.title('MedGAN Training Loss')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    plt.tight_layout()\n","    plt.savefig('car_medgan_loss_curves.png')\n","    plt.close()\n","\n","def plot_categorical_distributions(real_df, synthetic_df):\n","    \"\"\"\n","    Plot distributions of real vs synthetic data for categorical features\n","    \"\"\"\n","    n_features = len(real_df.columns)\n","\n","    plt.figure(figsize=(15, n_features * 4))\n","\n","    for i, col in enumerate(real_df.columns):\n","        plt.subplot(n_features, 1, i+1)\n","\n","        # Calculate proportions\n","        real_props = real_df[col].value_counts(normalize=True).sort_index()\n","        synth_props = synthetic_df[col].value_counts(normalize=True).sort_index()\n","\n","        # Get all categories\n","        all_categories = sorted(set(real_props.index) | set(synth_props.index))\n","\n","        # Create a DataFrame for plotting\n","        plot_df = pd.DataFrame({\n","            'Category': all_categories * 2,\n","            'Proportion': [real_props.get(cat, 0) for cat in all_categories] +\n","                         [synth_props.get(cat, 0) for cat in all_categories],\n","            'Type': ['Real'] * len(all_categories) + ['Synthetic'] * len(all_categories)\n","        })\n","\n","        # Plot\n","        sns.barplot(x='Category', y='Proportion', hue='Type', data=plot_df)\n","        plt.title(f'Distribution for {col}')\n","        plt.xticks(rotation=45)\n","        plt.ylabel('Proportion')\n","        plt.legend()\n","\n","    plt.tight_layout()\n","    plt.savefig('car_categorical_distributions.png')\n","    plt.close()\n","\n","def plot_class_distribution(real_labels, synthetic_labels, label_encoder):\n","    \"\"\"\n","    Plot the class distribution of real vs synthetic data\n","    \"\"\"\n","    plt.figure(figsize=(12, 6))\n","\n","    # Get class counts\n","    real_class_counts = pd.Series(real_labels).value_counts(normalize=True)\n","    synth_class_counts = pd.Series(synthetic_labels).value_counts(normalize=True)\n","\n","    # Create inverse label encoder\n","    inverse_label_encoder = {v: k for k, v in label_encoder.items()}\n","\n","    # Get all classes\n","    all_classes = sorted(set(real_class_counts.index) | set(synth_class_counts.index))\n","\n","    # Create plot data\n","    plot_df = pd.DataFrame({\n","        'Class': [inverse_label_encoder.get(c, c) for c in all_classes] * 2,\n","        'Proportion': [real_class_counts.get(c, 0) for c in all_classes] +\n","                     [synth_class_counts.get(c, 0) for c in all_classes],\n","        'Type': ['Real'] * len(all_classes) + ['Synthetic'] * len(all_classes)\n","    })\n","\n","    # Plot\n","    sns.barplot(x='Class', y='Proportion', hue='Type', data=plot_df)\n","    plt.title('Class Distribution: Real vs Synthetic Car Evaluations')\n","    plt.xticks(rotation=45)\n","    plt.tight_layout()\n","    plt.savefig('car_class_distribution.png')\n","    plt.close()\n","\n","# Main function\n","def main():\n","    # File path\n","    data_path = 'data/car.csv'\n","\n","    # Load and preprocess data\n","    (X_train_transformed, y_train, X_test_transformed, y_test,\n","     preprocessor, feature_names, label_encoder, inverse_label_encoder,\n","     cat_values, X_train_original, y_train_original) = load_car_data(data_path)\n","\n","    # Create dataset and dataloader\n","    train_dataset = CarDataset(X_train_transformed, y_train)\n","    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","\n","    # Initialize MedGAN model\n","    data_dim = X_train_transformed.shape[1]\n","    latent_dim = 100\n","    hidden_dim = 128\n","    autoencoder_latent_dim = 64\n","\n","    print(f\"Data dimension: {data_dim}\")\n","    print(f\"Latent dimension: {latent_dim}\")\n","    print(f\"Hidden dimension: {hidden_dim}\")\n","    print(f\"Autoencoder latent dimension: {autoencoder_latent_dim}\")\n","\n","    medgan = MedGAN(data_dim, latent_dim, hidden_dim, autoencoder_latent_dim)\n","\n","    # Pretrain the autoencoder\n","    print(\"Pretraining the autoencoder...\")\n","    ae_epochs = 100\n","    medgan.pretrain_autoencoder(train_loader, ae_epochs)\n","\n","    # Train the GAN\n","    print(\"Training the GAN...\")\n","    gan_epochs = 300\n","    medgan.train_gan(train_loader, gan_epochs)\n","\n","    # Save the model\n","    print(\"Saving model...\")\n","    medgan.save_model('car_medgan_model.pt')\n","\n","    # Plot loss curves\n","    plot_loss_curves(medgan)\n","\n","    # Generate synthetic data\n","    num_samples = 1000\n","    print(f\"Generating {num_samples} synthetic samples...\")\n","    synthetic_data_raw = medgan.generate_samples(num_samples)\n","\n","    # Post-process the synthetic data\n","    synthetic_df = post_process_car_data(synthetic_data_raw, preprocessor, cat_values, feature_names)\n","\n","    # Generate synthetic labels using a classifier trained on real data\n","    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n","    clf.fit(X_train_transformed, y_train)\n","    synthetic_data_transformed = preprocessor.transform(synthetic_df)\n","    synthetic_labels_raw = clf.predict(synthetic_data_transformed)\n","\n","    # Convert numeric labels to original class values\n","    synthetic_labels = [inverse_label_encoder[label] for label in synthetic_labels_raw]\n","\n","    # Add class labels to the synthetic dataframe\n","    synthetic_df['class'] = synthetic_labels\n","\n","    # Save the synthetic data\n","    synthetic_df.to_csv('synthetic_car_data_medgan.csv', index=False)\n","\n","    # Plot distributions\n","    plot_categorical_distributions(X_train_original, synthetic_df.drop('class', axis=1))\n","\n","    # Plot class distribution\n","    plot_class_distribution(y_train_original, synthetic_df['class'], label_encoder)\n","\n","    # Statistical similarity evaluation\n","    print(\"Evaluating statistical similarity...\")\n","    stat_results = evaluate_categorical_similarity(X_train_original, synthetic_df.drop('class', axis=1))\n","\n","    print(\"\\nJensen-Shannon Divergence (average):\", stat_results['JSD_avg'])\n","    print(\"\\nJSD per feature:\")\n","    for feature, jsd in stat_results['JSD'].items():\n","        print(f\"  {feature}: {jsd:.4f}\")\n","\n","    # Machine Learning Utility (TSTR) evaluation\n","    print(\"\\nEvaluating Machine Learning Utility (TSTR)...\")\n","    tstr_results = evaluate_tstr(X_train_transformed, synthetic_data_raw, y_train)\n","\n","    print(\"\\nTSTR Results:\")\n","    for clf_name, metrics in tstr_results.items():\n","        print(f\"{clf_name}: Accuracy = {metrics['accuracy']:.4f}, F1 Score = {metrics['f1_score']:.4f}\")\n","\n","    print(\"\\nEvaluation complete! Check the output directory for plots and saved model.\")\n"],"metadata":{"id":"nMpuC5bvcvPI","executionInfo":{"status":"ok","timestamp":1743765050464,"user_tz":-660,"elapsed":164,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zYtKeziPJLun","executionInfo":{"status":"ok","timestamp":1743765092341,"user_tz":-660,"elapsed":41872,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}},"outputId":"5ced9177-dda2-4bf4-89b8-6af9f67facc2"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset shape: (1729, 7)\n","Training data shape: (1383, 7)\n","Testing data shape: (346, 7)\n","Processed training data shape: (1383, 27)\n","Processed testing data shape: (346, 27)\n","Data dimension: 27\n","Latent dimension: 100\n","Hidden dimension: 128\n","Autoencoder latent dimension: 64\n","Pretraining the autoencoder...\n","Pretraining autoencoder...\n","Autoencoder Epoch [1/100] | Loss: 0.208357\n","Autoencoder Epoch [11/100] | Loss: 0.016645\n","Autoencoder Epoch [21/100] | Loss: 0.003024\n","Autoencoder Epoch [31/100] | Loss: 0.001315\n","Autoencoder Epoch [41/100] | Loss: 0.000752\n","Autoencoder Epoch [51/100] | Loss: 0.000522\n","Autoencoder Epoch [61/100] | Loss: 0.000381\n","Autoencoder Epoch [71/100] | Loss: 0.000307\n","Autoencoder Epoch [81/100] | Loss: 0.000262\n","Autoencoder Epoch [91/100] | Loss: 0.000210\n","Autoencoder Epoch [100/100] | Loss: 0.000185\n","Training the GAN...\n","Training MedGAN...\n","GAN Epoch [1/300] | D Loss: 1.366119 | G Loss: 0.670994\n","GAN Epoch [11/300] | D Loss: 1.205462 | G Loss: 0.787161\n","GAN Epoch [21/300] | D Loss: 0.950662 | G Loss: 1.019005\n","GAN Epoch [31/300] | D Loss: 0.738341 | G Loss: 1.391279\n","GAN Epoch [41/300] | D Loss: 0.661033 | G Loss: 1.720894\n","GAN Epoch [51/300] | D Loss: 0.574963 | G Loss: 1.968118\n","GAN Epoch [61/300] | D Loss: 0.518436 | G Loss: 2.236622\n","GAN Epoch [71/300] | D Loss: 0.529515 | G Loss: 2.412658\n","GAN Epoch [81/300] | D Loss: 0.527572 | G Loss: 2.449230\n","GAN Epoch [91/300] | D Loss: 0.510665 | G Loss: 2.653687\n","GAN Epoch [101/300] | D Loss: 0.445355 | G Loss: 2.588827\n","GAN Epoch [111/300] | D Loss: 0.497911 | G Loss: 2.707840\n","GAN Epoch [121/300] | D Loss: 0.489304 | G Loss: 2.859402\n","GAN Epoch [131/300] | D Loss: 0.537388 | G Loss: 2.808986\n","GAN Epoch [141/300] | D Loss: 0.504914 | G Loss: 2.791537\n","GAN Epoch [151/300] | D Loss: 0.504678 | G Loss: 2.823541\n","GAN Epoch [161/300] | D Loss: 0.535728 | G Loss: 2.782983\n","GAN Epoch [171/300] | D Loss: 0.498663 | G Loss: 2.743840\n","GAN Epoch [181/300] | D Loss: 0.503211 | G Loss: 2.844549\n","GAN Epoch [191/300] | D Loss: 0.548379 | G Loss: 2.817255\n","GAN Epoch [201/300] | D Loss: 0.536985 | G Loss: 2.708343\n","GAN Epoch [211/300] | D Loss: 0.520339 | G Loss: 2.825076\n","GAN Epoch [221/300] | D Loss: 0.556931 | G Loss: 2.791600\n","GAN Epoch [231/300] | D Loss: 0.533512 | G Loss: 2.817858\n","GAN Epoch [241/300] | D Loss: 0.499610 | G Loss: 2.710547\n","GAN Epoch [251/300] | D Loss: 0.522874 | G Loss: 2.711618\n","GAN Epoch [261/300] | D Loss: 0.524005 | G Loss: 2.719199\n","GAN Epoch [271/300] | D Loss: 0.514089 | G Loss: 2.759298\n","GAN Epoch [281/300] | D Loss: 0.588579 | G Loss: 2.771243\n","GAN Epoch [291/300] | D Loss: 0.537162 | G Loss: 2.821626\n","GAN Epoch [300/300] | D Loss: 0.528838 | G Loss: 2.709979\n","Saving model...\n","Generating 1000 synthetic samples...\n","Evaluating statistical similarity...\n","\n","Jensen-Shannon Divergence (average): 0.11743678408189319\n","\n","JSD per feature:\n","  buying: 0.0012\n","  maint: 0.0046\n","  doors: 0.0038\n","  persons: 0.0003\n","  lug_boot: 0.6931\n","  safety: 0.0016\n","\n","Evaluating Machine Learning Utility (TSTR)...\n","\n","TSTR Results:\n","Logistic Regression: Accuracy = 0.7148, F1 Score = 0.5959\n","MLP: Accuracy = 0.5884, F1 Score = 0.5692\n","Random Forest: Accuracy = 0.2383, F1 Score = 0.1756\n","XGBoost: Accuracy = 0.5451, F1 Score = 0.5232\n","\n","Evaluation complete! Check the output directory for plots and saved model.\n"]}]}]}