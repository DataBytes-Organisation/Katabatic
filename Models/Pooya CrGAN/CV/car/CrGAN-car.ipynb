{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPfUSPRHORA1nP9VrdNAMfD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dMjhjjje4pZr","executionInfo":{"status":"ok","timestamp":1744343003829,"user_tz":-600,"elapsed":14650,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}},"outputId":"75e1478c-11e5-4e65-d056-84453e92ed29"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n","/content/drive/MyDrive/Colab Notebooks/Katabatic/CrGAN/car/cross\n","Using device: cpu\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torch.autograd import Variable\n","from torch.autograd import grad as torch_grad\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.metrics import accuracy_score, f1_score, classification_report\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","from scipy.stats import entropy\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","import time\n","from tqdm import tqdm\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive/')\n","%cd /content/drive/MyDrive/Colab Notebooks/Katabatic/CrGAN/car/cross/\n","\n","# Suppress warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Setting random seeds for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","# Check if CUDA is available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","def load_car_data(data_path, shuffle=False, random_state=42):\n","    \"\"\"\n","    Load and preprocess the car dataset with option to shuffle\n","    \"\"\"\n","    # Column names for the car dataset\n","    column_names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n","\n","    # Load data\n","    df = pd.read_csv(data_path, header=None, names=column_names)\n","\n","    # Shuffle data if requested\n","    if shuffle:\n","        df = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n","\n","    print(f\"Dataset shape: {df.shape}\")\n","\n","    # Identify categorical columns\n","    categorical_cols = df.drop('class', axis=1).columns.tolist()  # All columns except class are categorical\n","\n","    # Create preprocessing pipeline for categorical data using one-hot encoding\n","    categorical_transformer = Pipeline(steps=[\n","        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n","    ])\n","\n","    preprocessor = ColumnTransformer(\n","        transformers=[\n","            ('cat', categorical_transformer, categorical_cols)\n","        ])\n","\n","    # Get unique classes and create encoders\n","    unique_classes = sorted(df['class'].unique())\n","    label_encoder = {cls: i for i, cls in enumerate(unique_classes)}\n","    inverse_label_encoder = {i: cls for cls, i in label_encoder.items()}\n","\n","    # Store original categorical values\n","    cat_values = {}\n","    for col in categorical_cols:\n","        cat_values[col] = sorted(df[col].unique())\n","\n","    # Split features and target\n","    X = df.drop('class', axis=1)\n","    y = df['class']\n","\n","    # Encode targets\n","    y_encoded = y.map(label_encoder)\n","\n","    # Fit and transform the features\n","    X_transformed = preprocessor.fit_transform(X)\n","\n","    # Get one-hot encoding feature names\n","    cat_encoder = preprocessor.named_transformers_['cat'].named_steps['onehot']\n","    feature_names = cat_encoder.get_feature_names_out(categorical_cols)\n","\n","    print(f\"Processed data shape: {X_transformed.shape}\")\n","\n","    return (X_transformed, y_encoded, X, y, preprocessor, feature_names,\n","            label_encoder, inverse_label_encoder, cat_values, df)\n","\n","# Custom dataset class\n","class CarDataset(Dataset):\n","    def __init__(self, features, labels=None):\n","        self.features = torch.tensor(features, dtype=torch.float32)\n","        if labels is not None:\n","            self.labels = torch.tensor(labels.values if hasattr(labels, 'values') else labels, dtype=torch.long)\n","        else:\n","            self.labels = None\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, idx):\n","        if self.labels is not None:\n","            return self.features[idx], self.labels[idx]\n","        else:\n","            return self.features[idx]\n","\n","# Generator Network\n","class Generator(nn.Module):\n","    def __init__(self, latent_dim, output_dim):\n","        super(Generator, self).__init__()\n","        self.latent_dim = latent_dim\n","        self.output_dim = output_dim\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(latent_dim, 128),\n","            nn.BatchNorm1d(128),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Linear(128, 256),\n","            nn.BatchNorm1d(256),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Linear(256, 512),\n","            nn.BatchNorm1d(512),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Linear(512, output_dim),\n","            nn.Sigmoid()  # Use Sigmoid for one-hot encoded data\n","        )\n","\n","    def forward(self, z):\n","        return self.model(z)\n","\n","# Critic Network (Discriminator)\n","class Critic(nn.Module):\n","    def __init__(self, input_dim):\n","        super(Critic, self).__init__()\n","        self.input_dim = input_dim\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(input_dim, 256),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(256, 128),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(128, 64),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(64, 1)\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Cramer GAN Implementation\n","class CramerGAN:\n","    def __init__(self, data_dim, latent_dim=100, critic_iterations=5, lambda_gp=10):\n","        self.data_dim = data_dim\n","        self.latent_dim = latent_dim\n","        self.critic_iterations = critic_iterations\n","        self.lambda_gp = lambda_gp\n","\n","        # Initialize networks\n","        self.generator = Generator(latent_dim, data_dim).to(device)\n","        self.critic = Critic(data_dim).to(device)\n","\n","        # Setup optimizers\n","        self.g_optimizer = optim.Adam(self.generator.parameters(), lr=0.0002, betas=(0.5, 0.9))\n","        self.c_optimizer = optim.Adam(self.critic.parameters(), lr=0.0002, betas=(0.5, 0.9))\n","\n","        # Initialize loss tracking\n","        self.g_losses = []\n","        self.c_losses = []\n","\n","    def _critic_train_iteration(self, real_data, batch_size):\n","        # Generate random noise\n","        noise = torch.randn(batch_size, self.latent_dim).to(device)\n","\n","        # Generate fake data\n","        fake_data = self.generator(noise)\n","\n","        # Get critic outputs\n","        critic_real = self.critic(real_data)\n","        critic_fake = self.critic(fake_data)\n","\n","        # Calculate Cramer distance\n","        critic_real2 = self.critic(torch.roll(real_data, shifts=1, dims=0))\n","        critic_fake2 = self.critic(torch.roll(fake_data, shifts=1, dims=0))\n","\n","        # Cramer GAN loss function\n","        c_loss = torch.mean(critic_real - critic_fake) - 0.5 * torch.mean(torch.pow(critic_real - critic_real2, 2)) + 0.5 * torch.mean(torch.pow(critic_fake - critic_fake2, 2))\n","\n","        # Calculate gradient penalty\n","        alpha = torch.rand(batch_size, 1).to(device)\n","        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n","        interpolates.requires_grad_(True)\n","\n","        critic_interpolates = self.critic(interpolates)\n","        gradients = torch_grad(outputs=critic_interpolates, inputs=interpolates,\n","                              grad_outputs=torch.ones_like(critic_interpolates).to(device),\n","                              create_graph=True, retain_graph=True)[0]\n","\n","        gradients = gradients.view(batch_size, -1)\n","        gradient_penalty = self.lambda_gp * ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n","\n","        # Update critic\n","        self.c_optimizer.zero_grad()\n","        c_loss_total = c_loss + gradient_penalty\n","        c_loss_total.backward()\n","        self.c_optimizer.step()\n","\n","        return c_loss_total.item()\n","\n","    def _generator_train_iteration(self, batch_size):\n","        # Generate random noise\n","        noise = torch.randn(batch_size, self.latent_dim).to(device)\n","\n","        # Generate fake data\n","        fake_data = self.generator(noise)\n","\n","        # Calculate critic outputs\n","        critic_fake = self.critic(fake_data)\n","        critic_fake2 = self.critic(torch.roll(fake_data, shifts=1, dims=0))\n","\n","        # Generator loss is negative of critic loss\n","        g_loss = -torch.mean(critic_fake) + 0.5 * torch.mean(torch.pow(critic_fake - critic_fake2, 2))\n","\n","        # Update generator\n","        self.g_optimizer.zero_grad()\n","        g_loss.backward()\n","        self.g_optimizer.step()\n","\n","        return g_loss.item()\n","\n","    def train(self, data_loader, epochs, save_interval=10, verbose=True):\n","        for epoch in range(epochs):\n","            epoch_start_time = time.time()\n","            c_loss_total = 0\n","            g_loss_total = 0\n","            num_batches = 0\n","\n","            for i, (real_data, _) in enumerate(data_loader):\n","                batch_size = real_data.size(0)\n","                real_data = real_data.to(device)\n","\n","                # Train critic\n","                for _ in range(self.critic_iterations):\n","                    c_loss = self._critic_train_iteration(real_data, batch_size)\n","                c_loss_total += c_loss\n","\n","                # Train generator\n","                g_loss = self._generator_train_iteration(batch_size)\n","                g_loss_total += g_loss\n","\n","                num_batches += 1\n","\n","            # Calculate average loss for the epoch\n","            c_loss_avg = c_loss_total / num_batches\n","            g_loss_avg = g_loss_total / num_batches\n","\n","            self.c_losses.append(c_loss_avg)\n","            self.g_losses.append(g_loss_avg)\n","\n","            epoch_time = time.time() - epoch_start_time\n","\n","            if verbose and (epoch % save_interval == 0 or epoch == epochs - 1):\n","                print(f\"Epoch [{epoch+1}/{epochs}] | Critic Loss: {c_loss_avg:.4f} | Generator Loss: {g_loss_avg:.4f} | Time: {epoch_time:.2f}s\")\n","\n","    def generate_samples(self, num_samples):\n","        self.generator.eval()\n","        noise = torch.randn(num_samples, self.latent_dim).to(device)\n","        with torch.no_grad():\n","            generated_data = self.generator(noise).cpu().numpy()\n","        self.generator.train()\n","        return generated_data\n","\n","    def save_model(self, path):\n","        torch.save({\n","            'generator_state_dict': self.generator.state_dict(),\n","            'critic_state_dict': self.critic.state_dict(),\n","            'g_optimizer_state_dict': self.g_optimizer.state_dict(),\n","            'c_optimizer_state_dict': self.c_optimizer.state_dict(),\n","        }, path)\n","\n","    def load_model(self, path):\n","        checkpoint = torch.load(path)\n","        self.generator.load_state_dict(checkpoint['generator_state_dict'])\n","        self.critic.load_state_dict(checkpoint['critic_state_dict'])\n","        self.g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n","        self.c_optimizer.load_state_dict(checkpoint['c_optimizer_state_dict'])\n","\n","# Post-process generated data for categorical features\n","def post_process_car_data(synthetic_data, preprocessor, cat_values, feature_names):\n","    \"\"\"\n","    Post-process the synthetic data to convert one-hot encoded features back to categorical values\n","    \"\"\"\n","    # Create a DataFrame with one-hot encoded columns\n","    synthetic_df = pd.DataFrame(synthetic_data, columns=feature_names)\n","\n","    # Extract categorical feature groups\n","    result_df = pd.DataFrame()\n","\n","    # Process each categorical column\n","    for col_name, values in cat_values.items():\n","        # Get one-hot columns for this feature\n","        col_pattern = f\"{col_name}_\"\n","        category_cols = [c for c in feature_names if c.startswith(col_pattern)]\n","\n","        # Get the most likely category for each sample\n","        category_probs = synthetic_df[category_cols].values\n","        category_indices = np.argmax(category_probs, axis=1)\n","\n","        # Map indices back to original categories\n","        # Extract the original category from the one-hot column name\n","        categories = [c.split('_', 1)[1] for c in category_cols]\n","        result_df[col_name] = [categories[idx] for idx in category_indices]\n","\n","    return result_df\n","\n","# Evaluation Metrics\n","\n","# 1. Machine Learning Utility (TSTR)\n","def evaluate_tstr(real_data, synthetic_data, real_labels, random_state=42):\n","    \"\"\"\n","    Train classifiers on synthetic data and test on real data (TSTR)\n","    Returns accuracy for each classifier\n","    \"\"\"\n","    # Train-test split for real data\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        real_data, real_labels, test_size=0.2, random_state=random_state\n","    )\n","\n","    # Synthetic data (all used for training)\n","    X_synth = synthetic_data\n","\n","    # Ensure proper dimensions for labels\n","    if isinstance(y_train, pd.Series):\n","        y_train = y_train.values\n","\n","    # Create synthetic labels based on real distribution\n","    num_classes = len(np.unique(y_train))\n","    class_distribution = np.bincount(y_train.astype(int), minlength=num_classes) / len(y_train)\n","    np.random.seed(random_state)\n","    y_synth = np.random.choice(range(num_classes), size=len(X_synth), p=class_distribution)\n","\n","    # Define classifiers\n","    classifiers = {\n","        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=random_state),\n","        'MLP': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=random_state),\n","        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=random_state),\n","        'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=random_state)\n","    }\n","\n","    results = {}\n","\n","    for name, clf in classifiers.items():\n","        # Train on synthetic data\n","        clf.fit(X_synth, y_synth)\n","\n","        # Test on real data\n","        y_pred = clf.predict(X_test)\n","\n","        # Calculate metrics\n","        accuracy = accuracy_score(y_test, y_pred)\n","\n","        # We use weighted f1 since we have multiple classes\n","        f1 = f1_score(y_test, y_pred, average='weighted')\n","\n","        results[name] = {\n","            'accuracy': accuracy,\n","            'f1_score': f1\n","        }\n","\n","    return results\n","\n","# 2. Statistical Similarity for categorical data\n","def evaluate_categorical_similarity(real_df, synthetic_df):\n","    \"\"\"\n","    Calculate statistical similarity for categorical features\n","    \"\"\"\n","    results = {'JSD': {}}\n","\n","    # For each categorical column, calculate JSD\n","    for col in real_df.columns:\n","        # Get value counts\n","        real_counts = real_df[col].value_counts(normalize=True).sort_index()\n","        synth_counts = synthetic_df[col].value_counts(normalize=True).sort_index()\n","\n","        # Align the distributions\n","        all_categories = sorted(set(real_counts.index) | set(synth_counts.index))\n","        real_dist = np.array([real_counts.get(cat, 0) for cat in all_categories])\n","        synth_dist = np.array([synth_counts.get(cat, 0) for cat in all_categories])\n","\n","        # Add small epsilon to avoid zeros\n","        epsilon = 1e-10\n","        real_dist = real_dist + epsilon\n","        synth_dist = synth_dist + epsilon\n","\n","        # Normalize\n","        real_dist = real_dist / real_dist.sum()\n","        synth_dist = synth_dist / synth_dist.sum()\n","\n","        # Calculate JSD\n","        jsd = jensen_shannon_divergence(real_dist, synth_dist)\n","        results['JSD'][col] = jsd\n","\n","    # Average JSD across all features\n","    results['JSD_avg'] = np.mean(list(results['JSD'].values()))\n","\n","    return results\n","\n","def jensen_shannon_divergence(p, q):\n","    \"\"\"\n","    Calculate Jensen-Shannon Divergence between distributions p and q\n","    \"\"\"\n","    # Ensure p and q are normalized\n","    p = p / np.sum(p)\n","    q = q / np.sum(q)\n","\n","    m = 0.5 * (p + q)\n","\n","    # Calculate JSD\n","    jsd = 0.5 * (entropy(p, m) + entropy(q, m))\n","\n","    return jsd\n","\n","def plot_loss_curves(model, fold=None, shuffle=False):\n","    \"\"\"\n","    Plot the loss curves for the generator and critic\n","    \"\"\"\n","    title_suffix = f\" (Fold {fold+1})\" if fold is not None else \"\"\n","    shuffle_suffix = \" (Shuffled)\" if shuffle else \"\"\n","\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(model.g_losses, label='Generator Loss')\n","    plt.plot(model.c_losses, label='Critic Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.title(f'CramerGAN Training Loss for Car Dataset{title_suffix}{shuffle_suffix}')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    # Save figure with appropriate naming\n","    if fold is not None:\n","        shuffle_text = \"shuffled_\" if shuffle else \"\"\n","        plt.savefig(f'car_crgan_loss_curves_{shuffle_text}fold_{fold+1}.png')\n","    else:\n","        plt.savefig('car_crgan_loss_curves.png')\n","\n","    plt.close()\n","\n","def plot_categorical_distributions(real_df, synthetic_df, fold=None, shuffle=False):\n","    \"\"\"\n","    Plot distributions of real vs synthetic data for categorical features\n","    \"\"\"\n","    n_features = len(real_df.columns)\n","    title_suffix = f\" (Fold {fold+1})\" if fold is not None else \"\"\n","    shuffle_suffix = \" (Shuffled)\" if shuffle else \"\"\n","\n","    plt.figure(figsize=(15, n_features * 4))\n","\n","    for i, col in enumerate(real_df.columns):\n","        plt.subplot(n_features, 1, i+1)\n","\n","        # Calculate proportions\n","        real_props = real_df[col].value_counts(normalize=True).sort_index()\n","        synth_props = synthetic_df[col].value_counts(normalize=True).sort_index()\n","\n","        # Get all categories\n","        all_categories = sorted(set(real_props.index) | set(synth_props.index))\n","\n","        # Create a DataFrame for plotting\n","        plot_df = pd.DataFrame({\n","            'Category': all_categories * 2,\n","            'Proportion': [real_props.get(cat, 0) for cat in all_categories] +\n","                         [synth_props.get(cat, 0) for cat in all_categories],\n","            'Type': ['Real'] * len(all_categories) + ['Synthetic'] * len(all_categories)\n","        })\n","\n","        # Plot\n","        sns.barplot(x='Category', y='Proportion', hue='Type', data=plot_df)\n","        plt.title(f'Distribution for {col}{title_suffix}{shuffle_suffix}')\n","        plt.xticks(rotation=45)\n","        plt.ylabel('Proportion')\n","        plt.legend()\n","\n","    plt.tight_layout()\n","\n","    # Save figure with appropriate naming\n","    if fold is not None:\n","        shuffle_text = \"shuffled_\" if shuffle else \"\"\n","        plt.savefig(f'car_categorical_distributions_{shuffle_text}fold_{fold+1}.png')\n","    else:\n","        plt.savefig('car_categorical_distributions.png')\n","\n","    plt.close()\n","\n","def plot_class_distribution(real_labels, synthetic_labels, label_encoder, fold=None, shuffle=False):\n","    \"\"\"\n","    Plot the class distribution of real vs synthetic data\n","    \"\"\"\n","    title_suffix = f\" (Fold {fold+1})\" if fold is not None else \"\"\n","    shuffle_suffix = \" (Shuffled)\" if shuffle else \"\"\n","\n","    plt.figure(figsize=(12, 6))\n","\n","    # Get class counts\n","    real_class_counts = pd.Series(real_labels).value_counts(normalize=True)\n","    synth_class_counts = pd.Series(synthetic_labels).value_counts(normalize=True)\n","\n","    # Create inverse label encoder\n","    inverse_label_encoder = {v: k for k, v in label_encoder.items()}\n","\n","    # Get all classes\n","    all_classes = sorted(set(real_class_counts.index) | set(synth_class_counts.index))\n","\n","    # Create plot data\n","    plot_df = pd.DataFrame({\n","        'Class': [inverse_label_encoder.get(c, c) for c in all_classes] * 2,\n","        'Proportion': [real_class_counts.get(c, 0) for c in all_classes] +\n","                     [synth_class_counts.get(c, 0) for c in all_classes],\n","        'Type': ['Real'] * len(all_classes) + ['Synthetic'] * len(all_classes)\n","    })\n","\n","    # Plot\n","    sns.barplot(x='Class', y='Proportion', hue='Type', data=plot_df)\n","    plt.title(f'Class Distribution: Real vs Synthetic Car Evaluations{title_suffix}{shuffle_suffix}')\n","    plt.xticks(rotation=45)\n","    plt.tight_layout()\n","\n","    # Save figure with appropriate naming\n","    if fold is not None:\n","        shuffle_text = \"shuffled_\" if shuffle else \"\"\n","        plt.savefig(f'car_class_distribution_{shuffle_text}fold_{fold+1}.png')\n","    else:\n","        plt.savefig('car_class_distribution.png')\n","\n","    plt.close()\n","\n","def run_cross_validation(data_path, shuffle=False, random_state=123, n_folds=5, epochs=100):\n","    \"\"\"\n","    Run k-fold cross-validation for CrGAN on the car dataset\n","    \"\"\"\n","    # Load and preprocess data\n","    (X_transformed, y_encoded, X_original, y_original,\n","     preprocessor, feature_names, label_encoder, inverse_label_encoder,\n","     cat_values, full_df) = load_car_data(data_path, shuffle=shuffle, random_state=random_state)\n","\n","    # Initialize KFold cross-validator\n","    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n","\n","    # Store results for each fold\n","    tstr_results_all = []\n","    jsd_results_all = []\n","\n","    # Define latent dimension\n","    latent_dim = 100\n","\n","    fold_results = {}\n","\n","    # Run cross-validation\n","    for fold, (train_idx, test_idx) in enumerate(kf.split(X_transformed)):\n","        print(f\"\\n{'='*50}\")\n","        print(f\"Running {'Shuffled ' if shuffle else ''}Fold {fold+1}/{n_folds}\")\n","        print(f\"{'='*50}\")\n","\n","        # Split data for this fold\n","        X_train_fold, X_test_fold = X_transformed[train_idx], X_transformed[test_idx]\n","        y_train_fold, y_test_fold = y_encoded.iloc[train_idx], y_encoded.iloc[test_idx]\n","        X_original_train, X_original_test = X_original.iloc[train_idx], X_original.iloc[test_idx]\n","\n","        # Create dataset and dataloader\n","        train_dataset = CarDataset(X_train_fold, y_train_fold)\n","        train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","\n","        # Initialize CramerGAN model\n","        data_dim = X_train_fold.shape[1]\n","        print(f\"Data dimension: {data_dim}\")\n","        print(f\"Latent dimension: {latent_dim}\")\n","\n","        crgan = CramerGAN(data_dim, latent_dim)\n","\n","        # Train the model\n","        print(f\"Training CramerGAN for {epochs} epochs...\")\n","        crgan.train(train_loader, epochs, save_interval=20)\n","\n","        # Save the model\n","        model_name = f\"car_crgan_model_{'shuffled_' if shuffle else ''}fold_{fold+1}.pt\"\n","        # crgan.save_model(model_name)\n","\n","        # Plot loss curves\n","        # plot_loss_curves(crgan, fold=fold, shuffle=shuffle)\n","\n","        # Generate synthetic data\n","        num_samples = 1000\n","        print(f\"Generating {num_samples} synthetic samples...\")\n","        synthetic_data_raw = crgan.generate_samples(num_samples)\n","\n","        # Post-process the synthetic data\n","        synthetic_df = post_process_car_data(synthetic_data_raw, preprocessor, cat_values, feature_names)\n","\n","        # Generate synthetic labels using a classifier trained on real data\n","        clf = RandomForestClassifier(n_estimators=100, random_state=42)\n","        clf.fit(X_train_fold, y_train_fold)\n","        synthetic_labels_raw = clf.predict(synthetic_data_raw)\n","\n","        # Convert numeric labels to original class values\n","        synthetic_labels = [inverse_label_encoder[label] for label in synthetic_labels_raw]\n","\n","        # Add class labels to the synthetic dataframe\n","        synthetic_df['class'] = synthetic_labels\n","\n","        # Save the synthetic data\n","        # synthetic_df.to_csv(f'synthetic_car_data_{\"shuffled_\" if shuffle else \"\"}fold_{fold+1}.csv', index=False)\n","\n","        # Plot distributions\n","        # plot_categorical_distributions(X_original_train, synthetic_df.drop('class', axis=1), fold=fold, shuffle=shuffle)\n","\n","        # Plot class distribution\n","        # plot_class_distribution(y_original.iloc[train_idx], synthetic_df['class'], label_encoder, fold=fold, shuffle=shuffle)\n","\n","        # Statistical similarity evaluation\n","        print(\"Evaluating statistical similarity...\")\n","        stat_results = evaluate_categorical_similarity(X_original_train, synthetic_df.drop('class', axis=1))\n","\n","        print(\"\\nJensen-Shannon Divergence (average):\", stat_results['JSD_avg'])\n","        print(\"\\nJSD per feature:\")\n","        for feature, jsd in stat_results['JSD'].items():\n","            print(f\"  {feature}: {jsd:.4f}\")\n","\n","        # Machine Learning Utility (TSTR) evaluation\n","        print(\"\\nEvaluating Machine Learning Utility (TSTR)...\")\n","        tstr_results = evaluate_tstr(X_transformed, synthetic_data_raw, y_encoded)\n","\n","        print(\"\\nTSTR Results:\")\n","        for clf_name, metrics in tstr_results.items():\n","            print(f\"{clf_name}: Accuracy = {metrics['accuracy']:.4f}, F1 Score = {metrics['f1_score']:.4f}\")\n","\n","        # Store results for this fold\n","        fold_results[f\"{'shuffled_' if shuffle else ''}fold_{fold+1}\"] = {\n","            'tstr': tstr_results,\n","            'jsd': stat_results\n","        }\n","\n","        # Collect results for averaging later\n","        tstr_results_all.append(tstr_results)\n","        jsd_results_all.append(stat_results)\n","\n","    return fold_results, tstr_results_all, jsd_results_all\n","\n","def aggregate_results(results_list, result_type='tstr'):\n","    \"\"\"\n","    Aggregate results across all folds and runs\n","    \"\"\"\n","    if result_type == 'tstr':\n","        # Initialize aggregate dictionary\n","        aggregate = {}\n","\n","        # Get all classifier names from the first result\n","        classifiers = list(results_list[0].keys())\n","\n","        # Initialize aggregate dictionary with metrics\n","        for clf in classifiers:\n","            aggregate[clf] = {'accuracy': [], 'f1_score': []}\n","\n","        # Collect all results\n","        for result in results_list:\n","            for clf, metrics in result.items():\n","                aggregate[clf]['accuracy'].append(metrics['accuracy'])\n","                aggregate[clf]['f1_score'].append(metrics['f1_score'])\n","\n","        # Calculate averages and standard deviations\n","        final_results = {}\n","        for clf, metrics in aggregate.items():\n","            final_results[clf] = {\n","                'accuracy_mean': np.mean(metrics['accuracy']),\n","                'accuracy_std': np.std(metrics['accuracy']),\n","                'f1_score_mean': np.mean(metrics['f1_score']),\n","                'f1_score_std': np.std(metrics['f1_score'])\n","            }\n","\n","        return final_results\n","\n","    elif result_type == 'jsd':\n","        # Initialize aggregate dictionary\n","        aggregate = {'JSD': {}, 'JSD_avg': []}\n","\n","        # Get all feature names from the first result\n","        features = list(results_list[0]['JSD'].keys())\n","\n","        # Initialize feature-specific JSD lists\n","        for feature in features:\n","            aggregate['JSD'][feature] = []\n","\n","        # Collect all results\n","        for result in results_list:\n","            for feature, jsd in result['JSD'].items():\n","                aggregate['JSD'][feature].append(jsd)\n","            aggregate['JSD_avg'].append(result['JSD_avg'])\n","\n","        # Calculate averages and standard deviations\n","        final_results = {'JSD': {}, 'JSD_avg_mean': np.mean(aggregate['JSD_avg']), 'JSD_avg_std': np.std(aggregate['JSD_avg'])}\n","\n","        for feature, jsd_values in aggregate['JSD'].items():\n","            final_results['JSD'][feature] = {\n","                'mean': np.mean(jsd_values),\n","                'std': np.std(jsd_values)\n","            }\n","\n","        return final_results\n","\n","def main():\n","    # File path\n","    data_path = 'car.csv'\n","    n_folds = 5\n","    epochs = 300\n","\n","    # Run cross-validation with original data order\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"RUNNING CROSS-VALIDATION WITH ORIGINAL DATA ORDER\")\n","    print(\"=\"*80 + \"\\n\")\n","    original_fold_results, original_tstr_results, original_jsd_results = run_cross_validation(\n","        data_path, shuffle=False, n_folds=n_folds, epochs=epochs\n","    )\n","\n","    # Run cross-validation with shuffled data\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"RUNNING CROSS-VALIDATION WITH SHUFFLED DATA (SEED=123)\")\n","    print(\"=\"*80 + \"\\n\")\n","    shuffled_fold_results, shuffled_tstr_results, shuffled_jsd_results = run_cross_validation(\n","        data_path, shuffle=True, random_state=123, n_folds=n_folds, epochs=epochs\n","    )\n","\n","    # Combine results from both runs\n","    all_tstr_results = original_tstr_results + shuffled_tstr_results\n","    all_jsd_results = original_jsd_results + shuffled_jsd_results\n","\n","    # Aggregate results\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"AGGREGATING RESULTS ACROSS ALL 10 RUNS\")\n","    print(\"=\"*80 + \"\\n\")\n","\n","    # Aggregate TSTR results\n","    agg_tstr_results = aggregate_results(all_tstr_results, result_type='tstr')\n","\n","    print(\"\\nAverage Machine Learning Utility (TSTR) Results:\")\n","    print(\"-\" * 80)\n","    print(f\"{'Classifier':<20} | {'Accuracy Mean':<15} | {'Accuracy Std':<15} | {'F1 Score Mean':<15} | {'F1 Score Std':<15}\")\n","    print(\"-\" * 80)\n","    for clf_name, metrics in agg_tstr_results.items():\n","        print(f\"{clf_name:<20} | {metrics['accuracy_mean']:.4f} ± {metrics['accuracy_std']:.4f} | \" +\n","              f\"{metrics['accuracy_std']:.4f} | {metrics['f1_score_mean']:.4f} | {metrics['f1_score_std']:.4f}\")\n","\n","    # Aggregate JSD results\n","    agg_jsd_results = aggregate_results(all_jsd_results, result_type='jsd')\n","\n","    print(\"\\nAverage Statistical Similarity Results (JSD):\")\n","    print(\"-\" * 60)\n","    print(f\"Overall JSD: {agg_jsd_results['JSD_avg_mean']:.4f} ± {agg_jsd_results['JSD_avg_std']:.4f}\")\n","    print(\"\\nFeature-specific JSDs:\")\n","    print(f\"{'Feature':<15} | {'JSD Mean':<15} | {'JSD Std':<15}\")\n","    print(\"-\" * 60)\n","    for feature, metrics in agg_jsd_results['JSD'].items():\n","        print(f\"{feature:<15} | {metrics['mean']:.4f} | {metrics['std']:.4f}\")\n","\n","    # Create and save summary results\n","    summary_results = {\n","        \"Machine Learning Utility\": agg_tstr_results,\n","        \"Statistical Similarity\": agg_jsd_results\n","    }\n","\n","    # Save all results to disk\n","    import pickle\n","    with open('car_crgan_cross_validation_results.pkl', 'wb') as f:\n","        pickle.dump({\n","            'original_fold_results': original_fold_results,\n","            'shuffled_fold_results': shuffled_fold_results,\n","            'aggregated_results': summary_results\n","        }, f)\n","\n","    # Create summary plots\n","    create_summary_plots(agg_tstr_results, agg_jsd_results)\n","\n","    print(\"\\nEvaluation complete! Check the output directory for plots and saved models.\")\n","\n","def create_summary_plots(tstr_results, jsd_results):\n","    \"\"\"\n","    Create summary plots for the cross-validation results\n","    \"\"\"\n","    # 1. Create accuracy bar plot\n","    plt.figure(figsize=(12, 6))\n","    classifiers = list(tstr_results.keys())\n","    accuracies = [tstr_results[clf]['accuracy_mean'] for clf in classifiers]\n","    errors = [tstr_results[clf]['accuracy_std'] for clf in classifiers]\n","\n","    bar_positions = np.arange(len(classifiers))\n","    plt.bar(bar_positions, accuracies, yerr=errors, alpha=0.7, capsize=10)\n","    plt.xticks(bar_positions, classifiers, rotation=45)\n","    plt.title('Average Classification Accuracy Across 10 Runs')\n","    plt.xlabel('Classifier')\n","    plt.ylabel('Accuracy')\n","    plt.tight_layout()\n","    plt.savefig('car_crgan_avg_accuracy.png')\n","    plt.close()\n","\n","    # 2. Create F1 score bar plot\n","    plt.figure(figsize=(12, 6))\n","    f1_scores = [tstr_results[clf]['f1_score_mean'] for clf in classifiers]\n","    f1_errors = [tstr_results[clf]['f1_score_std'] for clf in classifiers]\n","\n","    plt.bar(bar_positions, f1_scores, yerr=f1_errors, alpha=0.7, capsize=10)\n","    plt.xticks(bar_positions, classifiers, rotation=45)\n","    plt.title('Average F1 Score Across 10 Runs')\n","    plt.xlabel('Classifier')\n","    plt.ylabel('F1 Score')\n","    plt.tight_layout()\n","    plt.savefig('car_crgan_avg_f1_score.png')\n","    plt.close()\n","\n","    # 3. Create JSD bar plot\n","    plt.figure(figsize=(12, 6))\n","    features = list(jsd_results['JSD'].keys())\n","    jsds = [jsd_results['JSD'][feature]['mean'] for feature in features]\n","    jsd_errors = [jsd_results['JSD'][feature]['std'] for feature in features]\n","\n","    bar_positions = np.arange(len(features))\n","    plt.bar(bar_positions, jsds, yerr=jsd_errors, alpha=0.7, capsize=10)\n","    plt.xticks(bar_positions, features, rotation=45)\n","    plt.title('Average Jensen-Shannon Divergence Across 10 Runs')\n","    plt.xlabel('Feature')\n","    plt.ylabel('JSD')\n","    plt.tight_layout()\n","    plt.savefig('car_crgan_avg_jsd.png')\n","    plt.close()\n","\n","    # 4. Create boxplot of JSD values per feature\n","    plt.figure(figsize=(14, 8))\n","\n","    # Prepare data for boxplot\n","    feature_jsd_values = []\n","    for feature in features:\n","        values = []\n","        for fold in range(10):  # 5 original + 5 shuffled\n","            fold_idx = fold // 5\n","            within_fold_idx = fold % 5\n","            if fold_idx == 0:\n","                # Original data\n","                fold_name = f\"fold_{within_fold_idx+1}\"\n","            else:\n","                # Shuffled data\n","                fold_name = f\"shuffled_fold_{within_fold_idx+1}\"\n","        feature_jsd_values.append(values)\n","\n","    plt.boxplot(feature_jsd_values, labels=features)\n","    plt.title('Distribution of JSD Values Across All Runs')\n","    plt.xlabel('Feature')\n","    plt.ylabel('JSD')\n","    plt.xticks(rotation=45)\n","    plt.tight_layout()\n","    plt.savefig('car_crgan_jsd_distribution.png')\n","    plt.close()\n"]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UOE4thcv5unC","executionInfo":{"status":"ok","timestamp":1744346682112,"user_tz":-600,"elapsed":3678275,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}},"outputId":"64e9da9d-c8f8-4115-8a2e-3d1d33dd8513"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","RUNNING CROSS-VALIDATION WITH ORIGINAL DATA ORDER\n","================================================================================\n","\n","Dataset shape: (1729, 7)\n","Processed data shape: (1729, 27)\n","\n","==================================================\n","Running Fold 1/5\n","==================================================\n","Data dimension: 27\n","Latent dimension: 100\n","Training CramerGAN for 300 epochs...\n","Epoch [1/300] | Critic Loss: 3.8569 | Generator Loss: -0.5922 | Time: 5.71s\n","Epoch [21/300] | Critic Loss: -373.8811 | Generator Loss: 77.6456 | Time: 1.06s\n","Epoch [41/300] | Critic Loss: -227967.2472 | Generator Loss: 71002.2269 | Time: 1.04s\n","Epoch [61/300] | Critic Loss: -9044872.2273 | Generator Loss: 3013995.6136 | Time: 1.31s\n","Epoch [81/300] | Critic Loss: -108174624.7273 | Generator Loss: 40998244.9091 | Time: 1.13s\n","Epoch [101/300] | Critic Loss: -787329879.2727 | Generator Loss: 261220960.0000 | Time: 1.15s\n","Epoch [121/300] | Critic Loss: -3069637236.3636 | Generator Loss: 1215560768.0000 | Time: 1.14s\n","Epoch [141/300] | Critic Loss: -10387100997.8182 | Generator Loss: 4504132072.7273 | Time: 1.35s\n","Epoch [161/300] | Critic Loss: -36595777349.8182 | Generator Loss: 13410617157.8182 | Time: 1.07s\n","Epoch [181/300] | Critic Loss: -90788639464.7273 | Generator Loss: 37471934836.3636 | Time: 1.04s\n","Epoch [201/300] | Critic Loss: -251183894900.3636 | Generator Loss: 83579928203.6364 | Time: 1.61s\n","Epoch [221/300] | Critic Loss: -443127904814.5455 | Generator Loss: 192520729693.0909 | Time: 1.09s\n","Epoch [241/300] | Critic Loss: -1095991343662.5454 | Generator Loss: 371806508869.8182 | Time: 1.07s\n","Epoch [261/300] | Critic Loss: -2179934209489.4546 | Generator Loss: 726675333864.7273 | Time: 1.57s\n","Epoch [281/300] | Critic Loss: -3587548047546.1816 | Generator Loss: 1420969353960.7273 | Time: 1.06s\n","Epoch [300/300] | Critic Loss: -6592144986484.3633 | Generator Loss: 2522515731549.0908 | Time: 1.07s\n","Generating 1000 synthetic samples...\n","Evaluating statistical similarity...\n","\n","Jensen-Shannon Divergence (average): 0.4624940682218055\n","\n","JSD per feature:\n","  buying: 0.6902\n","  maint: 0.3762\n","  doors: 0.3779\n","  persons: 0.3188\n","  lug_boot: 0.6931\n","  safety: 0.3188\n","\n","Evaluating Machine Learning Utility (TSTR)...\n","\n","TSTR Results:\n","Logistic Regression: Accuracy = 0.6994, F1 Score = 0.5757\n","MLP: Accuracy = 0.6185, F1 Score = 0.5715\n","Random Forest: Accuracy = 0.6994, F1 Score = 0.5757\n","XGBoost: Accuracy = 0.5087, F1 Score = 0.5113\n","\n","==================================================\n","Running Fold 2/5\n","==================================================\n","Data dimension: 27\n","Latent dimension: 100\n","Training CramerGAN for 300 epochs...\n","Epoch [1/300] | Critic Loss: 3.1758 | Generator Loss: -0.5114 | Time: 1.11s\n","Epoch [21/300] | Critic Loss: -530.1567 | Generator Loss: 135.0250 | Time: 1.14s\n","Epoch [41/300] | Critic Loss: -273414.9602 | Generator Loss: 90229.7727 | Time: 1.52s\n","Epoch [61/300] | Critic Loss: -10640963.6364 | Generator Loss: 3471608.3636 | Time: 1.26s\n","Epoch [81/300] | Critic Loss: -126351905.4545 | Generator Loss: 39610753.0909 | Time: 1.12s\n","Epoch [101/300] | Critic Loss: -691969224.7273 | Generator Loss: 277390352.0000 | Time: 1.09s\n","Epoch [121/300] | Critic Loss: -3366457506.9091 | Generator Loss: 1330983575.2727 | Time: 1.41s\n","Epoch [141/300] | Critic Loss: -13051297605.8182 | Generator Loss: 4473500695.2727 | Time: 1.10s\n","Epoch [161/300] | Critic Loss: -36795034530.9091 | Generator Loss: 13473702632.7273 | Time: 1.17s\n","Epoch [181/300] | Critic Loss: -94462532514.9091 | Generator Loss: 37270104994.9091 | Time: 1.05s\n","Epoch [201/300] | Critic Loss: -249976919877.8182 | Generator Loss: 88631195461.8182 | Time: 1.60s\n","Epoch [221/300] | Critic Loss: -511090828008.7273 | Generator Loss: 174943587234.9091 | Time: 1.06s\n","Epoch [241/300] | Critic Loss: -1135656191441.4546 | Generator Loss: 383596759970.9091 | Time: 1.08s\n","Epoch [261/300] | Critic Loss: -1680769721064.7273 | Generator Loss: 738092735767.2727 | Time: 1.59s\n","Epoch [281/300] | Critic Loss: -3518861553291.6362 | Generator Loss: 1431076196724.3635 | Time: 1.09s\n","Epoch [300/300] | Critic Loss: -5992150633378.9092 | Generator Loss: 2287973540584.7271 | Time: 1.09s\n","Generating 1000 synthetic samples...\n","Evaluating statistical similarity...\n","\n","Jensen-Shannon Divergence (average): 0.46134994399483853\n","\n","JSD per feature:\n","  buying: 0.6869\n","  maint: 0.3826\n","  doors: 0.3713\n","  persons: 0.3123\n","  lug_boot: 0.6931\n","  safety: 0.3218\n","\n","Evaluating Machine Learning Utility (TSTR)...\n","\n","TSTR Results:\n","Logistic Regression: Accuracy = 0.6994, F1 Score = 0.5757\n","MLP: Accuracy = 0.4046, F1 Score = 0.4105\n","Random Forest: Accuracy = 0.6994, F1 Score = 0.5757\n","XGBoost: Accuracy = 0.5491, F1 Score = 0.5394\n","\n","==================================================\n","Running Fold 3/5\n","==================================================\n","Data dimension: 27\n","Latent dimension: 100\n","Training CramerGAN for 300 epochs...\n","Epoch [1/300] | Critic Loss: 3.7511 | Generator Loss: -0.1380 | Time: 1.12s\n","Epoch [21/300] | Critic Loss: -319.2785 | Generator Loss: 79.9221 | Time: 1.12s\n","Epoch [41/300] | Critic Loss: -230755.2429 | Generator Loss: 72993.9173 | Time: 1.16s\n","Epoch [61/300] | Critic Loss: -10387605.5455 | Generator Loss: 3381999.2500 | Time: 1.07s\n","Epoch [81/300] | Critic Loss: -95421922.5455 | Generator Loss: 43220484.3636 | Time: 1.10s\n","Epoch [101/300] | Critic Loss: -638258516.3636 | Generator Loss: 274913920.0000 | Time: 1.13s\n","Epoch [121/300] | Critic Loss: -3504696413.0909 | Generator Loss: 1347016657.4545 | Time: 1.31s\n","Epoch [141/300] | Critic Loss: -13899209914.1818 | Generator Loss: 4567399377.4545 | Time: 1.33s\n","Epoch [161/300] | Critic Loss: -37579344244.3636 | Generator Loss: 14844608325.8182 | Time: 1.11s\n","Epoch [181/300] | Critic Loss: -86419899112.7273 | Generator Loss: 39126984145.4545 | Time: 1.11s\n","Epoch [201/300] | Critic Loss: -222296061021.0909 | Generator Loss: 92624514699.6364 | Time: 1.31s\n","Epoch [221/300] | Critic Loss: -497831876421.8182 | Generator Loss: 193864792622.5454 | Time: 1.45s\n","Epoch [241/300] | Critic Loss: -998596774818.9091 | Generator Loss: 374390363973.8182 | Time: 1.16s\n","Epoch [261/300] | Critic Loss: -1869975233256.7273 | Generator Loss: 771411381154.9091 | Time: 1.13s\n","Epoch [281/300] | Critic Loss: -3748115275031.2729 | Generator Loss: 1392670950865.4546 | Time: 1.10s\n","Epoch [300/300] | Critic Loss: -6599449033262.5459 | Generator Loss: 2479249162240.0000 | Time: 1.70s\n","Generating 1000 synthetic samples...\n","Evaluating statistical similarity...\n","\n","Jensen-Shannon Divergence (average): 0.46573474829778516\n","\n","JSD per feature:\n","  buying: 0.6931\n","  maint: 0.3797\n","  doors: 0.3808\n","  persons: 0.3243\n","  lug_boot: 0.6931\n","  safety: 0.3233\n","\n","Evaluating Machine Learning Utility (TSTR)...\n","\n","TSTR Results:\n","Logistic Regression: Accuracy = 0.6994, F1 Score = 0.5757\n","MLP: Accuracy = 0.5462, F1 Score = 0.5285\n","Random Forest: Accuracy = 0.6763, F1 Score = 0.5653\n","XGBoost: Accuracy = 0.5867, F1 Score = 0.5627\n","\n","==================================================\n","Running Fold 4/5\n","==================================================\n","Data dimension: 27\n","Latent dimension: 100\n","Training CramerGAN for 300 epochs...\n","Epoch [1/300] | Critic Loss: 3.4437 | Generator Loss: -0.7951 | Time: 1.10s\n","Epoch [21/300] | Critic Loss: -492.0592 | Generator Loss: 137.2734 | Time: 1.70s\n","Epoch [41/300] | Critic Loss: -281146.0227 | Generator Loss: 91337.1591 | Time: 1.16s\n","Epoch [61/300] | Critic Loss: -11396663.6818 | Generator Loss: 3425796.5909 | Time: 1.12s\n","Epoch [81/300] | Critic Loss: -130001853.0909 | Generator Loss: 38447704.7273 | Time: 1.12s\n","Epoch [101/300] | Critic Loss: -743142458.1818 | Generator Loss: 271413976.7273 | Time: 1.17s\n","Epoch [121/300] | Critic Loss: -3056524381.0909 | Generator Loss: 1234405643.6364 | Time: 1.37s\n","Epoch [141/300] | Critic Loss: -11590999831.2727 | Generator Loss: 4489317562.1818 | Time: 1.11s\n","Epoch [161/300] | Critic Loss: -35665494946.9091 | Generator Loss: 14146720302.5455 | Time: 1.12s\n","Epoch [181/300] | Critic Loss: -104274225338.1818 | Generator Loss: 35566232296.7273 | Time: 1.25s\n","Epoch [201/300] | Critic Loss: -213659944401.4546 | Generator Loss: 88004372666.1818 | Time: 1.29s\n","Epoch [221/300] | Critic Loss: -517345052113.4545 | Generator Loss: 187953697140.3636 | Time: 1.09s\n","Epoch [241/300] | Critic Loss: -1008500992186.1818 | Generator Loss: 361812355258.1818 | Time: 1.12s\n","Epoch [261/300] | Critic Loss: -1909328159464.7273 | Generator Loss: 667640004608.0000 | Time: 1.57s\n","Epoch [281/300] | Critic Loss: -3324740870888.7271 | Generator Loss: 1333645237527.2727 | Time: 1.10s\n","Epoch [300/300] | Critic Loss: -5550805566929.4541 | Generator Loss: 2328079213288.7271 | Time: 1.10s\n","Generating 1000 synthetic samples...\n","Evaluating statistical similarity...\n","\n","Jensen-Shannon Divergence (average): 0.4619656476295028\n","\n","JSD per feature:\n","  buying: 0.6902\n","  maint: 0.3722\n","  doors: 0.3803\n","  persons: 0.3148\n","  lug_boot: 0.6931\n","  safety: 0.3213\n","\n","Evaluating Machine Learning Utility (TSTR)...\n","\n","TSTR Results:\n","Logistic Regression: Accuracy = 0.6994, F1 Score = 0.5757\n","MLP: Accuracy = 0.5434, F1 Score = 0.5077\n","Random Forest: Accuracy = 0.5462, F1 Score = 0.4942\n","XGBoost: Accuracy = 0.3613, F1 Score = 0.4051\n","\n","==================================================\n","Running Fold 5/5\n","==================================================\n","Data dimension: 27\n","Latent dimension: 100\n","Training CramerGAN for 300 epochs...\n","Epoch [1/300] | Critic Loss: 3.4508 | Generator Loss: -0.7384 | Time: 1.08s\n","Epoch [21/300] | Critic Loss: -330.0250 | Generator Loss: 73.5988 | Time: 1.08s\n","Epoch [41/300] | Critic Loss: -202914.0909 | Generator Loss: 71913.8285 | Time: 1.50s\n","Epoch [61/300] | Critic Loss: -8244584.5909 | Generator Loss: 2832329.1591 | Time: 1.11s\n","Epoch [81/300] | Critic Loss: -113073303.2727 | Generator Loss: 37030051.0909 | Time: 1.11s\n","Epoch [101/300] | Critic Loss: -740930978.9091 | Generator Loss: 240161303.2727 | Time: 1.15s\n","Epoch [121/300] | Critic Loss: -3407891921.4545 | Generator Loss: 1226579118.5455 | Time: 1.39s\n","Epoch [141/300] | Critic Loss: -13262406469.8182 | Generator Loss: 4629522106.1818 | Time: 1.25s\n","Epoch [161/300] | Critic Loss: -32342886213.8182 | Generator Loss: 12078048628.3636 | Time: 1.12s\n","Epoch [181/300] | Critic Loss: -89977444165.8182 | Generator Loss: 35746400628.3636 | Time: 1.13s\n","Epoch [201/300] | Critic Loss: -273487233024.0000 | Generator Loss: 86055119034.1818 | Time: 1.21s\n","Epoch [221/300] | Critic Loss: -460004982784.0000 | Generator Loss: 182492296285.0909 | Time: 1.06s\n","Epoch [241/300] | Critic Loss: -1100022987310.5454 | Generator Loss: 418854130036.3636 | Time: 1.07s\n","Epoch [261/300] | Critic Loss: -2220903998184.7271 | Generator Loss: 753726253242.1818 | Time: 1.07s\n","Epoch [281/300] | Critic Loss: -3670982477265.4546 | Generator Loss: 1267994650810.1819 | Time: 1.66s\n","Epoch [300/300] | Critic Loss: -6156463040698.1816 | Generator Loss: 2300301791604.3638 | Time: 1.08s\n","Generating 1000 synthetic samples...\n","Evaluating statistical similarity...\n","\n","Jensen-Shannon Divergence (average): 0.4626111828449167\n","\n","JSD per feature:\n","  buying: 0.6902\n","  maint: 0.3778\n","  doors: 0.3857\n","  persons: 0.3154\n","  lug_boot: 0.6931\n","  safety: 0.3134\n","\n","Evaluating Machine Learning Utility (TSTR)...\n","\n","TSTR Results:\n","Logistic Regression: Accuracy = 0.6994, F1 Score = 0.5757\n","MLP: Accuracy = 0.6012, F1 Score = 0.5418\n","Random Forest: Accuracy = 0.6994, F1 Score = 0.5757\n","XGBoost: Accuracy = 0.4653, F1 Score = 0.4945\n","\n","================================================================================\n","RUNNING CROSS-VALIDATION WITH SHUFFLED DATA (SEED=123)\n","================================================================================\n","\n","Dataset shape: (1729, 7)\n","Processed data shape: (1729, 27)\n","\n","==================================================\n","Running Shuffled Fold 1/5\n","==================================================\n","Data dimension: 27\n","Latent dimension: 100\n","Training CramerGAN for 300 epochs...\n","Epoch [1/300] | Critic Loss: 3.8575 | Generator Loss: -0.2267 | Time: 1.07s\n","Epoch [21/300] | Critic Loss: -385.2453 | Generator Loss: 104.6850 | Time: 1.07s\n","Epoch [41/300] | Critic Loss: -305254.0185 | Generator Loss: 113512.8374 | Time: 1.06s\n","Epoch [61/300] | Critic Loss: -11269363.2727 | Generator Loss: 3940871.9318 | Time: 1.37s\n","Epoch [81/300] | Critic Loss: -132542722.1818 | Generator Loss: 48426039.6364 | Time: 1.08s\n","Epoch [101/300] | Critic Loss: -820684200.7273 | Generator Loss: 326851720.7273 | Time: 1.09s\n","Epoch [121/300] | Critic Loss: -3829586501.8182 | Generator Loss: 1550815930.1818 | Time: 1.62s\n","Epoch [141/300] | Critic Loss: -14086044765.0909 | Generator Loss: 5749383959.2727 | Time: 1.06s\n","Epoch [161/300] | Critic Loss: -45347347735.2727 | Generator Loss: 17966195805.0909 | Time: 1.08s\n","Epoch [181/300] | Critic Loss: -119038487458.9091 | Generator Loss: 47155702690.9091 | Time: 1.07s\n","Epoch [201/300] | Critic Loss: -284957080110.5455 | Generator Loss: 108127698199.2727 | Time: 1.31s\n","Epoch [221/300] | Critic Loss: -559818446661.8182 | Generator Loss: 243416812450.9091 | Time: 1.13s\n","Epoch [241/300] | Critic Loss: -1302579553000.7273 | Generator Loss: 474190902365.0909 | Time: 1.06s\n","Epoch [261/300] | Critic Loss: -1940612948712.7273 | Generator Loss: 922319740183.2727 | Time: 1.67s\n","Epoch [281/300] | Critic Loss: -4515136488913.4541 | Generator Loss: 1719868361634.9092 | Time: 1.09s\n","Epoch [300/300] | Critic Loss: -6911031699642.1816 | Generator Loss: 2777585859118.5454 | Time: 1.05s\n","Generating 1000 synthetic samples...\n","Evaluating statistical similarity...\n","\n","Jensen-Shannon Divergence (average): 0.46449708584765365\n","\n","JSD per feature:\n","  buying: 0.6931\n","  maint: 0.3838\n","  doors: 0.3814\n","  persons: 0.3193\n","  lug_boot: 0.6931\n","  safety: 0.3163\n","\n","Evaluating Machine Learning Utility (TSTR)...\n","\n","TSTR Results:\n","Logistic Regression: Accuracy = 0.7341, F1 Score = 0.6215\n","MLP: Accuracy = 0.4509, F1 Score = 0.4852\n","Random Forest: Accuracy = 0.7312, F1 Score = 0.6431\n","XGBoost: Accuracy = 0.4393, F1 Score = 0.4965\n","\n","==================================================\n","Running Shuffled Fold 2/5\n","==================================================\n","Data dimension: 27\n","Latent dimension: 100\n","Training CramerGAN for 300 epochs...\n","Epoch [1/300] | Critic Loss: 3.6537 | Generator Loss: -0.5394 | Time: 1.08s\n","Epoch [21/300] | Critic Loss: -546.0167 | Generator Loss: 127.8291 | Time: 1.07s\n","Epoch [41/300] | Critic Loss: -297333.2756 | Generator Loss: 105757.0597 | Time: 1.74s\n","Epoch [61/300] | Critic Loss: -11237161.5909 | Generator Loss: 3958891.0909 | Time: 1.08s\n","Epoch [81/300] | Critic Loss: -122401637.8182 | Generator Loss: 52016458.1818 | Time: 1.09s\n","Epoch [101/300] | Critic Loss: -803047965.0909 | Generator Loss: 317579179.6364 | Time: 1.10s\n","Epoch [121/300] | Critic Loss: -3736372887.2727 | Generator Loss: 1604764800.0000 | Time: 1.22s\n","Epoch [141/300] | Critic Loss: -14826557160.7273 | Generator Loss: 5276095441.4545 | Time: 1.08s\n","Epoch [161/300] | Critic Loss: -36948401431.2727 | Generator Loss: 16720930443.6364 | Time: 1.07s\n","Epoch [181/300] | Critic Loss: -126920193861.8182 | Generator Loss: 42443021591.2727 | Time: 1.68s\n","Epoch [201/300] | Critic Loss: -256847936046.5454 | Generator Loss: 102021812596.3636 | Time: 1.07s\n","Epoch [221/300] | Critic Loss: -600239576901.8182 | Generator Loss: 225362703453.0909 | Time: 1.05s\n","Epoch [241/300] | Critic Loss: -1093037808919.2727 | Generator Loss: 481519185547.6364 | Time: 1.07s\n","Epoch [261/300] | Critic Loss: -2271973188328.7271 | Generator Loss: 892347309335.2727 | Time: 1.42s\n","Epoch [281/300] | Critic Loss: -4177493015086.5454 | Generator Loss: 1655301245858.9092 | Time: 1.14s\n","Epoch [300/300] | Critic Loss: -7434386919796.3633 | Generator Loss: 2846134298810.1816 | Time: 1.09s\n","Generating 1000 synthetic samples...\n","Evaluating statistical similarity...\n","\n","Jensen-Shannon Divergence (average): 0.46384045199289076\n","\n","JSD per feature:\n","  buying: 0.6902\n","  maint: 0.3855\n","  doors: 0.3832\n","  persons: 0.3183\n","  lug_boot: 0.6931\n","  safety: 0.3128\n","\n","Evaluating Machine Learning Utility (TSTR)...\n","\n","TSTR Results:\n","Logistic Regression: Accuracy = 0.7341, F1 Score = 0.6215\n","MLP: Accuracy = 0.4393, F1 Score = 0.4686\n","Random Forest: Accuracy = 0.6879, F1 Score = 0.6333\n","XGBoost: Accuracy = 0.4480, F1 Score = 0.5156\n","\n","==================================================\n","Running Shuffled Fold 3/5\n","==================================================\n","Data dimension: 27\n","Latent dimension: 100\n","Training CramerGAN for 300 epochs...\n","Epoch [1/300] | Critic Loss: 3.8115 | Generator Loss: -0.4858 | Time: 1.09s\n","Epoch [21/300] | Critic Loss: -484.2222 | Generator Loss: 113.9603 | Time: 1.08s\n","Epoch [41/300] | Critic Loss: -338621.8438 | Generator Loss: 109127.4822 | Time: 1.62s\n","Epoch [61/300] | Critic Loss: -13451748.4091 | Generator Loss: 4094972.4773 | Time: 1.06s\n","Epoch [81/300] | Critic Loss: -132192502.5455 | Generator Loss: 50741679.2727 | Time: 1.07s\n","Epoch [101/300] | Critic Loss: -795934894.5455 | Generator Loss: 308691774.5455 | Time: 1.63s\n","Epoch [121/300] | Critic Loss: -4009726324.3636 | Generator Loss: 1660251077.8182 | Time: 1.08s\n","Epoch [141/300] | Critic Loss: -16798008506.1818 | Generator Loss: 5473786740.3636 | Time: 1.11s\n","Epoch [161/300] | Critic Loss: -42927074210.9091 | Generator Loss: 17373924910.5455 | Time: 1.08s\n","Epoch [181/300] | Critic Loss: -111991684375.2727 | Generator Loss: 45890925661.0909 | Time: 1.62s\n","Epoch [201/300] | Critic Loss: -294347785681.4545 | Generator Loss: 104087391883.6364 | Time: 1.07s\n","Epoch [221/300] | Critic Loss: -590962550970.1818 | Generator Loss: 233656932165.8182 | Time: 1.06s\n","Epoch [241/300] | Critic Loss: -1209377542516.3635 | Generator Loss: 506610652997.8182 | Time: 1.53s\n","Epoch [261/300] | Critic Loss: -1924953633885.0908 | Generator Loss: 863227996346.1818 | Time: 1.06s\n","Epoch [281/300] | Critic Loss: -4379058825402.1816 | Generator Loss: 1712745575517.0908 | Time: 1.08s\n","Epoch [300/300] | Critic Loss: -7780748731112.7275 | Generator Loss: 2756958951237.8184 | Time: 1.19s\n","Generating 1000 synthetic samples...\n","Evaluating statistical similarity...\n","\n","Jensen-Shannon Divergence (average): 0.46614533688404974\n","\n","JSD per feature:\n","  buying: 0.6902\n","  maint: 0.3909\n","  doors: 0.3832\n","  persons: 0.3198\n","  lug_boot: 0.6931\n","  safety: 0.3198\n","\n","Evaluating Machine Learning Utility (TSTR)...\n","\n","TSTR Results:\n","Logistic Regression: Accuracy = 0.7341, F1 Score = 0.6215\n","MLP: Accuracy = 0.6185, F1 Score = 0.6056\n","Random Forest: Accuracy = 0.6994, F1 Score = 0.6167\n","XGBoost: Accuracy = 0.5520, F1 Score = 0.5599\n","\n","==================================================\n","Running Shuffled Fold 4/5\n","==================================================\n","Data dimension: 27\n","Latent dimension: 100\n","Training CramerGAN for 300 epochs...\n","Epoch [1/300] | Critic Loss: 3.7333 | Generator Loss: -0.7913 | Time: 1.07s\n","Epoch [21/300] | Critic Loss: -670.4007 | Generator Loss: 151.1689 | Time: 1.61s\n","Epoch [41/300] | Critic Loss: -312233.4375 | Generator Loss: 111858.3991 | Time: 1.11s\n","Epoch [61/300] | Critic Loss: -10939019.1364 | Generator Loss: 4052654.8864 | Time: 1.09s\n","Epoch [81/300] | Critic Loss: -132589231.2727 | Generator Loss: 47031692.7273 | Time: 1.09s\n","Epoch [101/300] | Critic Loss: -895728896.0000 | Generator Loss: 322076424.7273 | Time: 1.66s\n","Epoch [121/300] | Critic Loss: -4170884608.0000 | Generator Loss: 1321862213.8182 | Time: 1.09s\n","Epoch [141/300] | Critic Loss: -16106461556.3636 | Generator Loss: 5616693736.7273 | Time: 1.16s\n","Epoch [161/300] | Critic Loss: -45325104779.6364 | Generator Loss: 17320263959.2727 | Time: 1.13s\n","Epoch [181/300] | Critic Loss: -105805656436.3636 | Generator Loss: 41031163345.4545 | Time: 1.61s\n","Epoch [201/300] | Critic Loss: -269383481902.5454 | Generator Loss: 101429138525.0909 | Time: 1.09s\n","Epoch [221/300] | Critic Loss: -609531833995.6364 | Generator Loss: 225469218816.0000 | Time: 1.08s\n","Epoch [241/300] | Critic Loss: -1184817157957.8181 | Generator Loss: 455779930484.3636 | Time: 1.12s\n","Epoch [261/300] | Critic Loss: -2110129343394.9092 | Generator Loss: 808848451770.1818 | Time: 1.63s\n","Epoch [281/300] | Critic Loss: -3748776712005.8184 | Generator Loss: 1454578836200.7273 | Time: 1.10s\n","Epoch [300/300] | Critic Loss: -6664330793332.3633 | Generator Loss: 2771125391732.3638 | Time: 1.09s\n","Generating 1000 synthetic samples...\n","Evaluating statistical similarity...\n","\n","Jensen-Shannon Divergence (average): 0.4627011697742766\n","\n","JSD per feature:\n","  buying: 0.6902\n","  maint: 0.3832\n","  doors: 0.3832\n","  persons: 0.3123\n","  lug_boot: 0.6931\n","  safety: 0.3143\n","\n","Evaluating Machine Learning Utility (TSTR)...\n","\n","TSTR Results:\n","Logistic Regression: Accuracy = 0.7341, F1 Score = 0.6215\n","MLP: Accuracy = 0.4133, F1 Score = 0.4503\n","Random Forest: Accuracy = 0.7341, F1 Score = 0.6215\n","XGBoost: Accuracy = 0.5289, F1 Score = 0.5756\n","\n","==================================================\n","Running Shuffled Fold 5/5\n","==================================================\n","Data dimension: 27\n","Latent dimension: 100\n","Training CramerGAN for 300 epochs...\n","Epoch [1/300] | Critic Loss: 3.1952 | Generator Loss: -0.6519 | Time: 1.09s\n","Epoch [21/300] | Critic Loss: -411.8332 | Generator Loss: 101.5195 | Time: 1.08s\n","Epoch [41/300] | Critic Loss: -244783.8651 | Generator Loss: 91032.7472 | Time: 1.37s\n","Epoch [61/300] | Critic Loss: -9521606.5455 | Generator Loss: 3454075.3636 | Time: 1.08s\n","Epoch [81/300] | Critic Loss: -105638377.4545 | Generator Loss: 41049938.9091 | Time: 1.06s\n","Epoch [101/300] | Critic Loss: -792969064.7273 | Generator Loss: 282075954.9091 | Time: 1.62s\n","Epoch [121/300] | Critic Loss: -3672015360.0000 | Generator Loss: 1352520541.0909 | Time: 1.08s\n","Epoch [141/300] | Critic Loss: -14525526388.3636 | Generator Loss: 4578353454.5455 | Time: 1.07s\n","Epoch [161/300] | Critic Loss: -38938791936.0000 | Generator Loss: 13887233675.6364 | Time: 1.35s\n","Epoch [181/300] | Critic Loss: -89404764160.0000 | Generator Loss: 38803044165.8182 | Time: 1.07s\n","Epoch [201/300] | Critic Loss: -221708505460.3636 | Generator Loss: 92201558760.7273 | Time: 1.06s\n","Epoch [221/300] | Critic Loss: -533654104250.1818 | Generator Loss: 200974301742.5454 | Time: 1.06s\n","Epoch [241/300] | Critic Loss: -984025125236.3636 | Generator Loss: 377472402897.4545 | Time: 1.29s\n","Epoch [261/300] | Critic Loss: -2195501112599.2727 | Generator Loss: 793256966702.5454 | Time: 1.09s\n","Epoch [281/300] | Critic Loss: -4238972408180.3638 | Generator Loss: 1452048848709.8181 | Time: 1.10s\n","Epoch [300/300] | Critic Loss: -6467102822772.3633 | Generator Loss: 2449440076893.0908 | Time: 1.15s\n","Generating 1000 synthetic samples...\n","Evaluating statistical similarity...\n","\n","Jensen-Shannon Divergence (average): 0.4626707547775238\n","\n","JSD per feature:\n","  buying: 0.6902\n","  maint: 0.3763\n","  doors: 0.3775\n","  persons: 0.3179\n","  lug_boot: 0.6931\n","  safety: 0.3209\n","\n","Evaluating Machine Learning Utility (TSTR)...\n","\n","TSTR Results:\n","Logistic Regression: Accuracy = 0.7341, F1 Score = 0.6215\n","MLP: Accuracy = 0.6185, F1 Score = 0.6141\n","Random Forest: Accuracy = 0.1156, F1 Score = 0.1531\n","XGBoost: Accuracy = 0.3064, F1 Score = 0.3944\n","\n","================================================================================\n","AGGREGATING RESULTS ACROSS ALL 10 RUNS\n","================================================================================\n","\n","\n","Average Machine Learning Utility (TSTR) Results:\n","--------------------------------------------------------------------------------\n","Classifier           | Accuracy Mean   | Accuracy Std    | F1 Score Mean   | F1 Score Std   \n","--------------------------------------------------------------------------------\n","Logistic Regression  | 0.7168 ± 0.0173 | 0.0173 | 0.5986 | 0.0229\n","MLP                  | 0.5254 ± 0.0852 | 0.0852 | 0.5184 | 0.0634\n","Random Forest        | 0.6289 ± 0.1782 | 0.1782 | 0.5454 | 0.1371\n","XGBoost              | 0.4746 ± 0.0845 | 0.0845 | 0.5055 | 0.0592\n","\n","Average Statistical Similarity Results (JSD):\n","------------------------------------------------------------\n","Overall JSD: 0.4634 ± 0.0015\n","\n","Feature-specific JSDs:\n","Feature         | JSD Mean        | JSD Std        \n","------------------------------------------------------------\n","buying          | 0.6904 | 0.0017\n","maint           | 0.3808 | 0.0052\n","doors           | 0.3804 | 0.0039\n","persons         | 0.3173 | 0.0035\n","lug_boot        | 0.6931 | 0.0000\n","safety          | 0.3183 | 0.0036\n","\n","Evaluation complete! Check the output directory for plots and saved models.\n"]}]}]}