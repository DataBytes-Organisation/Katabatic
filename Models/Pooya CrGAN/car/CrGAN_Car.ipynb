{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "947ac609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessed data saved to: preprocessed_car.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Data Preprocessing ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load car dataset\n",
    "df = pd.read_csv(\"car-official.csv\")\n",
    "\n",
    "# Encode target column\n",
    "le = LabelEncoder()\n",
    "df['class'] = le.fit_transform(df['class'])\n",
    "\n",
    "# Categorical columns (all except 'class')\n",
    "cat_cols = [col for col in df.columns if col != 'class']\n",
    "\n",
    "# One-hot encode features\n",
    "df_cat = pd.get_dummies(df[cat_cols], drop_first=False).astype(int)\n",
    "\n",
    "# Target column\n",
    "df_target = df[['class']]\n",
    "\n",
    "# Record group sizes for GAN output splitting (if needed later)\n",
    "cat_group_sizes = [\n",
    "    len(pd.get_dummies(df[col], drop_first=False).columns)\n",
    "    for col in cat_cols\n",
    "]\n",
    "\n",
    "# Combine features and target\n",
    "df_processed = pd.concat([df_cat, df_target], axis=1)\n",
    "\n",
    "# Save preprocessed data\n",
    "out_path = \"preprocessed_car.csv\"\n",
    "df_processed.to_csv(out_path, index=False)\n",
    "print(\"✅ Preprocessed data saved to:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "336c00be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "— Rep 1/3  Fold 1/2 —\n",
      " Ep 1/100  D_loss=1.2785  G_loss=0.6690\n",
      " Ep 20/100  D_loss=1.0441  G_loss=2.0299\n",
      " Ep 40/100  D_loss=1.2539  G_loss=2.2646\n",
      " Ep 60/100  D_loss=1.0598  G_loss=1.2805\n",
      " Ep 80/100  D_loss=1.1717  G_loss=2.0051\n",
      " Ep 100/100  D_loss=0.6089  G_loss=2.4384\n",
      "\n",
      "— Rep 1/3  Fold 2/2 —\n",
      " Ep 1/100  D_loss=1.3130  G_loss=0.6244\n",
      " Ep 20/100  D_loss=0.7113  G_loss=2.5002\n",
      " Ep 40/100  D_loss=0.1990  G_loss=2.7012\n",
      " Ep 60/100  D_loss=0.4334  G_loss=1.8863\n",
      " Ep 80/100  D_loss=0.5669  G_loss=2.4862\n",
      " Ep 100/100  D_loss=0.7529  G_loss=1.3862\n",
      "\n",
      "— Rep 2/3  Fold 1/2 —\n",
      " Ep 1/100  D_loss=1.3047  G_loss=0.6571\n",
      " Ep 20/100  D_loss=0.4094  G_loss=2.6086\n",
      " Ep 40/100  D_loss=1.6211  G_loss=1.5562\n",
      " Ep 60/100  D_loss=0.7200  G_loss=2.4813\n",
      " Ep 80/100  D_loss=0.5187  G_loss=2.7554\n",
      " Ep 100/100  D_loss=0.4123  G_loss=2.6142\n",
      "\n",
      "— Rep 2/3  Fold 2/2 —\n",
      " Ep 1/100  D_loss=1.3148  G_loss=0.6733\n",
      " Ep 20/100  D_loss=0.5695  G_loss=1.9879\n",
      " Ep 40/100  D_loss=1.4279  G_loss=2.3275\n",
      " Ep 60/100  D_loss=1.6628  G_loss=0.7869\n",
      " Ep 80/100  D_loss=1.4628  G_loss=1.0466\n",
      " Ep 100/100  D_loss=0.6157  G_loss=1.7832\n",
      "\n",
      "— Rep 3/3  Fold 1/2 —\n",
      " Ep 1/100  D_loss=1.2919  G_loss=0.6724\n",
      " Ep 20/100  D_loss=1.2212  G_loss=1.6460\n",
      " Ep 40/100  D_loss=0.6804  G_loss=1.6724\n",
      " Ep 60/100  D_loss=0.5457  G_loss=2.8996\n",
      " Ep 80/100  D_loss=0.6003  G_loss=1.8363\n",
      " Ep 100/100  D_loss=0.1858  G_loss=2.4053\n",
      "\n",
      "— Rep 3/3  Fold 2/2 —\n",
      " Ep 1/100  D_loss=1.3076  G_loss=0.6352\n",
      " Ep 20/100  D_loss=0.3340  G_loss=2.2873\n",
      " Ep 40/100  D_loss=0.9903  G_loss=1.5741\n",
      " Ep 60/100  D_loss=0.3905  G_loss=3.9940\n",
      " Ep 80/100  D_loss=1.1985  G_loss=1.6663\n",
      " Ep 100/100  D_loss=0.1710  G_loss=2.6047\n",
      "\n",
      "=== CV Results (mean ± std) ===\n",
      " • LR   TSTR = 65.76% ± 6.53%\n",
      " • MLP  TSTR = 69.98% ± 1.31%\n",
      " • RF   TSTR = 50.48% ± 18.86%\n",
      " • XGB  TSTR = 53.92% ± 8.69%\n",
      " • JSD = 0.0000 ± 0.0000\n",
      " • WD  = 0.0000 ± 0.0000\n"
     ]
    }
   ],
   "source": [
    "# --- GAN Training & Evaluation ---\n",
    "\n",
    "# Installs & Imports\n",
    "# !pip install torch torchvision scipy scikit-learn xgboost --quiet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Hyperparameters\n",
    "PREPROCESSED_PATH = \"preprocessed_car.csv\"\n",
    "LATENT_DIM        = 100\n",
    "BATCH_SIZE        = 64\n",
    "EPOCHS            = 100\n",
    "REPEATS           = 3\n",
    "FOLDS             = 2\n",
    "SYN_RATIO         = 0.5\n",
    "NUMERIC_COLS      = []  # No numeric columns\n",
    "TARGET_COL        = \"class\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z_dim,256), nn.ReLU(),\n",
    "            nn.Linear(256,512),   nn.ReLU(),\n",
    "            nn.Linear(512,256),   nn.ReLU(),\n",
    "            nn.Linear(256,out_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim,512), nn.ReLU(),\n",
    "            nn.Linear(512,256),   nn.ReLU(),\n",
    "            nn.Linear(256,128),   nn.ReLU(),\n",
    "            nn.Linear(128,1),     nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_cramer_gan(G, D, loader, epochs):\n",
    "    G, D = G.to(device), D.to(device)\n",
    "    optg = optim.Adam(G.parameters(), lr=2e-4)\n",
    "    optd = optim.Adam(D.parameters(), lr=2e-4)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    for ep in range(1, epochs+1):\n",
    "        for real_batch, _ in loader:\n",
    "            real_batch = real_batch.to(device)\n",
    "            bsz = real_batch.size(0)\n",
    "            # — D step\n",
    "            optd.zero_grad()\n",
    "            z      = torch.randn(bsz, LATENT_DIM, device=device)\n",
    "            fake   = G(z).detach()\n",
    "            d_real = D(real_batch)\n",
    "            d_fake = D(fake)\n",
    "            lossd  = loss_fn(d_real, torch.ones_like(d_real)) + \\\n",
    "                     loss_fn(d_fake, torch.zeros_like(d_fake))\n",
    "            lossd.backward();  optd.step()\n",
    "            # — G step\n",
    "            optg.zero_grad()\n",
    "            z     = torch.randn(bsz, LATENT_DIM, device=device)\n",
    "            fake2 = G(z)\n",
    "            dg    = D(fake2)\n",
    "            lossg = loss_fn(dg, torch.ones_like(dg))\n",
    "            lossg.backward(); optg.step()\n",
    "        if ep%20==0 or ep==1 or ep==epochs:\n",
    "            print(f\" Ep {ep}/{epochs}  D_loss={lossd.item():.4f}  G_loss={lossg.item():.4f}\")\n",
    "    return G, D\n",
    "\n",
    "def generate_synthetic(G, n_samples):\n",
    "    G = G.to(device).eval()\n",
    "    with torch.no_grad():\n",
    "        z    = torch.randn(n_samples, LATENT_DIM, device=device)\n",
    "        data = G(z).cpu().numpy()\n",
    "    return data\n",
    "\n",
    "def compute_tstr_all(X_real, y_real, X_syn, y_syn):\n",
    "    # Train each classifier on synthetic → score on real\n",
    "    results = {}\n",
    "    for name, clf in [\n",
    "        (\"LR\",  LogisticRegression(max_iter=5000)),\n",
    "        (\"MLP\", MLPClassifier(hidden_layer_sizes=(128,64), max_iter=1000)),\n",
    "        (\"RF\",  RandomForestClassifier(n_estimators=200)),\n",
    "        (\"XGB\", XGBClassifier(eval_metric=\"logloss\"))\n",
    "    ]:\n",
    "        clf.fit(X_syn, y_syn)\n",
    "        results[name] = clf.score(X_real, y_real)*100.0\n",
    "    return results\n",
    "\n",
    "def compute_jsd_wd(X_real, X_syn, num_idx):\n",
    "    if not num_idx:\n",
    "        return 0.0, 0.0  # explicitly return zero if no numeric features\n",
    "    jsd_list, wd_list = [], []\n",
    "    for i in num_idx:\n",
    "        p_real, _ = np.histogram(X_real[:, i], bins=50, density=True)\n",
    "        p_syn, _ = np.histogram(X_syn[:, i], bins=50, density=True)\n",
    "        jsd_list.append(jensenshannon(p_real, p_syn))\n",
    "        wd_list.append(wasserstein_distance(X_real[:, i], X_syn[:, i]))\n",
    "    return np.mean(jsd_list), np.mean(wd_list)\n",
    "\n",
    "# Load Preprocessed Data\n",
    "df = pd.read_csv(PREPROCESSED_PATH)\n",
    "X_full = df.drop(columns=[TARGET_COL]).values.astype(np.float32)\n",
    "y_full = df[TARGET_COL].values.astype(int)\n",
    "num_idx = [df.columns.get_loc(c) for c in NUMERIC_COLS]\n",
    "\n",
    "# 3×(2-Fold CV)\n",
    "tstr_scores = {m:[] for m in [\"LR\",\"MLP\",\"RF\",\"XGB\"]}\n",
    "jsd_scores, wd_scores = [], []\n",
    "\n",
    "kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "for rep in range(1, REPEATS+1):\n",
    "    for fold,(train_idx, test_idx) in enumerate(kf.split(X_full),1):\n",
    "        print(f\"\\n— Rep {rep}/{REPEATS}  Fold {fold}/{FOLDS} —\")\n",
    "        X_tr, X_te = X_full[train_idx], X_full[test_idx]\n",
    "        y_tr, y_te = y_full[train_idx], y_full[test_idx]\n",
    "        loader = DataLoader(\n",
    "            TensorDataset(torch.from_numpy(X_tr), torch.from_numpy(y_tr)),\n",
    "            batch_size=BATCH_SIZE, shuffle=True\n",
    "        )\n",
    "        # train\n",
    "        G = Generator(LATENT_DIM, X_tr.shape[1])\n",
    "        D = Discriminator(X_tr.shape[1])\n",
    "        G, D = train_cramer_gan(G, D, loader, epochs=EPOCHS)\n",
    "        # synth\n",
    "        n_syn = int(SYN_RATIO * len(X_tr))\n",
    "        X_syn = generate_synthetic(G, n_syn)\n",
    "        y_syn = np.random.choice(y_tr, size=n_syn, replace=True)\n",
    "        # metrics\n",
    "        tstrs = compute_tstr_all(X_te, y_te, X_syn, y_syn)\n",
    "        for m,sc in tstrs.items(): tstr_scores[m].append(sc)\n",
    "        js, wd = compute_jsd_wd(X_te, X_syn, num_idx)\n",
    "        jsd_scores.append(js);  wd_scores.append(wd)\n",
    "\n",
    "# Report Results\n",
    "print(\"\\n=== CV Results (mean ± std) ===\")\n",
    "for m in [\"LR\",\"MLP\",\"RF\",\"XGB\"]:\n",
    "    arr = np.array(tstr_scores[m])\n",
    "    print(f\" • {m:4s} TSTR = {arr.mean():.2f}% ± {arr.std():.2f}%\")\n",
    "print(f\" • JSD = {np.mean(jsd_scores):.4f} ± {np.std(jsd_scores):.4f}\")\n",
    "print(f\" • WD  = {np.mean(wd_scores):.4f} ± {np.std(wd_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef3f6bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ep 1/100  D_loss=1.2215  G_loss=0.6590\n",
      " Ep 20/100  D_loss=0.3558  G_loss=2.3499\n",
      " Ep 40/100  D_loss=0.7506  G_loss=1.7841\n",
      " Ep 60/100  D_loss=1.0647  G_loss=1.9161\n",
      " Ep 80/100  D_loss=1.1154  G_loss=1.8145\n",
      " Ep 100/100  D_loss=0.3999  G_loss=2.9209\n",
      "\n",
      "✅ Saved final synthetic dataset (864 rows) to:\n",
      "  synthetic_car_final.csv\n"
     ]
    }
   ],
   "source": [
    "# Final Model Training & Save Synthetic Dataset\n",
    "\n",
    "# Train on full dataset\n",
    "full_loader = DataLoader(\n",
    "    TensorDataset(torch.from_numpy(X_full), torch.from_numpy(y_full)),\n",
    "    batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "Gf = Generator(LATENT_DIM, X_full.shape[1])\n",
    "Df = Discriminator(X_full.shape[1])\n",
    "Gf, Df = train_cramer_gan(Gf, Df, full_loader, epochs=EPOCHS)\n",
    "\n",
    "# Generate synthetic data\n",
    "n_final = int(SYN_RATIO * len(X_full))\n",
    "Xf_syn = generate_synthetic(Gf, n_final)\n",
    "yf_syn = np.random.choice(y_full, size=n_final, replace=True)\n",
    "\n",
    "# Save to CSV\n",
    "cols = df.columns[:-1]\n",
    "syn_df = pd.DataFrame(Xf_syn, columns=cols)\n",
    "syn_df[TARGET_COL] = yf_syn\n",
    "out_path = \"synthetic_car_final.csv\"\n",
    "syn_df.to_csv(out_path, index=False)\n",
    "print(f\"\\n✅ Saved final synthetic dataset ({n_final} rows) to:\\n  {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
