{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOPAAxc+fhrApKjJQBFmXwv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3hR9kzgzyWyh","executionInfo":{"status":"ok","timestamp":1743925428232,"user_tz":-600,"elapsed":10855,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}},"outputId":"b0ce3ef0-9593-4d0f-c6bb-b90d1068870b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.14.1)\n","Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n","/content/drive/MyDrive/Colab Notebooks/Katabatic/CrGAN/poker\n","Using device: cpu\n"]}],"source":["%pip install xgboost\n","\n","import numpy as np\n","import pandas as pd\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torch.autograd import Variable\n","from torch.autograd import grad as torch_grad\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, classification_report\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","from scipy.stats import entropy\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","import time\n","from tqdm import tqdm\n","\n","from google.colab import drive\n","\n","drive.mount('/content/drive/')\n","%cd /content/drive/MyDrive/Colab Notebooks/Katabatic/CrGAN/poker/\n","\n","# Suppress warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Setting random seeds for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","# Check if CUDA is available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"]},{"cell_type":"code","source":["# Load and preprocess data\n","def load_poker_data(train_path, test_path):\n","    # Column names based on the dataset description\n","    column_names = ['S1', 'C1', 'S2', 'C2', 'S3', 'C3', 'S4', 'C4', 'S5', 'C5', 'CLASS']\n","\n","    # Load data\n","    df_train = pd.read_csv(train_path, header=None, names=column_names)\n","    df_test = pd.read_csv(test_path, header=None, names=column_names)\n","\n","    print(f\"Training data shape: {df_train.shape}\")\n","    print(f\"Testing data shape: {df_test.shape}\")\n","\n","    # Split features and target\n","    X_train = df_train.drop('CLASS', axis=1)\n","    y_train = df_train['CLASS']\n","    X_test = df_test.drop('CLASS', axis=1)\n","    y_test = df_test['CLASS']\n","\n","    # Create preprocessing pipeline - for poker hands, we'll use one-hot encoding for suits\n","    # and normalize the card ranks\n","\n","    # Define suit columns and rank columns\n","    suit_cols = ['S1', 'S2', 'S3', 'S4', 'S5']\n","    rank_cols = ['C1', 'C2', 'C3', 'C4', 'C5']\n","\n","    # Create transformers\n","    suit_transformer = Pipeline(steps=[\n","        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n","    ])\n","\n","    rank_transformer = Pipeline(steps=[\n","        ('scaler', StandardScaler())\n","    ])\n","\n","    preprocessor = ColumnTransformer(\n","        transformers=[\n","            ('suit', suit_transformer, suit_cols),\n","            ('rank', rank_transformer, rank_cols)\n","        ])\n","\n","    # Fit and transform the data\n","    X_train_transformed = preprocessor.fit_transform(X_train)\n","    X_test_transformed = preprocessor.transform(X_test)\n","\n","    # Get feature names\n","    suit_feature_names = preprocessor.named_transformers_['suit'].named_steps['onehot'].get_feature_names_out(suit_cols)\n","    all_feature_names = list(suit_feature_names) + list(rank_cols)\n","\n","    print(f\"Processed training data shape: {X_train_transformed.shape}\")\n","    print(f\"Processed testing data shape: {X_test_transformed.shape}\")\n","\n","    return X_train_transformed, y_train, X_test_transformed, y_test, preprocessor, all_feature_names\n","\n","# Custom dataset class\n","class PokerDataset(Dataset):\n","    def __init__(self, features, labels=None):\n","        self.features = torch.tensor(features, dtype=torch.float32)\n","        if labels is not None:\n","            self.labels = torch.tensor(labels, dtype=torch.float32).view(-1, 1)\n","        else:\n","            self.labels = None\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, idx):\n","        if self.labels is not None:\n","            return self.features[idx], self.labels[idx]\n","        else:\n","            return self.features[idx]\n","\n","# Generator Network\n","class Generator(nn.Module):\n","    def __init__(self, latent_dim, output_dim):\n","        super(Generator, self).__init__()\n","        self.latent_dim = latent_dim\n","        self.output_dim = output_dim\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(latent_dim, 256),\n","            nn.BatchNorm1d(256),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Linear(256, 512),\n","            nn.BatchNorm1d(512),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Linear(512, 1024),\n","            nn.BatchNorm1d(1024),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Linear(1024, output_dim),\n","            nn.Tanh()  # Output layer - maps to (-1, 1) range\n","        )\n","\n","    def forward(self, z):\n","        return self.model(z)\n","\n","# Critic Network (Discriminator)\n","class Critic(nn.Module):\n","    def __init__(self, input_dim):\n","        super(Critic, self).__init__()\n","        self.input_dim = input_dim\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(input_dim, 512),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(512, 256),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(256, 128),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(128, 1)\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Cramer GAN Implementation\n","class CramerGAN:\n","    def __init__(self, data_dim, latent_dim=100, critic_iterations=5, lambda_gp=10):\n","        self.data_dim = data_dim\n","        self.latent_dim = latent_dim\n","        self.critic_iterations = critic_iterations\n","        self.lambda_gp = lambda_gp\n","\n","        # Initialize networks\n","        self.generator = Generator(latent_dim, data_dim).to(device)\n","        self.critic = Critic(data_dim).to(device)\n","\n","        # Setup optimizers\n","        self.g_optimizer = optim.Adam(self.generator.parameters(), lr=0.0002, betas=(0.5, 0.9))\n","        self.c_optimizer = optim.Adam(self.critic.parameters(), lr=0.0002, betas=(0.5, 0.9))\n","\n","        # Initialize loss tracking\n","        self.g_losses = []\n","        self.c_losses = []\n","\n","    def _critic_train_iteration(self, real_data, batch_size):\n","        # Generate random noise\n","        noise = torch.randn(batch_size, self.latent_dim).to(device)\n","\n","        # Generate fake data\n","        fake_data = self.generator(noise)\n","\n","        # Get critic outputs\n","        critic_real = self.critic(real_data)\n","        critic_fake = self.critic(fake_data)\n","\n","        # Calculate Cramer distance\n","        critic_real2 = self.critic(torch.roll(real_data, shifts=1, dims=0))\n","        critic_fake2 = self.critic(torch.roll(fake_data, shifts=1, dims=0))\n","\n","        # Cramer GAN loss function\n","        c_loss = torch.mean(critic_real - critic_fake) - 0.5 * torch.mean(torch.pow(critic_real - critic_real2, 2)) + 0.5 * torch.mean(torch.pow(critic_fake - critic_fake2, 2))\n","\n","        # Calculate gradient penalty\n","        alpha = torch.rand(batch_size, 1).to(device)\n","        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n","        interpolates.requires_grad_(True)\n","\n","        critic_interpolates = self.critic(interpolates)\n","        gradients = torch_grad(outputs=critic_interpolates, inputs=interpolates,\n","                              grad_outputs=torch.ones_like(critic_interpolates).to(device),\n","                              create_graph=True, retain_graph=True)[0]\n","\n","        gradients = gradients.view(batch_size, -1)\n","        gradient_penalty = self.lambda_gp * ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n","\n","        # Update critic\n","        self.c_optimizer.zero_grad()\n","        c_loss_total = c_loss + gradient_penalty\n","        c_loss_total.backward()\n","        self.c_optimizer.step()\n","\n","        return c_loss_total.item()\n","\n","    def _generator_train_iteration(self, batch_size):\n","        # Generate random noise\n","        noise = torch.randn(batch_size, self.latent_dim).to(device)\n","\n","        # Generate fake data\n","        fake_data = self.generator(noise)\n","\n","        # Calculate critic outputs\n","        critic_fake = self.critic(fake_data)\n","        critic_fake2 = self.critic(torch.roll(fake_data, shifts=1, dims=0))\n","\n","        # Generator loss is negative of critic loss\n","        g_loss = -torch.mean(critic_fake) + 0.5 * torch.mean(torch.pow(critic_fake - critic_fake2, 2))\n","\n","        # Update generator\n","        self.g_optimizer.zero_grad()\n","        g_loss.backward()\n","        self.g_optimizer.step()\n","\n","        return g_loss.item()\n","\n","    def train(self, data_loader, epochs, save_interval=10, verbose=True):\n","        for epoch in range(epochs):\n","            epoch_start_time = time.time()\n","            c_loss_total = 0\n","            g_loss_total = 0\n","            num_batches = 0\n","\n","            for i, (real_data, _) in enumerate(data_loader):\n","                batch_size = real_data.size(0)\n","                real_data = real_data.to(device)\n","\n","                # Train critic\n","                for _ in range(self.critic_iterations):\n","                    c_loss = self._critic_train_iteration(real_data, batch_size)\n","                c_loss_total += c_loss\n","\n","                # Train generator\n","                g_loss = self._generator_train_iteration(batch_size)\n","                g_loss_total += g_loss\n","\n","                num_batches += 1\n","\n","            # Calculate average loss for the epoch\n","            c_loss_avg = c_loss_total / num_batches\n","            g_loss_avg = g_loss_total / num_batches\n","\n","            self.c_losses.append(c_loss_avg)\n","            self.g_losses.append(g_loss_avg)\n","\n","            epoch_time = time.time() - epoch_start_time\n","\n","            if verbose and (epoch % save_interval == 0 or epoch == epochs - 1):\n","                print(f\"Epoch [{epoch+1}/{epochs}] | Critic Loss: {c_loss_avg:.4f} | Generator Loss: {g_loss_avg:.4f} | Time: {epoch_time:.2f}s\")\n","\n","    def generate_samples(self, num_samples):\n","        self.generator.eval()\n","        noise = torch.randn(num_samples, self.latent_dim).to(device)\n","        with torch.no_grad():\n","            generated_data = self.generator(noise).cpu().numpy()\n","        self.generator.train()\n","        return generated_data\n","\n","    def save_model(self, path):\n","        torch.save({\n","            'generator_state_dict': self.generator.state_dict(),\n","            'critic_state_dict': self.critic.state_dict(),\n","            'g_optimizer_state_dict': self.g_optimizer.state_dict(),\n","            'c_optimizer_state_dict': self.c_optimizer.state_dict(),\n","        }, path)\n","\n","    def load_model(self, path):\n","        checkpoint = torch.load(path)\n","        self.generator.load_state_dict(checkpoint['generator_state_dict'])\n","        self.critic.load_state_dict(checkpoint['critic_state_dict'])\n","        self.g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n","        self.c_optimizer.load_state_dict(checkpoint['c_optimizer_state_dict'])\n","\n","# Post-process generated data to make it valid poker hands\n","def post_process_poker_data(synthetic_data, preprocessor):\n","    \"\"\"\n","    Post-process generated data to ensure it can be translated back into valid poker hands\n","    \"\"\"\n","    # Get the number of suit features (one-hot encoded)\n","    n_suit_features = len(preprocessor.named_transformers_['suit'].named_steps['onehot'].get_feature_names_out(['S1', 'S2', 'S3', 'S4', 'S5']))\n","\n","    # Split the synthetic data into suits and ranks\n","    synthetic_suits = synthetic_data[:, :n_suit_features]\n","    synthetic_ranks = synthetic_data[:, n_suit_features:]\n","\n","    # For each card's suit (one-hot encoded), take the max value to get the most likely suit\n","    num_cards = 5\n","    n_suits = 4  # Hearts, Spades, Diamonds, Clubs\n","\n","    processed_suits = np.zeros((len(synthetic_data), num_cards))\n","\n","    for i in range(num_cards):\n","        one_hot_indices = np.argmax(synthetic_suits[:, i*n_suits:(i+1)*n_suits], axis=1)\n","        processed_suits[:, i] = one_hot_indices + 1  # Add 1 to match original encoding (1-4)\n","\n","    # Inverse transform the rank data\n","    processed_ranks = preprocessor.named_transformers_['rank'].named_steps['scaler'].inverse_transform(synthetic_ranks)\n","\n","    # Clip and round ranks to valid values (1-13)\n","    processed_ranks = np.clip(np.round(processed_ranks), 1, 13)\n","\n","    # Combine suits and ranks\n","    processed_data = np.zeros((len(synthetic_data), num_cards * 2))\n","\n","    for i in range(num_cards):\n","        processed_data[:, 2*i] = processed_suits[:, i]       # Suit\n","        processed_data[:, 2*i+1] = processed_ranks[:, i]     # Rank\n","\n","    return processed_data"],"metadata":{"id":"gOXsxIkuy-dt","executionInfo":{"status":"ok","timestamp":1743925428317,"user_tz":-600,"elapsed":80,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Evaluation Metrics\n","\n","# 1. Machine Learning Utility (TSTR)\n","def evaluate_tstr(real_data, synthetic_data, real_labels, random_state=42):\n","    \"\"\"\n","    Train classifiers on synthetic data and test on real data (TSTR)\n","    Returns accuracy for each classifier\n","    \"\"\"\n","    # Train-test split for real data\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        real_data, real_labels, test_size=0.2, random_state=random_state\n","    )\n","\n","    # Synthetic data (all used for training)\n","    X_synth = synthetic_data\n","\n","    # Ensure proper dimensions for labels\n","    if isinstance(y_train, pd.Series):\n","        y_train = y_train.values\n","\n","    # Create synthetic labels based on real distribution\n","    # Get class distribution from y_test (to ensure synthetic labels match test classes)\n","    class_distribution = np.bincount(y_test.astype(int), minlength=10).astype(float)\n","    class_distribution += 1e-6  # Add small value to ensure all classes have a non-zero probability\n","    class_distribution /= class_distribution.sum()  # Normalize\n","\n","    # Sample synthetic labels using this adjusted distribution\n","    np.random.seed(random_state)\n","    y_synth = np.random.choice(range(10), size=len(X_synth), p=class_distribution)\n","\n","    # Define classifiers\n","    classifiers = {\n","        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=random_state),\n","        'MLP': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=random_state),\n","        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=random_state),\n","        'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=random_state)\n","    }\n","\n","    results = {}\n","\n","    for name, clf in classifiers.items():\n","        # Train on synthetic data\n","        clf.fit(X_synth, y_synth)\n","\n","        # Test on real data\n","        y_pred = clf.predict(X_test)\n","\n","        # Calculate metrics\n","        accuracy = accuracy_score(y_test, y_pred)\n","\n","        # We use weighted f1 since poker hands dataset is heavily imbalanced\n","        f1 = f1_score(y_test, y_pred, average='weighted')\n","\n","        results[name] = {\n","            'accuracy': accuracy,\n","            'f1_score': f1\n","        }\n","\n","    return results\n","\n","# 2. Statistical Similarity\n","def jensen_shannon_divergence(p, q):\n","    \"\"\"\n","    Calculate Jensen-Shannon Divergence between distributions p and q\n","    \"\"\"\n","    # Ensure p and q are normalized\n","    p = p / np.sum(p)\n","    q = q / np.sum(q)\n","\n","    m = 0.5 * (p + q)\n","\n","    # Calculate JSD\n","    jsd = 0.5 * (entropy(p, m) + entropy(q, m))\n","\n","    return jsd\n","\n","def wasserstein_distance(p, q):\n","    \"\"\"\n","    Calculate 1D Wasserstein distance (Earth Mover's Distance)\n","    \"\"\"\n","    from scipy.stats import wasserstein_distance\n","\n","    return wasserstein_distance(p, q)\n","\n","def evaluate_statistical_similarity(real_data, synthetic_data, feature_names):\n","    \"\"\"\n","    Calculate statistical similarity metrics between real and synthetic data\n","    \"\"\"\n","    results = {'JSD': {}, 'WD': {}}\n","\n","    # Calculate metrics for each feature\n","    for i in range(real_data.shape[1]):\n","        feature_name = feature_names[i] if i < len(feature_names) else f\"feature_{i}\"\n","\n","        # Get feature values\n","        real_values = real_data[:, i]\n","        synth_values = synthetic_data[:, i]\n","\n","        # Calculate histogram (discrete distribution)\n","        hist_bins = min(50, len(np.unique(real_values)))\n","\n","        hist_real, bin_edges = np.histogram(real_values, bins=hist_bins, density=True)\n","        hist_synth, _ = np.histogram(synth_values, bins=bin_edges, density=True)\n","\n","        # Add a small epsilon to avoid division by zero\n","        epsilon = 1e-10\n","        hist_real = hist_real + epsilon\n","        hist_synth = hist_synth + epsilon\n","\n","        # Calculate JSD\n","        jsd = jensen_shannon_divergence(hist_real, hist_synth)\n","        results['JSD'][feature_name] = jsd\n","\n","        # Calculate Wasserstein Distance\n","        wd = wasserstein_distance(real_values, synth_values)\n","        results['WD'][feature_name] = wd\n","\n","    # Calculate average metrics\n","    results['JSD_avg'] = np.mean(list(results['JSD'].values()))\n","    results['WD_avg'] = np.mean(list(results['WD'].values()))\n","\n","    return results\n","\n","def plot_loss_curves(model):\n","    \"\"\"\n","    Plot the loss curves for the generator and critic\n","    \"\"\"\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(model.g_losses, label='Generator Loss')\n","    plt.plot(model.c_losses, label='Critic Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.title('CramerGAN Training Loss for Poker Hand Dataset')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig('poker_crgan_loss_curves.png')\n","    plt.close()\n","\n","def plot_feature_distributions(real_data, synthetic_data, feature_names, n_features=10):\n","    \"\"\"\n","    Plot distributions of real vs synthetic data for selected features\n","    \"\"\"\n","    if n_features > len(feature_names):\n","        n_features = len(feature_names)\n","\n","    # Select a subset of features to visualize\n","    selected_indices = np.random.choice(range(len(feature_names)), size=n_features, replace=False)\n","\n","    plt.figure(figsize=(15, 20))\n","    for i, idx in enumerate(selected_indices):\n","        feature_name = feature_names[idx]\n","\n","        plt.subplot(n_features, 1, i+1)\n","\n","        # Get feature values\n","        real_values = real_data[:, idx]\n","        synth_values = synthetic_data[:, idx]\n","\n","        # Plot histograms\n","        sns.histplot(real_values, kde=True, stat=\"density\", label=\"Real\", alpha=0.6, color=\"blue\")\n","        sns.histplot(synth_values, kde=True, stat=\"density\", label=\"Synthetic\", alpha=0.6, color=\"red\")\n","\n","        plt.title(f\"Distribution for {feature_name}\")\n","        plt.legend()\n","\n","    plt.tight_layout()\n","    plt.savefig('poker_feature_distributions.png')\n","    plt.close()\n","\n","def plot_class_distribution(real_labels, synthetic_labels):\n","    \"\"\"\n","    Plot the class distribution of real vs synthetic data\n","    \"\"\"\n","    plt.figure(figsize=(12, 6))\n","\n","    # Get class counts\n","    real_class_counts = np.bincount(real_labels.astype(int), minlength=10)\n","    synth_class_counts = np.bincount(synthetic_labels.astype(int), minlength=10)\n","\n","    # Normalize\n","    real_class_dist = real_class_counts / np.sum(real_class_counts)\n","    synth_class_dist = synth_class_counts / np.sum(synth_class_counts)\n","\n","    # Define class names\n","    class_names = [\n","        'Nothing in hand',\n","        'One pair',\n","        'Two pairs',\n","        'Three of a kind',\n","        'Straight',\n","        'Flush',\n","        'Full house',\n","        'Four of a kind',\n","        'Straight flush',\n","        'Royal flush'\n","    ]\n","\n","    # Plot\n","    bar_width = 0.35\n","    x = np.arange(10)\n","\n","    plt.bar(x - bar_width/2, real_class_dist, bar_width, label='Real Data', color='blue', alpha=0.7)\n","    plt.bar(x + bar_width/2, synth_class_dist, bar_width, label='Synthetic Data', color='red', alpha=0.7)\n","\n","    plt.xlabel('Poker Hand')\n","    plt.ylabel('Proportion')\n","    plt.title('Class Distribution: Real vs Synthetic Poker Hands')\n","    plt.xticks(x, class_names, rotation=45, ha='right')\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.savefig('poker_class_distribution.png')\n","    plt.close()\n","\n","# Main function\n","def main():\n","    # File paths\n","    train_path = 'data/poker-hand-training-true.csv'\n","    test_path = 'data/poker-hand-testing.csv'\n","\n","    # Load and preprocess data\n","    X_train, y_train, X_test, y_test, preprocessor, feature_names = load_poker_data(train_path, test_path)\n","\n","    # Create dataset and dataloader\n","    train_dataset = PokerDataset(X_train, y_train)\n","    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","\n","    # Initialize and train the model\n","    data_dim = X_train.shape[1]\n","    latent_dim = 100\n","\n","    print(f\"Data dimension: {data_dim}\")\n","    print(f\"Latent dimension: {latent_dim}\")\n","\n","    crgan = CramerGAN(data_dim, latent_dim)\n","\n","    # Train the model\n","    epochs = 100\n","    print(f\"Training CramerGAN for {epochs} epochs...\")\n","    crgan.train(train_loader, epochs, save_interval=10)\n","\n","    # Save the model\n","    crgan.save_model('poker_crgan_model.pt')\n","\n","    # Plot loss curves\n","    plot_loss_curves(crgan)\n","\n","    # Generate synthetic data\n","    num_samples = 1000\n","    print(f\"Generating {num_samples} synthetic samples...\")\n","    synthetic_data_raw = crgan.generate_samples(num_samples)\n","\n","    # Post-process the synthetic data to make it valid poker hands\n","    synthetic_data_processed = post_process_poker_data(synthetic_data_raw, preprocessor)\n","\n","    # Generate synthetic labels using a classifier trained on real data\n","    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n","    clf.fit(X_train, y_train)\n","    synthetic_labels = clf.predict(synthetic_data_raw)\n","\n","    # Plot class distribution\n","    plot_class_distribution(y_train, synthetic_labels)\n","\n","    # Statistical similarity evaluation\n","    print(\"Evaluating statistical similarity...\")\n","    stat_results = evaluate_statistical_similarity(X_train, synthetic_data_raw, feature_names)\n","\n","    print(\"\\nJensen-Shannon Divergence (average):\", stat_results['JSD_avg'])\n","    print(\"Wasserstein Distance (average):\", stat_results['WD_avg'])\n","\n","    # Machine Learning Utility (TSTR) evaluation\n","    print(\"\\nEvaluating Machine Learning Utility (TSTR)...\")\n","    tstr_results = evaluate_tstr(X_train, synthetic_data_raw, y_train)\n","\n","    print(\"\\nTSTR Results:\")\n","    for clf_name, metrics in tstr_results.items():\n","        print(f\"{clf_name}: Accuracy = {metrics['accuracy']:.4f}, F1 Score = {metrics['f1_score']:.4f}\")\n","\n","    # Plot feature distributions\n","    plot_feature_distributions(X_train, synthetic_data_raw, feature_names)\n","\n","    print(\"\\nEvaluation complete! Check the output directory for plots and saved model.\")\n","\n"],"metadata":{"id":"NkDUQ1ZEzAG2","executionInfo":{"status":"ok","timestamp":1743925428358,"user_tz":-600,"elapsed":49,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"tDGjV1eezBhC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743930507532,"user_tz":-600,"elapsed":625826,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}},"outputId":"abb5770a-b621-43a6-8cbb-1f82b2007fcb"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Training data shape: (25010, 11)\n","Testing data shape: (1000000, 11)\n","Processed training data shape: (25010, 25)\n","Processed testing data shape: (1000000, 25)\n","Data dimension: 25\n","Latent dimension: 100\n","Training CramerGAN for 100 epochs...\n","Epoch [1/100] | Critic Loss: -6389.4067 | Generator Loss: 142.7483 | Time: 69.83s\n","Epoch [11/100] | Critic Loss: -12541250863772.7344 | Generator Loss: 453898789449.1429 | Time: 50.62s\n","Epoch [21/100] | Critic Loss: -2613237636217208.0000 | Generator Loss: 106842132433857.3125 | Time: 49.75s\n","Epoch [31/100] | Critic Loss: -63612343801053856.0000 | Generator Loss: 2636557737739243.0000 | Time: 48.80s\n","Epoch [41/100] | Critic Loss: -702356441053372928.0000 | Generator Loss: 9841405389412226.0000 | Time: 49.78s\n","Epoch [51/100] | Critic Loss: -4426850062399958016.0000 | Generator Loss: 59454153814431288.0000 | Time: 50.90s\n","Epoch [61/100] | Critic Loss: -20006992484721840128.0000 | Generator Loss: 277621657833869888.0000 | Time: 49.23s\n","Epoch [71/100] | Critic Loss: -70989279235030040576.0000 | Generator Loss: 1016449519020039680.0000 | Time: 49.92s\n","Epoch [81/100] | Critic Loss: -203388902456583356416.0000 | Generator Loss: 3142974281279907840.0000 | Time: 50.28s\n","Epoch [91/100] | Critic Loss: -531498348182964600832.0000 | Generator Loss: 6848185283544275968.0000 | Time: 54.10s\n","Epoch [100/100] | Critic Loss: -1200569140961243299840.0000 | Generator Loss: 16068648671777957888.0000 | Time: 50.68s\n","Generating 1000 synthetic samples...\n","Evaluating statistical similarity...\n","\n","Jensen-Shannon Divergence (average): nan\n","Wasserstein Distance (average): 1.2309392370210603\n","\n","Evaluating Machine Learning Utility (TSTR)...\n","\n","TSTR Results:\n","Logistic Regression: Accuracy = 0.5102, F1 Score = 0.3447\n","MLP: Accuracy = 0.3693, F1 Score = 0.3717\n","Random Forest: Accuracy = 0.5102, F1 Score = 0.3447\n","XGBoost: Accuracy = 0.5102, F1 Score = 0.3447\n","\n","Evaluation complete! Check the output directory for plots and saved model.\n"]}]}]}