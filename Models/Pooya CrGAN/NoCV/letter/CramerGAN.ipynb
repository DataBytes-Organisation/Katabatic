{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNx87u1OzVeHdU/EXVqByUl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torch.autograd import Variable\n","from torch.autograd import grad as torch_grad\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, classification_report\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","from scipy.stats import entropy\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","import time\n","from tqdm import tqdm\n","from google.colab import drive\n","\n","\n","drive.mount('/content/drive/')\n","%cd /content/drive/MyDrive/Colab Notebooks/Katabatic/CrGAN/letter/\n","\n","# Suppress warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Setting random seeds for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","# Check if CUDA is available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WBwnD9ZXPa7W","executionInfo":{"status":"ok","timestamp":1743656978460,"user_tz":-660,"elapsed":12764,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}},"outputId":"af1bf13c-c91a-49c3-af08-6a6d21d190c6"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n","/content/drive/MyDrive/Colab Notebooks/Katabatic/CrGAN/letter\n","Using device: cpu\n"]}]},{"cell_type":"code","source":["def load_letter_data(data_path):\n","    # Column names based on the dataset description\n","\n","    column_names = ['letter', 'xbox', 'ybox', 'width', 'high', 'onpix', 'xbar', 'ybar',\n","                   'x2bar', 'y2bar', 'xybar', 'x2ybar', 'xy2bar', 'xedge', 'xedgey', 'yedge', 'yedgex']\n","\n","    # Load data\n","    df = pd.read_csv(data_path, header=None, names=column_names)\n","\n","    print(f\"Dataset shape: {df.shape}\")\n","\n","    # Split into train and test (80% train, 20% test)\n","    df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n","\n","    print(f\"Training data shape: {df_train.shape}\")\n","    print(f\"Testing data shape: {df_test.shape}\")\n","\n","    # Split features and target\n","    X_train = df_train.drop('letter', axis=1)\n","    y_train = df_train['letter']\n","    X_test = df_test.drop('letter', axis=1)\n","    y_test = df_test['letter']\n","\n","    X_train = X_train.apply(pd.to_numeric, errors='coerce')\n","    X_test = X_test.apply(pd.to_numeric, errors='coerce')\n","\n","    # Create preprocessing pipeline\n","    # For letter recognition, we'll standardize all numerical features\n","    numeric_cols = X_train.columns.tolist()\n","\n","    # Create transformers\n","    numeric_transformer = Pipeline(steps=[\n","        ('scaler', StandardScaler())\n","    ])\n","\n","    preprocessor = ColumnTransformer(\n","        transformers=[\n","            ('num', numeric_transformer, numeric_cols)\n","        ])\n","\n","    # Fit and transform the data\n","    X_train_transformed = preprocessor.fit_transform(X_train)\n","    X_test_transformed = preprocessor.transform(X_test)\n","\n","    print(f\"Processed training data shape: {X_train_transformed.shape}\")\n","    print(f\"Processed testing data shape: {X_test_transformed.shape}\")\n","\n","    # Encode targets to numeric\n","    label_encoder = {chr(65 + i): i for i in range(26)}  # A=0, B=1, ..., Z=25\n","    y_train_encoded = y_train.map(label_encoder)\n","    y_test_encoded = y_test.map(label_encoder)\n","\n","    return X_train_transformed, y_train_encoded, X_test_transformed, y_test_encoded, preprocessor, numeric_cols, label_encoder\n","\n","# Custom dataset class\n","class LetterDataset(Dataset):\n","    def __init__(self, features, labels=None):\n","        self.features = torch.tensor(features, dtype=torch.float32)\n","        if labels is not None:\n","            self.labels = torch.tensor(labels.values, dtype=torch.long)\n","        else:\n","            self.labels = None\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, idx):\n","        if self.labels is not None:\n","            return self.features[idx], self.labels[idx]\n","        else:\n","            return self.features[idx]\n","\n","# Generator Network\n","class Generator(nn.Module):\n","    def __init__(self, latent_dim, output_dim):\n","        super(Generator, self).__init__()\n","        self.latent_dim = latent_dim\n","        self.output_dim = output_dim\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(latent_dim, 128),\n","            nn.BatchNorm1d(128),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Linear(128, 256),\n","            nn.BatchNorm1d(256),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Linear(256, 512),\n","            nn.BatchNorm1d(512),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Linear(512, output_dim),\n","            nn.Tanh()  # Output layer - maps to (-1, 1) range\n","        )\n","\n","    def forward(self, z):\n","        return self.model(z)\n","\n","# Critic Network (Discriminator)\n","class Critic(nn.Module):\n","    def __init__(self, input_dim):\n","        super(Critic, self).__init__()\n","        self.input_dim = input_dim\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(input_dim, 256),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(256, 128),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(128, 64),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(64, 1)\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Cramer GAN Implementation\n","class CramerGAN:\n","    def __init__(self, data_dim, latent_dim=100, critic_iterations=5, lambda_gp=10):\n","        self.data_dim = data_dim\n","        self.latent_dim = latent_dim\n","        self.critic_iterations = critic_iterations\n","        self.lambda_gp = lambda_gp\n","\n","        # Initialize networks\n","        self.generator = Generator(latent_dim, data_dim).to(device)\n","        self.critic = Critic(data_dim).to(device)\n","\n","        # Setup optimizers\n","        self.g_optimizer = optim.Adam(self.generator.parameters(), lr=0.0002, betas=(0.5, 0.9))\n","        self.c_optimizer = optim.Adam(self.critic.parameters(), lr=0.0002, betas=(0.5, 0.9))\n","\n","        # Initialize loss tracking\n","        self.g_losses = []\n","        self.c_losses = []\n","\n","    def _critic_train_iteration(self, real_data, batch_size):\n","        # Generate random noise\n","        noise = torch.randn(batch_size, self.latent_dim).to(device)\n","\n","        # Generate fake data\n","        fake_data = self.generator(noise)\n","\n","        # Get critic outputs\n","        critic_real = self.critic(real_data)\n","        critic_fake = self.critic(fake_data)\n","\n","        # Calculate Cramer distance\n","        critic_real2 = self.critic(torch.roll(real_data, shifts=1, dims=0))\n","        critic_fake2 = self.critic(torch.roll(fake_data, shifts=1, dims=0))\n","\n","        # Cramer GAN loss function\n","        c_loss = torch.mean(critic_real - critic_fake) - 0.5 * torch.mean(torch.pow(critic_real - critic_real2, 2)) + 0.5 * torch.mean(torch.pow(critic_fake - critic_fake2, 2))\n","\n","        # Calculate gradient penalty\n","        alpha = torch.rand(batch_size, 1).to(device)\n","        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n","        interpolates.requires_grad_(True)\n","\n","        critic_interpolates = self.critic(interpolates)\n","        gradients = torch_grad(outputs=critic_interpolates, inputs=interpolates,\n","                              grad_outputs=torch.ones_like(critic_interpolates).to(device),\n","                              create_graph=True, retain_graph=True)[0]\n","\n","        gradients = gradients.view(batch_size, -1)\n","        gradient_penalty = self.lambda_gp * ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n","\n","        # Update critic\n","        self.c_optimizer.zero_grad()\n","        c_loss_total = c_loss + gradient_penalty\n","        c_loss_total.backward()\n","        self.c_optimizer.step()\n","\n","        return c_loss_total.item()\n","\n","    def _generator_train_iteration(self, batch_size):\n","        # Generate random noise\n","        noise = torch.randn(batch_size, self.latent_dim).to(device)\n","\n","        # Generate fake data\n","        fake_data = self.generator(noise)\n","\n","        # Calculate critic outputs\n","        critic_fake = self.critic(fake_data)\n","        critic_fake2 = self.critic(torch.roll(fake_data, shifts=1, dims=0))\n","\n","        # Generator loss is negative of critic loss\n","        g_loss = -torch.mean(critic_fake) + 0.5 * torch.mean(torch.pow(critic_fake - critic_fake2, 2))\n","\n","        # Update generator\n","        self.g_optimizer.zero_grad()\n","        g_loss.backward()\n","        self.g_optimizer.step()\n","\n","        return g_loss.item()\n","\n","    def train(self, data_loader, epochs, save_interval=10, verbose=True):\n","        for epoch in range(epochs):\n","            epoch_start_time = time.time()\n","            c_loss_total = 0\n","            g_loss_total = 0\n","            num_batches = 0\n","\n","            for i, (real_data, _) in enumerate(data_loader):\n","                batch_size = real_data.size(0)\n","                real_data = real_data.to(device)\n","\n","                # Train critic\n","                for _ in range(self.critic_iterations):\n","                    c_loss = self._critic_train_iteration(real_data, batch_size)\n","                c_loss_total += c_loss\n","\n","                # Train generator\n","                g_loss = self._generator_train_iteration(batch_size)\n","                g_loss_total += g_loss\n","\n","                num_batches += 1\n","\n","            # Calculate average loss for the epoch\n","            c_loss_avg = c_loss_total / num_batches\n","            g_loss_avg = g_loss_total / num_batches\n","\n","            self.c_losses.append(c_loss_avg)\n","            self.g_losses.append(g_loss_avg)\n","\n","            epoch_time = time.time() - epoch_start_time\n","\n","            if verbose and (epoch % save_interval == 0 or epoch == epochs - 1):\n","                print(f\"Epoch [{epoch+1}/{epochs}] | Critic Loss: {c_loss_avg:.4f} | Generator Loss: {g_loss_avg:.4f} | Time: {epoch_time:.2f}s\")\n","\n","    def generate_samples(self, num_samples):\n","        self.generator.eval()\n","        noise = torch.randn(num_samples, self.latent_dim).to(device)\n","        with torch.no_grad():\n","            generated_data = self.generator(noise).cpu().numpy()\n","        self.generator.train()\n","        return generated_data\n","\n","    def save_model(self, path):\n","        torch.save({\n","            'generator_state_dict': self.generator.state_dict(),\n","            'critic_state_dict': self.critic.state_dict(),\n","            'g_optimizer_state_dict': self.g_optimizer.state_dict(),\n","            'c_optimizer_state_dict': self.c_optimizer.state_dict(),\n","        }, path)\n","\n","    def load_model(self, path):\n","        checkpoint = torch.load(path)\n","        self.generator.load_state_dict(checkpoint['generator_state_dict'])\n","        self.critic.load_state_dict(checkpoint['critic_state_dict'])\n","        self.g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n","        self.c_optimizer.load_state_dict(checkpoint['c_optimizer_state_dict'])\n","\n","# Post-process generated data\n","def post_process_letter_data(synthetic_data, preprocessor, X_train_original):\n","    \"\"\"\n","    Post-process generated data to ensure it can be translated back into valid values\n","    Uses the scaler from within the column transformer to perform inverse transform\n","    \"\"\"\n","    # Extract the scaler from the column transformer\n","    scaler = preprocessor.named_transformers_['num'].named_steps['scaler']\n","\n","    # Use the scaler to inverse transform the data\n","    synthetic_data_processed = scaler.inverse_transform(synthetic_data)\n","\n","    # Round the values to integers as per dataset description\n","    synthetic_data_processed = np.array(synthetic_data_processed, dtype=float)\n","    synthetic_data_processed = np.round(synthetic_data_processed).astype(int)\n","\n","    # Determine min and max values from original data\n","    min_vals = np.min(X_train_original, axis=0)\n","    max_vals = np.max(X_train_original, axis=0)\n","\n","    # Convert min_vals and max_vals to float type\n","    min_vals = np.array(min_vals, dtype=float)\n","    max_vals = np.array(max_vals, dtype=float)\n","\n","    # Ensure values are within reasonable ranges\n","    synthetic_data_processed = np.clip(synthetic_data_processed, min_vals, max_vals)\n","\n","    return synthetic_data_processed"],"metadata":{"id":"6WuosKtTPnut","executionInfo":{"status":"ok","timestamp":1743656978662,"user_tz":-660,"elapsed":220,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"fntg2e0wPTw9","executionInfo":{"status":"ok","timestamp":1743656978760,"user_tz":-660,"elapsed":96,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}}},"outputs":[],"source":["# Evaluation Metrics\n","\n","# 1. Machine Learning Utility (TSTR)\n","def evaluate_tstr(real_data, synthetic_data, real_labels, random_state=42):\n","    \"\"\"\n","    Train classifiers on synthetic data and test on real data (TSTR)\n","    Returns accuracy for each classifier\n","    \"\"\"\n","    # Train-test split for real data\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        real_data, real_labels, test_size=0.2, random_state=random_state\n","    )\n","\n","    # Synthetic data (all used for training)\n","    X_synth = synthetic_data\n","\n","    # Ensure proper dimensions for labels\n","    if isinstance(y_train, pd.Series):\n","        y_train = y_train.values\n","\n","    # Create synthetic labels based on real distribution\n","    class_distribution = np.bincount(y_train.astype(int), minlength=26) / len(y_train)\n","    np.random.seed(random_state)\n","    y_synth = np.random.choice(range(26), size=len(X_synth), p=class_distribution)\n","\n","    # Define classifiers\n","    classifiers = {\n","        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=random_state),\n","        'MLP': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=random_state),\n","        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=random_state),\n","        'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=random_state)\n","    }\n","\n","    results = {}\n","\n","    for name, clf in classifiers.items():\n","        # Train on synthetic data\n","        clf.fit(X_synth, y_synth)\n","\n","        # Test on real data\n","        y_pred = clf.predict(X_test)\n","\n","        # Calculate metrics\n","        accuracy = accuracy_score(y_test, y_pred)\n","\n","        # We use weighted f1 since we have multiple classes\n","        f1 = f1_score(y_test, y_pred, average='weighted')\n","\n","        results[name] = {\n","            'accuracy': accuracy,\n","            'f1_score': f1\n","        }\n","\n","    return results\n","\n","# 2. Statistical Similarity\n","def jensen_shannon_divergence(p, q):\n","    \"\"\"\n","    Calculate Jensen-Shannon Divergence between distributions p and q\n","    \"\"\"\n","    # Ensure p and q are normalized\n","    p = p / np.sum(p)\n","    q = q / np.sum(q)\n","\n","    m = 0.5 * (p + q)\n","\n","    # Calculate JSD\n","    jsd = 0.5 * (entropy(p, m) + entropy(q, m))\n","\n","    return jsd\n","\n","def wasserstein_distance(p, q):\n","    \"\"\"\n","    Calculate 1D Wasserstein distance (Earth Mover's Distance)\n","    \"\"\"\n","    from scipy.stats import wasserstein_distance\n","\n","    return wasserstein_distance(p, q)\n","\n","def evaluate_statistical_similarity(real_data, synthetic_data, feature_names):\n","    \"\"\"\n","    Calculate statistical similarity metrics between real and synthetic data\n","    \"\"\"\n","    results = {'JSD': {}, 'WD': {}}\n","\n","    # Calculate metrics for each feature\n","    for i in range(real_data.shape[1]):\n","        feature_name = feature_names[i] if i < len(feature_names) else f\"feature_{i}\"\n","\n","        # Get feature values\n","        real_values = real_data[:, i]\n","        synth_values = synthetic_data[:, i]\n","\n","        # Calculate histogram (discrete distribution)\n","        hist_bins = min(50, len(np.unique(real_values)))\n","\n","        hist_real, bin_edges = np.histogram(real_values, bins=hist_bins, density=True)\n","        hist_synth, _ = np.histogram(synth_values, bins=bin_edges, density=True)\n","\n","        # Add a small epsilon to avoid division by zero\n","        epsilon = 1e-10\n","        hist_real = hist_real + epsilon\n","        hist_synth = hist_synth + epsilon\n","\n","        # Calculate JSD\n","        jsd = jensen_shannon_divergence(hist_real, hist_synth)\n","        results['JSD'][feature_name] = jsd\n","\n","        # Calculate Wasserstein Distance\n","        wd = wasserstein_distance(real_values, synth_values)\n","        results['WD'][feature_name] = wd\n","\n","    # Calculate average metrics\n","    results['JSD_avg'] = np.mean(list(results['JSD'].values()))\n","    results['WD_avg'] = np.mean(list(results['WD'].values()))\n","\n","    return results\n","\n","def plot_loss_curves(model):\n","    \"\"\"\n","    Plot the loss curves for the generator and critic\n","    \"\"\"\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(model.g_losses, label='Generator Loss')\n","    plt.plot(model.c_losses, label='Critic Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.title('CramerGAN Training Loss for Letter Recognition Dataset')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig('letter_crgan_loss_curves.png')\n","    plt.close()\n","\n","def plot_feature_distributions(real_data, synthetic_data, feature_names, n_features=10):\n","    \"\"\"\n","    Plot distributions of real vs synthetic data for selected features\n","    \"\"\"\n","    if n_features > len(feature_names):\n","        n_features = len(feature_names)\n","\n","    # Select a subset of features to visualize\n","    selected_indices = np.random.choice(range(len(feature_names)), size=n_features, replace=False)\n","\n","    plt.figure(figsize=(15, 20))\n","    for i, idx in enumerate(selected_indices):\n","        feature_name = feature_names[idx]\n","\n","        plt.subplot(n_features, 1, i+1)\n","\n","        # Get feature values\n","        real_values = real_data[:, idx]\n","        synth_values = synthetic_data[:, idx]\n","\n","        # Plot histograms\n","        sns.histplot(real_values, kde=True, stat=\"density\", label=\"Real\", alpha=0.6, color=\"blue\")\n","        sns.histplot(synth_values, kde=True, stat=\"density\", label=\"Synthetic\", alpha=0.6, color=\"red\")\n","\n","        plt.title(f\"Distribution for {feature_name}\")\n","        plt.legend()\n","\n","    plt.tight_layout()\n","    plt.savefig('letter_feature_distributions.png')\n","    plt.close()\n","\n","def plot_class_distribution(real_labels, synthetic_labels, label_encoder):\n","    \"\"\"\n","    Plot the class distribution of real vs synthetic data\n","    \"\"\"\n","    plt.figure(figsize=(15, 6))\n","\n","    # Get class counts\n","    real_class_counts = np.bincount(real_labels.astype(int), minlength=26)\n","    synth_class_counts = np.bincount(synthetic_labels.astype(int), minlength=26)\n","\n","    # Normalize\n","    real_class_dist = real_class_counts / np.sum(real_class_counts)\n","    synth_class_dist = synth_class_counts / np.sum(synth_class_counts)\n","\n","    # Define class names\n","    inverse_label_encoder = {v: k for k, v in label_encoder.items()}\n","    class_names = [inverse_label_encoder[i] for i in range(26)]\n","\n","    # Plot\n","    bar_width = 0.35\n","    x = np.arange(26)\n","\n","    plt.bar(x - bar_width/2, real_class_dist, bar_width, label='Real Data', color='blue', alpha=0.7)\n","    plt.bar(x + bar_width/2, synth_class_dist, bar_width, label='Synthetic Data', color='red', alpha=0.7)\n","\n","    plt.xlabel('Letter')\n","    plt.ylabel('Proportion')\n","    plt.title('Class Distribution: Real vs Synthetic Letters')\n","    plt.xticks(x, class_names)\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.savefig('letter_class_distribution.png')\n","    plt.close()\n","\n","# Main function\n","def main():\n","    # File path\n","    data_path = 'data/letter-recognition.csv'\n","\n","    # Load and preprocess data\n","    X_train_transformed, y_train, X_test_transformed, y_test, preprocessor, feature_names, label_encoder = load_letter_data(data_path)\n","\n","    # Keep the original data before transformation\n","    df = pd.read_csv(data_path, header=None, names=['letter'] + feature_names)\n","    df_train, _ = train_test_split(df, test_size=0.2, random_state=42)\n","    X_train_original = df_train.drop('letter', axis=1).values\n","\n","    # Create dataset and dataloader\n","    train_dataset = LetterDataset(X_train_transformed, y_train)\n","    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","\n","    # Initialize and train the model\n","    data_dim = X_train_transformed.shape[1]\n","    latent_dim = 100\n","\n","    print(f\"Data dimension: {data_dim}\")\n","    print(f\"Latent dimension: {latent_dim}\")\n","\n","    crgan = CramerGAN(data_dim, latent_dim)\n","\n","    # Train the model\n","    epochs = 300\n","    print(f\"Training CramerGAN for {epochs} epochs...\")\n","    crgan.train(train_loader, epochs, save_interval=10)\n","\n","    # Save the model\n","    crgan.save_model('letter_crgan_model.pt')\n","\n","    # Plot loss curves\n","    plot_loss_curves(crgan)\n","\n","    # Generate synthetic data\n","    num_samples = 1000\n","    print(f\"Generating {num_samples} synthetic samples...\")\n","    synthetic_data_raw = crgan.generate_samples(num_samples)\n","\n","    # Post-process the synthetic data - pass original data for reference\n","    synthetic_data_processed = post_process_letter_data(synthetic_data_raw, preprocessor, X_train_original)\n","\n","    # Create a DataFrame with the processed data for easier handling\n","    synthetic_df = pd.DataFrame(synthetic_data_processed, columns=feature_names)\n","\n","    # Generate synthetic labels using a classifier trained on real data\n","    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n","    clf.fit(X_train_transformed, y_train)\n","    synthetic_labels = clf.predict(synthetic_data_raw)\n","\n","    # Plot class distribution\n","    plot_class_distribution(y_train, synthetic_labels, label_encoder)\n","\n","    # Statistical similarity evaluation\n","    print(\"Evaluating statistical similarity...\")\n","    stat_results = evaluate_statistical_similarity(X_train_transformed, synthetic_data_raw, feature_names)\n","\n","    print(\"\\nJensen-Shannon Divergence (average):\", stat_results['JSD_avg'])\n","    print(\"Wasserstein Distance (average):\", stat_results['WD_avg'])\n","\n","    # Machine Learning Utility (TSTR) evaluation\n","    print(\"\\nEvaluating Machine Learning Utility (TSTR)...\")\n","    tstr_results = evaluate_tstr(X_train_transformed, synthetic_data_raw, y_train)\n","\n","    print(\"\\nTSTR Results:\")\n","    for clf_name, metrics in tstr_results.items():\n","        print(f\"{clf_name}: Accuracy = {metrics['accuracy']:.4f}, F1 Score = {metrics['f1_score']:.4f}\")\n","\n","    # Plot feature distributions\n","    plot_feature_distributions(X_train_transformed, synthetic_data_raw, feature_names)\n","\n","    print(\"\\nEvaluation complete! Check the output directory for plots and saved model.\")\n"]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ydMFZNOaPzdl","executionInfo":{"status":"ok","timestamp":1743661493231,"user_tz":-660,"elapsed":4514472,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}},"outputId":"637353bd-e256-449b-dea2-1c0cb444e340"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset shape: (20001, 17)\n","Training data shape: (16000, 17)\n","Testing data shape: (4001, 17)\n","Processed training data shape: (16000, 16)\n","Processed testing data shape: (4001, 16)\n","Data dimension: 16\n","Latent dimension: 100\n","Training CramerGAN for 300 epochs...\n","Epoch [1/300] | Critic Loss: -61.9478 | Generator Loss: -0.2903 | Time: 26.81s\n","Epoch [11/300] | Critic Loss: -2169750034.4320 | Generator Loss: 93432205.5040 | Time: 16.00s\n","Epoch [21/300] | Critic Loss: -868787038715.9041 | Generator Loss: 45180822962.1760 | Time: 14.67s\n","Epoch [31/300] | Critic Loss: -29934419879919.6172 | Generator Loss: 1392949661270.0161 | Time: 14.59s\n","Epoch [41/300] | Critic Loss: -300029108996276.2500 | Generator Loss: 16107160467406.8477 | Time: 14.70s\n","Epoch [51/300] | Critic Loss: -2066011291301970.0000 | Generator Loss: 102558559807471.6094 | Time: 14.23s\n","Epoch [61/300] | Critic Loss: -9222863723180326.0000 | Generator Loss: 454643537363861.5000 | Time: 14.28s\n","Epoch [71/300] | Critic Loss: -32111210151163200.0000 | Generator Loss: 1607190608064020.5000 | Time: 14.43s\n","Epoch [81/300] | Critic Loss: -91874853517137024.0000 | Generator Loss: 4870039483229143.0000 | Time: 14.30s\n","Epoch [91/300] | Critic Loss: -258617252433125952.0000 | Generator Loss: 12841368185387090.0000 | Time: 14.52s\n","Epoch [101/300] | Critic Loss: -611773135308397184.0000 | Generator Loss: 29867834194274548.0000 | Time: 14.27s\n","Epoch [111/300] | Critic Loss: -1266030762747070720.0000 | Generator Loss: 64498119819334256.0000 | Time: 14.23s\n","Epoch [121/300] | Critic Loss: -2643509312713144832.0000 | Generator Loss: 131592180819411600.0000 | Time: 14.92s\n","Epoch [131/300] | Critic Loss: -4950536724648372224.0000 | Generator Loss: 251198468149009696.0000 | Time: 14.43s\n","Epoch [141/300] | Critic Loss: -8671922605624093696.0000 | Generator Loss: 446372726205675712.0000 | Time: 14.39s\n","Epoch [151/300] | Critic Loss: -14957355130801403904.0000 | Generator Loss: 797078869823910272.0000 | Time: 14.58s\n","Epoch [161/300] | Critic Loss: -26008289369788706816.0000 | Generator Loss: 1337506253339021568.0000 | Time: 14.72s\n","Epoch [171/300] | Critic Loss: -42651107438429986816.0000 | Generator Loss: 2222494932691389952.0000 | Time: 16.56s\n","Epoch [181/300] | Critic Loss: -69788552826387996672.0000 | Generator Loss: 3459085076435618816.0000 | Time: 14.96s\n","Epoch [191/300] | Critic Loss: -103611785216894320640.0000 | Generator Loss: 5268714006424609792.0000 | Time: 14.61s\n","Epoch [201/300] | Critic Loss: -152473169730044624896.0000 | Generator Loss: 8013475471371832320.0000 | Time: 15.30s\n","Epoch [211/300] | Critic Loss: -231058304782150139904.0000 | Generator Loss: 11840859232062601216.0000 | Time: 15.63s\n","Epoch [221/300] | Critic Loss: -326970416707546447872.0000 | Generator Loss: 17318004541748754432.0000 | Time: 15.06s\n","Epoch [231/300] | Critic Loss: -474018849323254546432.0000 | Generator Loss: 24721939085703057408.0000 | Time: 15.06s\n","Epoch [241/300] | Critic Loss: -697654764522660954112.0000 | Generator Loss: 35155352652051828736.0000 | Time: 15.00s\n","Epoch [251/300] | Critic Loss: -927934868962774614016.0000 | Generator Loss: 49177900485633515520.0000 | Time: 15.87s\n","Epoch [261/300] | Critic Loss: -1256615509614416625664.0000 | Generator Loss: 64151410753975975936.0000 | Time: 15.26s\n","Epoch [271/300] | Critic Loss: -1612523659923594149888.0000 | Generator Loss: 83000450123399462912.0000 | Time: 16.90s\n","Epoch [281/300] | Critic Loss: -2061177395661217464320.0000 | Generator Loss: 105744622481606950912.0000 | Time: 15.89s\n","Epoch [291/300] | Critic Loss: -2553209272710817382400.0000 | Generator Loss: 132235856650462412800.0000 | Time: 15.56s\n","Epoch [300/300] | Critic Loss: -3006763169163300044800.0000 | Generator Loss: 159762913596694757376.0000 | Time: 16.46s\n","Generating 1000 synthetic samples...\n","Evaluating statistical similarity...\n","\n","Jensen-Shannon Divergence (average): 0.5244962297353419\n","Wasserstein Distance (average): 1.1514701448552538\n","\n","Evaluating Machine Learning Utility (TSTR)...\n","\n","TSTR Results:\n","Logistic Regression: Accuracy = 0.0600, F1 Score = 0.0118\n","MLP: Accuracy = 0.0278, F1 Score = 0.0121\n","Random Forest: Accuracy = 0.0322, F1 Score = 0.0020\n","XGBoost: Accuracy = 0.0362, F1 Score = 0.0025\n","\n","Evaluation complete! Check the output directory for plots and saved model.\n"]}]}]}