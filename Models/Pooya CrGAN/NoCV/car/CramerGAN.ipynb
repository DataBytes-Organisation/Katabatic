{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNI/OHqN0Bqm+j8EpfOWCg5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torch.autograd import Variable\n","from torch.autograd import grad as torch_grad\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, classification_report\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","from scipy.stats import entropy\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","import time\n","from tqdm import tqdm\n","from google.colab import drive\n","\n","\n","drive.mount('/content/drive/')\n","%cd /content/drive/MyDrive/Colab Notebooks/Katabatic/CrGAN/car/\n","\n","# Suppress warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Setting random seeds for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","# Check if CUDA is available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"239pXGX1-bGZ","executionInfo":{"status":"ok","timestamp":1743757879829,"user_tz":-660,"elapsed":11600,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}},"outputId":"49e7a971-571d-4b40-9c9f-0a39777ca03c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n","/content/drive/MyDrive/Colab Notebooks/Katabatic/CrGAN/car\n","Using device: cpu\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"HOHTrzFj-Sf8","executionInfo":{"status":"ok","timestamp":1743757879944,"user_tz":-660,"elapsed":113,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}}},"outputs":[],"source":["def load_car_data(data_path):\n","    # Column names for the car dataset\n","    column_names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n","\n","    # Load data\n","    df = pd.read_csv(data_path, header=None, names=column_names)\n","\n","    print(f\"Dataset shape: {df.shape}\")\n","\n","    # Split into train and test (80% train, 20% test)\n","    df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n","\n","    print(f\"Training data shape: {df_train.shape}\")\n","    print(f\"Testing data shape: {df_test.shape}\")\n","\n","    # Split features and target\n","    X_train = df_train.drop('class', axis=1)\n","    y_train = df_train['class']\n","    X_test = df_test.drop('class', axis=1)\n","    y_test = df_test['class']\n","\n","    # Identify categorical columns\n","    categorical_cols = X_train.columns.tolist()  # All columns are categorical\n","\n","    # Create preprocessing pipeline for categorical data using one-hot encoding\n","    categorical_transformer = Pipeline(steps=[\n","        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n","    ])\n","\n","    preprocessor = ColumnTransformer(\n","        transformers=[\n","            ('cat', categorical_transformer, categorical_cols)\n","        ])\n","\n","    # Fit and transform the data\n","    X_train_transformed = preprocessor.fit_transform(X_train)\n","    X_test_transformed = preprocessor.transform(X_test)\n","\n","    print(f\"Processed training data shape: {X_train_transformed.shape}\")\n","    print(f\"Processed testing data shape: {X_test_transformed.shape}\")\n","\n","    # Get one-hot encoding feature names for later use\n","    cat_encoder = preprocessor.named_transformers_['cat'].named_steps['onehot']\n","    feature_names = cat_encoder.get_feature_names_out(categorical_cols)\n","\n","    # Create label encoder for the target classes\n","    unique_classes = sorted(df['class'].unique())\n","    label_encoder = {cls: i for i, cls in enumerate(unique_classes)}\n","    inverse_label_encoder = {i: cls for cls, i in label_encoder.items()}\n","\n","    # Encode targets\n","    y_train_encoded = y_train.map(label_encoder)\n","    y_test_encoded = y_test.map(label_encoder)\n","\n","    # Store original categorical values\n","    cat_values = {}\n","    for col in categorical_cols:\n","        cat_values[col] = sorted(df[col].unique())\n","\n","    return (X_train_transformed, y_train_encoded, X_test_transformed, y_test_encoded,\n","            preprocessor, feature_names, label_encoder, inverse_label_encoder, cat_values,\n","            X_train, y_train)\n","\n","# Custom dataset class\n","class CarDataset(Dataset):\n","    def __init__(self, features, labels=None):\n","        self.features = torch.tensor(features, dtype=torch.float32)\n","        if labels is not None:\n","            self.labels = torch.tensor(labels.values, dtype=torch.long)\n","        else:\n","            self.labels = None\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, idx):\n","        if self.labels is not None:\n","            return self.features[idx], self.labels[idx]\n","        else:\n","            return self.features[idx]\n","\n","# Generator Network\n","class Generator(nn.Module):\n","    def __init__(self, latent_dim, output_dim):\n","        super(Generator, self).__init__()\n","        self.latent_dim = latent_dim\n","        self.output_dim = output_dim\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(latent_dim, 128),\n","            nn.BatchNorm1d(128),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Linear(128, 256),\n","            nn.BatchNorm1d(256),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Linear(256, 512),\n","            nn.BatchNorm1d(512),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Linear(512, output_dim),\n","            nn.Sigmoid()  # Use Sigmoid for one-hot encoded data\n","        )\n","\n","    def forward(self, z):\n","        return self.model(z)\n","\n","# Critic Network (Discriminator)\n","class Critic(nn.Module):\n","    def __init__(self, input_dim):\n","        super(Critic, self).__init__()\n","        self.input_dim = input_dim\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(input_dim, 256),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(256, 128),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(128, 64),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(64, 1)\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Cramer GAN Implementation\n","class CramerGAN:\n","    def __init__(self, data_dim, latent_dim=100, critic_iterations=5, lambda_gp=10):\n","        self.data_dim = data_dim\n","        self.latent_dim = latent_dim\n","        self.critic_iterations = critic_iterations\n","        self.lambda_gp = lambda_gp\n","\n","        # Initialize networks\n","        self.generator = Generator(latent_dim, data_dim).to(device)\n","        self.critic = Critic(data_dim).to(device)\n","\n","        # Setup optimizers\n","        self.g_optimizer = optim.Adam(self.generator.parameters(), lr=0.0002, betas=(0.5, 0.9))\n","        self.c_optimizer = optim.Adam(self.critic.parameters(), lr=0.0002, betas=(0.5, 0.9))\n","\n","        # Initialize loss tracking\n","        self.g_losses = []\n","        self.c_losses = []\n","\n","    def _critic_train_iteration(self, real_data, batch_size):\n","        # Generate random noise\n","        noise = torch.randn(batch_size, self.latent_dim).to(device)\n","\n","        # Generate fake data\n","        fake_data = self.generator(noise)\n","\n","        # Get critic outputs\n","        critic_real = self.critic(real_data)\n","        critic_fake = self.critic(fake_data)\n","\n","        # Calculate Cramer distance\n","        critic_real2 = self.critic(torch.roll(real_data, shifts=1, dims=0))\n","        critic_fake2 = self.critic(torch.roll(fake_data, shifts=1, dims=0))\n","\n","        # Cramer GAN loss function\n","        c_loss = torch.mean(critic_real - critic_fake) - 0.5 * torch.mean(torch.pow(critic_real - critic_real2, 2)) + 0.5 * torch.mean(torch.pow(critic_fake - critic_fake2, 2))\n","\n","        # Calculate gradient penalty\n","        alpha = torch.rand(batch_size, 1).to(device)\n","        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n","        interpolates.requires_grad_(True)\n","\n","        critic_interpolates = self.critic(interpolates)\n","        gradients = torch_grad(outputs=critic_interpolates, inputs=interpolates,\n","                              grad_outputs=torch.ones_like(critic_interpolates).to(device),\n","                              create_graph=True, retain_graph=True)[0]\n","\n","        gradients = gradients.view(batch_size, -1)\n","        gradient_penalty = self.lambda_gp * ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n","\n","        # Update critic\n","        self.c_optimizer.zero_grad()\n","        c_loss_total = c_loss + gradient_penalty\n","        c_loss_total.backward()\n","        self.c_optimizer.step()\n","\n","        return c_loss_total.item()\n","\n","    def _generator_train_iteration(self, batch_size):\n","        # Generate random noise\n","        noise = torch.randn(batch_size, self.latent_dim).to(device)\n","\n","        # Generate fake data\n","        fake_data = self.generator(noise)\n","\n","        # Calculate critic outputs\n","        critic_fake = self.critic(fake_data)\n","        critic_fake2 = self.critic(torch.roll(fake_data, shifts=1, dims=0))\n","\n","        # Generator loss is negative of critic loss\n","        g_loss = -torch.mean(critic_fake) + 0.5 * torch.mean(torch.pow(critic_fake - critic_fake2, 2))\n","\n","        # Update generator\n","        self.g_optimizer.zero_grad()\n","        g_loss.backward()\n","        self.g_optimizer.step()\n","\n","        return g_loss.item()\n","\n","    def train(self, data_loader, epochs, save_interval=10, verbose=True):\n","        for epoch in range(epochs):\n","            epoch_start_time = time.time()\n","            c_loss_total = 0\n","            g_loss_total = 0\n","            num_batches = 0\n","\n","            for i, (real_data, _) in enumerate(data_loader):\n","                batch_size = real_data.size(0)\n","                real_data = real_data.to(device)\n","\n","                # Train critic\n","                for _ in range(self.critic_iterations):\n","                    c_loss = self._critic_train_iteration(real_data, batch_size)\n","                c_loss_total += c_loss\n","\n","                # Train generator\n","                g_loss = self._generator_train_iteration(batch_size)\n","                g_loss_total += g_loss\n","\n","                num_batches += 1\n","\n","            # Calculate average loss for the epoch\n","            c_loss_avg = c_loss_total / num_batches\n","            g_loss_avg = g_loss_total / num_batches\n","\n","            self.c_losses.append(c_loss_avg)\n","            self.g_losses.append(g_loss_avg)\n","\n","            epoch_time = time.time() - epoch_start_time\n","\n","            if verbose and (epoch % save_interval == 0 or epoch == epochs - 1):\n","                print(f\"Epoch [{epoch+1}/{epochs}] | Critic Loss: {c_loss_avg:.4f} | Generator Loss: {g_loss_avg:.4f} | Time: {epoch_time:.2f}s\")\n","\n","    def generate_samples(self, num_samples):\n","        self.generator.eval()\n","        noise = torch.randn(num_samples, self.latent_dim).to(device)\n","        with torch.no_grad():\n","            generated_data = self.generator(noise).cpu().numpy()\n","        self.generator.train()\n","        return generated_data\n","\n","    def save_model(self, path):\n","        torch.save({\n","            'generator_state_dict': self.generator.state_dict(),\n","            'critic_state_dict': self.critic.state_dict(),\n","            'g_optimizer_state_dict': self.g_optimizer.state_dict(),\n","            'c_optimizer_state_dict': self.c_optimizer.state_dict(),\n","        }, path)\n","\n","    def load_model(self, path):\n","        checkpoint = torch.load(path)\n","        self.generator.load_state_dict(checkpoint['generator_state_dict'])\n","        self.critic.load_state_dict(checkpoint['critic_state_dict'])\n","        self.g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n","        self.c_optimizer.load_state_dict(checkpoint['c_optimizer_state_dict'])\n","\n","# Post-process generated data for categorical features\n","def post_process_car_data(synthetic_data, preprocessor, cat_values, feature_names):\n","    \"\"\"\n","    Post-process the synthetic data to convert one-hot encoded features back to categorical values\n","    \"\"\"\n","    # Create a DataFrame with one-hot encoded columns\n","    synthetic_df = pd.DataFrame(synthetic_data, columns=feature_names)\n","\n","    # Extract categorical feature groups\n","    result_df = pd.DataFrame()\n","\n","    # Process each categorical column\n","    for col_name, values in cat_values.items():\n","        # Get one-hot columns for this feature\n","        col_pattern = f\"{col_name}_\"\n","        category_cols = [c for c in feature_names if c.startswith(col_pattern)]\n","\n","        # Get the most likely category for each sample\n","        category_probs = synthetic_df[category_cols].values\n","        category_indices = np.argmax(category_probs, axis=1)\n","\n","        # Map indices back to original categories\n","        # Extract the original category from the one-hot column name\n","        categories = [c.split('_', 1)[1] for c in category_cols]\n","        result_df[col_name] = [categories[idx] for idx in category_indices]\n","\n","    return result_df\n","\n"]},{"cell_type":"code","source":["# Evaluation Metrics\n","\n","# 1. Machine Learning Utility (TSTR)\n","def evaluate_tstr(real_data, synthetic_data, real_labels, random_state=42):\n","    \"\"\"\n","    Train classifiers on synthetic data and test on real data (TSTR)\n","    Returns accuracy for each classifier\n","    \"\"\"\n","    # Train-test split for real data\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        real_data, real_labels, test_size=0.2, random_state=random_state\n","    )\n","\n","    # Synthetic data (all used for training)\n","    X_synth = synthetic_data\n","\n","    # Ensure proper dimensions for labels\n","    if isinstance(y_train, pd.Series):\n","        y_train = y_train.values\n","\n","    # Create synthetic labels based on real distribution\n","    num_classes = len(np.unique(y_train))\n","    class_distribution = np.bincount(y_train.astype(int), minlength=num_classes) / len(y_train)\n","    np.random.seed(random_state)\n","    y_synth = np.random.choice(range(num_classes), size=len(X_synth), p=class_distribution)\n","\n","    # Define classifiers\n","    classifiers = {\n","        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=random_state),\n","        'MLP': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=random_state),\n","        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=random_state),\n","        'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=random_state)\n","    }\n","\n","    results = {}\n","\n","    for name, clf in classifiers.items():\n","        # Train on synthetic data\n","        clf.fit(X_synth, y_synth)\n","\n","        # Test on real data\n","        y_pred = clf.predict(X_test)\n","\n","        # Calculate metrics\n","        accuracy = accuracy_score(y_test, y_pred)\n","\n","        # We use weighted f1 since we have multiple classes\n","        f1 = f1_score(y_test, y_pred, average='weighted')\n","\n","        results[name] = {\n","            'accuracy': accuracy,\n","            'f1_score': f1\n","        }\n","\n","    return results\n","\n","# 2. Statistical Similarity for categorical data\n","def evaluate_categorical_similarity(real_df, synthetic_df):\n","    \"\"\"\n","    Calculate statistical similarity for categorical features\n","    \"\"\"\n","    results = {'JSD': {}}\n","\n","    # For each categorical column, calculate JSD\n","    for col in real_df.columns:\n","        # Get value counts\n","        real_counts = real_df[col].value_counts(normalize=True).sort_index()\n","        synth_counts = synthetic_df[col].value_counts(normalize=True).sort_index()\n","\n","        # Align the distributions\n","        all_categories = sorted(set(real_counts.index) | set(synth_counts.index))\n","        real_dist = np.array([real_counts.get(cat, 0) for cat in all_categories])\n","        synth_dist = np.array([synth_counts.get(cat, 0) for cat in all_categories])\n","\n","        # Add small epsilon to avoid zeros\n","        epsilon = 1e-10\n","        real_dist = real_dist + epsilon\n","        synth_dist = synth_dist + epsilon\n","\n","        # Normalize\n","        real_dist = real_dist / real_dist.sum()\n","        synth_dist = synth_dist / synth_dist.sum()\n","\n","        # Calculate JSD\n","        jsd = jensen_shannon_divergence(real_dist, synth_dist)\n","        results['JSD'][col] = jsd\n","\n","    # Average JSD across all features\n","    results['JSD_avg'] = np.mean(list(results['JSD'].values()))\n","\n","    return results\n","\n","def jensen_shannon_divergence(p, q):\n","    \"\"\"\n","    Calculate Jensen-Shannon Divergence between distributions p and q\n","    \"\"\"\n","    # Ensure p and q are normalized\n","    p = p / np.sum(p)\n","    q = q / np.sum(q)\n","\n","    m = 0.5 * (p + q)\n","\n","    # Calculate JSD\n","    jsd = 0.5 * (entropy(p, m) + entropy(q, m))\n","\n","    return jsd\n","\n","def plot_loss_curves(model):\n","    \"\"\"\n","    Plot the loss curves for the generator and critic\n","    \"\"\"\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(model.g_losses, label='Generator Loss')\n","    plt.plot(model.c_losses, label='Critic Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.title('CramerGAN Training Loss for Car Dataset')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig('car_crgan_loss_curves.png')\n","    plt.close()\n","\n","def plot_categorical_distributions(real_df, synthetic_df):\n","    \"\"\"\n","    Plot distributions of real vs synthetic data for categorical features\n","    \"\"\"\n","    n_features = len(real_df.columns)\n","\n","    plt.figure(figsize=(15, n_features * 4))\n","\n","    for i, col in enumerate(real_df.columns):\n","        plt.subplot(n_features, 1, i+1)\n","\n","        # Calculate proportions\n","        real_props = real_df[col].value_counts(normalize=True).sort_index()\n","        synth_props = synthetic_df[col].value_counts(normalize=True).sort_index()\n","\n","        # Get all categories\n","        all_categories = sorted(set(real_props.index) | set(synth_props.index))\n","\n","        # Create a DataFrame for plotting\n","        plot_df = pd.DataFrame({\n","            'Category': all_categories * 2,\n","            'Proportion': [real_props.get(cat, 0) for cat in all_categories] +\n","                         [synth_props.get(cat, 0) for cat in all_categories],\n","            'Type': ['Real'] * len(all_categories) + ['Synthetic'] * len(all_categories)\n","        })\n","\n","        # Plot\n","        sns.barplot(x='Category', y='Proportion', hue='Type', data=plot_df)\n","        plt.title(f'Distribution for {col}')\n","        plt.xticks(rotation=45)\n","        plt.ylabel('Proportion')\n","        plt.legend()\n","\n","    plt.tight_layout()\n","    plt.savefig('car_categorical_distributions.png')\n","    plt.close()\n","\n","def plot_class_distribution(real_labels, synthetic_labels, label_encoder):\n","    \"\"\"\n","    Plot the class distribution of real vs synthetic data\n","    \"\"\"\n","    plt.figure(figsize=(12, 6))\n","\n","    # Get class counts\n","    real_class_counts = pd.Series(real_labels).value_counts(normalize=True)\n","    synth_class_counts = pd.Series(synthetic_labels).value_counts(normalize=True)\n","\n","    # Create inverse label encoder\n","    inverse_label_encoder = {v: k for k, v in label_encoder.items()}\n","\n","    # Get all classes\n","    all_classes = sorted(set(real_class_counts.index) | set(synth_class_counts.index))\n","\n","    # Create plot data\n","    plot_df = pd.DataFrame({\n","        'Class': [inverse_label_encoder.get(c, c) for c in all_classes] * 2,\n","        'Proportion': [real_class_counts.get(c, 0) for c in all_classes] +\n","                     [synth_class_counts.get(c, 0) for c in all_classes],\n","        'Type': ['Real'] * len(all_classes) + ['Synthetic'] * len(all_classes)\n","    })\n","\n","    # Plot\n","    sns.barplot(x='Class', y='Proportion', hue='Type', data=plot_df)\n","    plt.title('Class Distribution: Real vs Synthetic Car Evaluations')\n","    plt.xticks(rotation=45)\n","    plt.tight_layout()\n","    plt.savefig('car_class_distribution.png')\n","    plt.close()\n","\n","# Main function\n","def main():\n","    # File path\n","    data_path = 'data/car.csv'\n","\n","    # Load and preprocess data\n","    (X_train_transformed, y_train, X_test_transformed, y_test,\n","     preprocessor, feature_names, label_encoder, inverse_label_encoder,\n","     cat_values, X_train_original, y_train_original) = load_car_data(data_path)\n","\n","    # Create dataset and dataloader\n","    train_dataset = CarDataset(X_train_transformed, y_train)\n","    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","\n","    # Initialize and train the model\n","    data_dim = X_train_transformed.shape[1]\n","    latent_dim = 100\n","\n","    print(f\"Data dimension: {data_dim}\")\n","    print(f\"Latent dimension: {latent_dim}\")\n","\n","    crgan = CramerGAN(data_dim, latent_dim)\n","\n","    # Train the model\n","    epochs = 300\n","    print(f\"Training CramerGAN for {epochs} epochs...\")\n","    crgan.train(train_loader, epochs, save_interval=10)\n","\n","    # Save the model\n","    crgan.save_model('car_crgan_model.pt')\n","\n","    # Plot loss curves\n","    plot_loss_curves(crgan)\n","\n","    # Generate synthetic data\n","    num_samples = 1000\n","    print(f\"Generating {num_samples} synthetic samples...\")\n","    synthetic_data_raw = crgan.generate_samples(num_samples)\n","\n","    # Post-process the synthetic data\n","    synthetic_df = post_process_car_data(synthetic_data_raw, preprocessor, cat_values, feature_names)\n","\n","    # Generate synthetic labels using a classifier trained on real data\n","    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n","    clf.fit(X_train_transformed, y_train)\n","    synthetic_labels_raw = clf.predict(synthetic_data_raw)\n","\n","    # Convert numeric labels to original class values\n","    synthetic_labels = [inverse_label_encoder[label] for label in synthetic_labels_raw]\n","\n","    # Add class labels to the synthetic dataframe\n","    synthetic_df['class'] = synthetic_labels\n","\n","    # Save the synthetic data\n","    synthetic_df.to_csv('synthetic_car_data.csv', index=False)\n","\n","    # Plot distributions\n","    plot_categorical_distributions(X_train_original, synthetic_df.drop('class', axis=1))\n","\n","    # Plot class distribution\n","    plot_class_distribution(y_train_original, synthetic_df['class'], label_encoder)\n","\n","    # Statistical similarity evaluation\n","    print(\"Evaluating statistical similarity...\")\n","    stat_results = evaluate_categorical_similarity(X_train_original, synthetic_df.drop('class', axis=1))\n","\n","    print(\"\\nJensen-Shannon Divergence (average):\", stat_results['JSD_avg'])\n","    print(\"\\nJSD per feature:\")\n","    for feature, jsd in stat_results['JSD'].items():\n","        print(f\"  {feature}: {jsd:.4f}\")\n","\n","    # Machine Learning Utility (TSTR) evaluation\n","    print(\"\\nEvaluating Machine Learning Utility (TSTR)...\")\n","    tstr_results = evaluate_tstr(X_train_transformed, synthetic_data_raw, y_train)\n","\n","    print(\"\\nTSTR Results:\")\n","    for clf_name, metrics in tstr_results.items():\n","        print(f\"{clf_name}: Accuracy = {metrics['accuracy']:.4f}, F1 Score = {metrics['f1_score']:.4f}\")\n","\n","    print(\"\\nEvaluation complete! Check the output directory for plots and saved model.\")\n","\n"],"metadata":{"id":"0X3OYNNG-X6Z","executionInfo":{"status":"ok","timestamp":1743757880027,"user_tz":-660,"elapsed":81,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":[" if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jls5Bzf--Uid","executionInfo":{"status":"ok","timestamp":1743758273662,"user_tz":-660,"elapsed":393633,"user":{"displayName":"Pooya Forghani","userId":"09002604427734085896"}},"outputId":"758b9c6b-3034-4457-8104-df702bb2d908"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset shape: (1729, 7)\n","Training data shape: (1383, 7)\n","Testing data shape: (346, 7)\n","Processed training data shape: (1383, 27)\n","Processed testing data shape: (346, 27)\n","Data dimension: 27\n","Latent dimension: 100\n","Training CramerGAN for 300 epochs...\n","Epoch [1/300] | Critic Loss: 3.8554 | Generator Loss: -0.5898 | Time: 2.20s\n","Epoch [11/300] | Critic Loss: -4.8336 | Generator Loss: -1.8212 | Time: 1.20s\n","Epoch [21/300] | Critic Loss: -418.5198 | Generator Loss: 89.9413 | Time: 1.11s\n","Epoch [31/300] | Critic Loss: -17773.0257 | Generator Loss: 4861.4480 | Time: 1.74s\n","Epoch [41/300] | Critic Loss: -247343.7216 | Generator Loss: 76637.0178 | Time: 1.07s\n","Epoch [51/300] | Critic Loss: -1657627.8864 | Generator Loss: 597947.8068 | Time: 1.15s\n","Epoch [61/300] | Critic Loss: -9442491.1364 | Generator Loss: 3085252.8864 | Time: 1.44s\n","Epoch [71/300] | Critic Loss: -34292301.0909 | Generator Loss: 11778900.5455 | Time: 1.79s\n","Epoch [81/300] | Critic Loss: -112259998.5455 | Generator Loss: 41866527.4545 | Time: 1.26s\n","Epoch [91/300] | Critic Loss: -289431988.3636 | Generator Loss: 107919898.9091 | Time: 1.18s\n","Epoch [101/300] | Critic Loss: -802366141.0909 | Generator Loss: 266329851.6364 | Time: 1.05s\n","Epoch [111/300] | Critic Loss: -1452698827.6364 | Generator Loss: 601581169.4545 | Time: 1.04s\n","Epoch [121/300] | Critic Loss: -3125348864.0000 | Generator Loss: 1238097856.0000 | Time: 1.05s\n","Epoch [131/300] | Critic Loss: -6669002658.9091 | Generator Loss: 2411454976.0000 | Time: 1.25s\n","Epoch [141/300] | Critic Loss: -10435984011.6364 | Generator Loss: 4586243979.6364 | Time: 1.61s\n","Epoch [151/300] | Critic Loss: -25654072878.5455 | Generator Loss: 8075209122.9091 | Time: 1.05s\n","Epoch [161/300] | Critic Loss: -36802285195.6364 | Generator Loss: 13492697181.0909 | Time: 1.04s\n","Epoch [171/300] | Critic Loss: -56698067502.5455 | Generator Loss: 21857046714.1818 | Time: 1.06s\n","Epoch [181/300] | Critic Loss: -90772975988.3636 | Generator Loss: 37949676264.7273 | Time: 1.07s\n","Epoch [191/300] | Critic Loss: -172579518277.8182 | Generator Loss: 60665363735.2727 | Time: 1.10s\n","Epoch [201/300] | Critic Loss: -255813983511.2727 | Generator Loss: 84378117585.4545 | Time: 1.58s\n","Epoch [211/300] | Critic Loss: -363200590941.0909 | Generator Loss: 126520427985.4545 | Time: 1.05s\n","Epoch [221/300] | Critic Loss: -435917419613.0909 | Generator Loss: 197476807773.0909 | Time: 1.19s\n","Epoch [231/300] | Critic Loss: -732236215389.0909 | Generator Loss: 285873933218.9091 | Time: 1.05s\n","Epoch [241/300] | Critic Loss: -1103771598848.0000 | Generator Loss: 374595762734.5455 | Time: 1.15s\n","Epoch [251/300] | Critic Loss: -1537664310551.2727 | Generator Loss: 582053667560.7273 | Time: 1.17s\n","Epoch [261/300] | Critic Loss: -2202911762618.1816 | Generator Loss: 727462415266.9091 | Time: 1.04s\n","Epoch [271/300] | Critic Loss: -2726849603770.1816 | Generator Loss: 1111580415813.8181 | Time: 1.60s\n","Epoch [281/300] | Critic Loss: -3663603052171.6362 | Generator Loss: 1445735539432.7273 | Time: 1.66s\n","Epoch [291/300] | Critic Loss: -5244425219723.6367 | Generator Loss: 1820564199237.8181 | Time: 1.18s\n","Epoch [300/300] | Critic Loss: -6696004294469.8184 | Generator Loss: 2544297526737.4546 | Time: 1.47s\n","Generating 1000 synthetic samples...\n","Evaluating statistical similarity...\n","\n","Jensen-Shannon Divergence (average): 0.46492280263243746\n","\n","JSD per feature:\n","  buying: 0.6902\n","  maint: 0.3897\n","  doors: 0.3785\n","  persons: 0.3223\n","  lug_boot: 0.6931\n","  safety: 0.3158\n","\n","Evaluating Machine Learning Utility (TSTR)...\n","\n","TSTR Results:\n","Logistic Regression: Accuracy = 0.7148, F1 Score = 0.5959\n","MLP: Accuracy = 0.5668, F1 Score = 0.5515\n","Random Forest: Accuracy = 0.6895, F1 Score = 0.5945\n","XGBoost: Accuracy = 0.4513, F1 Score = 0.4670\n","\n","Evaluation complete! Check the output directory for plots and saved model.\n"]}]}]}