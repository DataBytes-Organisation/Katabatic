{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c43aae40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessed data saved to: preprocessed_letter.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"letter-official.csv\")\n",
    "\n",
    "# Encode the target column\n",
    "le = LabelEncoder()\n",
    "df['class'] = le.fit_transform(df['class'])\n",
    "\n",
    "# Identify categorical feature columns (exclude the target)\n",
    "cat_cols = [col for col in df.columns if col != 'class']\n",
    "\n",
    "# One-hot encode all categorical features\n",
    "df_cat = pd.get_dummies(df[cat_cols], drop_first=False).astype(int)\n",
    "\n",
    "# Combine with the target\n",
    "df_target = df[['class']]\n",
    "df_processed = pd.concat([df_cat, df_target], axis=1)\n",
    "\n",
    "# Save preprocessed data\n",
    "out_path = \"preprocessed_letter.csv\"\n",
    "df_processed.to_csv(out_path, index=False)\n",
    "print(\"✅ Preprocessed data saved to:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bb13e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "— Rep 1/3 Fold 1/2 —\n",
      "Epoch 1/100 - D_loss=0.5853, G_loss=3.3117\n",
      "Epoch 20/100 - D_loss=0.0339, G_loss=4.0738\n",
      "Epoch 40/100 - D_loss=0.1069, G_loss=4.0237\n",
      "Epoch 60/100 - D_loss=0.2775, G_loss=5.2093\n",
      "Epoch 80/100 - D_loss=0.2159, G_loss=4.9296\n",
      "Epoch 100/100 - D_loss=0.1496, G_loss=4.2301\n",
      "\n",
      "— Rep 1/3 Fold 2/2 —\n",
      "Epoch 1/100 - D_loss=0.4528, G_loss=2.9751\n",
      "Epoch 20/100 - D_loss=0.0422, G_loss=4.1894\n",
      "Epoch 40/100 - D_loss=0.0306, G_loss=4.3150\n",
      "Epoch 60/100 - D_loss=0.0501, G_loss=4.6644\n",
      "Epoch 80/100 - D_loss=0.0107, G_loss=5.8826\n",
      "Epoch 100/100 - D_loss=0.1077, G_loss=5.1141\n",
      "\n",
      "— Rep 2/3 Fold 1/2 —\n",
      "Epoch 1/100 - D_loss=1.0902, G_loss=2.2109\n",
      "Epoch 20/100 - D_loss=0.3344, G_loss=3.2917\n",
      "Epoch 40/100 - D_loss=0.0141, G_loss=5.2785\n",
      "Epoch 60/100 - D_loss=0.0473, G_loss=5.4545\n",
      "Epoch 80/100 - D_loss=0.1243, G_loss=6.6321\n",
      "Epoch 100/100 - D_loss=0.0087, G_loss=4.8518\n",
      "\n",
      "— Rep 2/3 Fold 2/2 —\n",
      "Epoch 1/100 - D_loss=1.5435, G_loss=2.5496\n",
      "Epoch 20/100 - D_loss=0.0713, G_loss=4.5292\n",
      "Epoch 40/100 - D_loss=0.0182, G_loss=4.4686\n",
      "Epoch 60/100 - D_loss=0.0159, G_loss=4.5953\n",
      "Epoch 80/100 - D_loss=0.3242, G_loss=4.3041\n",
      "Epoch 100/100 - D_loss=0.1943, G_loss=5.6003\n",
      "\n",
      "— Rep 3/3 Fold 1/2 —\n",
      "Epoch 1/100 - D_loss=0.5407, G_loss=2.8284\n",
      "Epoch 20/100 - D_loss=0.0171, G_loss=5.1248\n",
      "Epoch 40/100 - D_loss=0.3313, G_loss=4.5948\n",
      "Epoch 60/100 - D_loss=0.3124, G_loss=4.0645\n",
      "Epoch 80/100 - D_loss=0.0118, G_loss=5.3370\n",
      "Epoch 100/100 - D_loss=0.0521, G_loss=5.4840\n",
      "\n",
      "— Rep 3/3 Fold 2/2 —\n",
      "Epoch 1/100 - D_loss=0.7220, G_loss=2.7133\n",
      "Epoch 20/100 - D_loss=0.7628, G_loss=4.1777\n",
      "Epoch 40/100 - D_loss=0.5240, G_loss=4.6444\n",
      "Epoch 60/100 - D_loss=0.0245, G_loss=4.7713\n",
      "Epoch 80/100 - D_loss=0.0164, G_loss=4.5650\n",
      "Epoch 100/100 - D_loss=0.5419, G_loss=5.5416\n",
      "\n",
      "=== CV Results (mean ± std) ===\n",
      " • LR   TSTR = 4.27% ± 1.15%\n",
      " • MLP  TSTR = 4.08% ± 0.60%\n",
      " • RF   TSTR = 3.43% ± 0.65%\n",
      " • XGB  TSTR = 3.66% ± 0.82%\n",
      " • JSD = 0.0000 ± 0.0000\n",
      " • WD  = 0.0000 ± 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "PREPROCESSED_PATH = \"preprocessed_letter.csv\"\n",
    "LATENT_DIM        = 100\n",
    "BATCH_SIZE        = 64\n",
    "EPOCHS            = 100\n",
    "REPEATS           = 3\n",
    "FOLDS             = 2\n",
    "SYN_RATIO         = 0.5\n",
    "TARGET_COL        = \"class\"\n",
    "\n",
    "# Imports\n",
    "# !pip install torch torchvision scipy scikit-learn xgboost --quiet\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(PREPROCESSED_PATH)\n",
    "X_full = df.drop(columns=[TARGET_COL]).values.astype(np.float32)\n",
    "y_full = df[TARGET_COL].values.astype(int)\n",
    "NUMERIC_COLS = []  # No numeric features\n",
    "num_idx = []       # No numeric indices\n",
    "\n",
    "# Define models\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 512), nn.ReLU(),\n",
    "            nn.Linear(512, 256), nn.ReLU(),\n",
    "            nn.Linear(256, out_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 512), nn.ReLU(),\n",
    "            nn.Linear(512, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 1), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_cramer_gan(G, D, loader, epochs):\n",
    "    G, D = G.to(device), D.to(device)\n",
    "    optg = optim.Adam(G.parameters(), lr=2e-4)\n",
    "    optd = optim.Adam(D.parameters(), lr=2e-4)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    for ep in range(1, epochs+1):\n",
    "        for real_batch, _ in loader:\n",
    "            real_batch = real_batch.to(device)\n",
    "            bsz = real_batch.size(0)\n",
    "\n",
    "            # Discriminator step\n",
    "            optd.zero_grad()\n",
    "            z = torch.randn(bsz, LATENT_DIM, device=device)\n",
    "            fake = G(z).detach()\n",
    "            d_real = D(real_batch)\n",
    "            d_fake = D(fake)\n",
    "            lossd = loss_fn(d_real, torch.ones_like(d_real)) + \\\n",
    "                    loss_fn(d_fake, torch.zeros_like(d_fake))\n",
    "            lossd.backward()\n",
    "            optd.step()\n",
    "\n",
    "            # Generator step\n",
    "            optg.zero_grad()\n",
    "            z = torch.randn(bsz, LATENT_DIM, device=device)\n",
    "            fake2 = G(z)\n",
    "            dg = D(fake2)\n",
    "            lossg = loss_fn(dg, torch.ones_like(dg))\n",
    "            lossg.backward()\n",
    "            optg.step()\n",
    "\n",
    "        if ep % 20 == 0 or ep == 1 or ep == epochs:\n",
    "            print(f\"Epoch {ep}/{epochs} - D_loss={lossd.item():.4f}, G_loss={lossg.item():.4f}\")\n",
    "    return G, D\n",
    "\n",
    "def generate_synthetic(G, n_samples):\n",
    "    G = G.to(device).eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(n_samples, LATENT_DIM, device=device)\n",
    "        return G(z).cpu().numpy()\n",
    "\n",
    "def compute_tstr_all(X_real, y_real, X_syn, y_syn):\n",
    "    results = {}\n",
    "    for name, clf in [\n",
    "        (\"LR\", LogisticRegression(max_iter=5000)),\n",
    "        (\"MLP\", MLPClassifier(hidden_layer_sizes=(128,64), max_iter=1000)),\n",
    "        (\"RF\", RandomForestClassifier(n_estimators=200)),\n",
    "        (\"XGB\", XGBClassifier(eval_metric=\"logloss\"))\n",
    "    ]:\n",
    "        clf.fit(X_syn, y_syn)\n",
    "        results[name] = clf.score(X_real, y_real) * 100.0\n",
    "    return results\n",
    "\n",
    "def compute_jsd_wd(X_real, X_syn, num_idx):\n",
    "    if not num_idx:\n",
    "        return 0.0, 0.0\n",
    "    jsd_list, wd_list = [], []\n",
    "    for i in num_idx:\n",
    "        p_real, _ = np.histogram(X_real[:, i], bins=50, density=True)\n",
    "        p_syn, _ = np.histogram(X_syn[:, i], bins=50, density=True)\n",
    "        jsd_list.append(jensenshannon(p_real, p_syn))\n",
    "        wd_list.append(wasserstein_distance(X_real[:, i], X_syn[:, i]))\n",
    "    return np.mean(jsd_list), np.mean(wd_list)\n",
    "\n",
    "# Cross-validation\n",
    "tstr_scores = {m: [] for m in [\"LR\", \"MLP\", \"RF\", \"XGB\"]}\n",
    "jsd_scores, wd_scores = [], []\n",
    "kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "for rep in range(REPEATS):\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X_full), 1):\n",
    "        print(f\"\\n— Rep {rep + 1}/{REPEATS} Fold {fold}/{FOLDS} —\")\n",
    "        X_tr, X_te = X_full[train_idx], X_full[test_idx]\n",
    "        y_tr, y_te = y_full[train_idx], y_full[test_idx]\n",
    "\n",
    "        loader = DataLoader(\n",
    "            TensorDataset(torch.from_numpy(X_tr), torch.from_numpy(y_tr)),\n",
    "            batch_size=BATCH_SIZE, shuffle=True\n",
    "        )\n",
    "\n",
    "        G = Generator(LATENT_DIM, X_tr.shape[1])\n",
    "        D = Discriminator(X_tr.shape[1])\n",
    "        G, D = train_cramer_gan(G, D, loader, EPOCHS)\n",
    "\n",
    "        X_syn = generate_synthetic(G, int(SYN_RATIO * len(X_tr)))\n",
    "        y_syn = np.random.choice(y_tr, size=X_syn.shape[0], replace=True)\n",
    "\n",
    "        tstr = compute_tstr_all(X_te, y_te, X_syn, y_syn)\n",
    "        for m, score in tstr.items():\n",
    "            tstr_scores[m].append(score)\n",
    "\n",
    "        js, wd = compute_jsd_wd(X_te, X_syn, num_idx)\n",
    "        jsd_scores.append(js)\n",
    "        wd_scores.append(wd)\n",
    "\n",
    "# Report results\n",
    "print(\"\\n=== CV Results (mean ± std) ===\")\n",
    "for m in tstr_scores:\n",
    "    arr = np.array(tstr_scores[m])\n",
    "    print(f\" • {m:4s} TSTR = {arr.mean():.2f}% ± {arr.std():.2f}%\")\n",
    "print(f\" • JSD = {np.mean(jsd_scores):.4f} ± {np.std(jsd_scores):.4f}\")\n",
    "print(f\" • WD  = {np.mean(wd_scores):.4f} ± {np.std(wd_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "237d33d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - D_loss=1.0826, G_loss=1.2519\n",
      "Epoch 20/100 - D_loss=0.0299, G_loss=4.1428\n",
      "Epoch 40/100 - D_loss=0.0471, G_loss=7.1316\n",
      "Epoch 60/100 - D_loss=0.0187, G_loss=5.0072\n",
      "Epoch 80/100 - D_loss=0.0135, G_loss=5.2872\n",
      "Epoch 100/100 - D_loss=0.0800, G_loss=5.9345\n",
      "\n",
      "✅ Final synthetic dataset saved to: synthetic_letter_final.csv\n"
     ]
    }
   ],
   "source": [
    "# Final model on all data\n",
    "full_loader = DataLoader(\n",
    "    TensorDataset(torch.from_numpy(X_full), torch.from_numpy(y_full)),\n",
    "    batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "Gf = Generator(LATENT_DIM, X_full.shape[1])\n",
    "Df = Discriminator(X_full.shape[1])\n",
    "Gf, Df = train_cramer_gan(Gf, Df, full_loader, EPOCHS)\n",
    "\n",
    "# Generate final synthetic data\n",
    "n_final = int(SYN_RATIO * len(X_full))\n",
    "Xf_syn = generate_synthetic(Gf, n_final)\n",
    "yf_syn = np.random.choice(y_full, size=n_final, replace=True)\n",
    "\n",
    "# Save to CSV\n",
    "cols = df.columns[:-1]\n",
    "syn_df = pd.DataFrame(Xf_syn, columns=cols)\n",
    "syn_df[TARGET_COL] = yf_syn\n",
    "out_path = \"synthetic_letter_final.csv\"\n",
    "syn_df.to_csv(out_path, index=False)\n",
    "print(f\"\\n✅ Final synthetic dataset saved to: {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
