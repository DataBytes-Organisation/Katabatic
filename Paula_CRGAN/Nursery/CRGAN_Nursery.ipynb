{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install liac-arff --quiet\n",
        "\n",
        "# 2. Load and convert\n",
        "import arff\n",
        "import pandas as pd\n",
        "\n",
        "# Adjust the paths as needed\n",
        "arff_path = \"/content/drive/MyDrive/Katabatic/Data/Nursery/nursery 1.arff\"\n",
        "csv_path  = \"/content/drive/MyDrive/Katabatic/Data/Nursery/nursery.csv\"\n",
        "\n",
        "# 3. Parse the ARFF\n",
        "with open(arff_path, 'r') as f:\n",
        "    arff_data = arff.load(f)\n",
        "\n",
        "# 4. Build a DataFrame\n",
        "columns = [attr[0] for attr in arff_data['attributes']]\n",
        "df      = pd.DataFrame(arff_data['data'], columns=columns)\n",
        "\n",
        "# 5. Save out as CSV\n",
        "df.to_csv(csv_path, index=False)  # <-- remove the \"//7\" here!\n",
        "print(f\"✅ Saved CSV to {csv_path}\")"
      ],
      "metadata": {
        "id": "i8xLxx3D2IVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eb05ad0-6981-48af-a7cb-398d552d2c21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved CSV to /content/drive/MyDrive/Katabatic/Data/Nursery/nursery.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 1. Load & inspect\n",
        "file_path = '/content/drive/MyDrive/Katabatic/Data/Nursery/nursery.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# 2. Features vs target\n",
        "feature_cols = df.columns[:-1].tolist()\n",
        "target_col   = 'class'\n",
        "\n",
        "# 3. Encode target to 0/1/2/… integers\n",
        "le = LabelEncoder()\n",
        "df[target_col] = le.fit_transform(df[target_col])\n",
        "\n",
        "# 4. One‐hot encode features → 0/1 ints\n",
        "df_cat = pd.get_dummies(df[feature_cols], drop_first=False)\n",
        "df_cat = df_cat.astype(int)\n",
        "\n",
        "# 5. Record each categorical group’s size\n",
        "cat_group_sizes = [df[col].nunique() for col in feature_cols]\n",
        "\n",
        "# 6. Build final DataFrame & save\n",
        "df_processed = pd.concat([\n",
        "    df_cat.reset_index(drop=True),\n",
        "    df[target_col].reset_index(drop=True).rename('class')\n",
        "], axis=1)\n",
        "\n",
        "out_path = '/content/drive/MyDrive/Katabatic/Data/Nursery/preprocessed_nursery.csv'\n",
        "df_processed.to_csv(out_path, index=False)\n",
        "print(f\"✅ Preprocessed data (0/1) saved to: {out_path}\")\n",
        "print(\"➤ cat_group_sizes:\", cat_group_sizes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QwZ-Kc6O5P2",
        "outputId": "d5daf317-40a2-4d40-815b-5669284c99c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Preprocessed data (0/1) saved to: /content/drive/MyDrive/Katabatic/Data/Nursery/preprocessed_nursery.csv\n",
            "➤ cat_group_sizes: [3, 5, 4, 4, 3, 2, 3, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#make sure each original feature sums to 1 across its one-hot columns:\n",
        "for col in feature_cols:\n",
        "    # grab the dummy columns for that feature\n",
        "    dummies = [c for c in df_cat.columns if c.startswith(col + '_')]\n",
        "    assert (df_cat[dummies].sum(axis=1) == 1).all()\n",
        "\n",
        "# 2) confirm target is integer coded\n",
        "print(df_processed['class'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li282-xuQfQL",
        "outputId": "b71f01f2-a823-46f2-a4c5-05be627ac2fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class\n",
            "0    4320\n",
            "1    4266\n",
            "3    4044\n",
            "4     328\n",
            "2       2\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- 0. Installs & Imports --------------------\n",
        "!pip install torch torchvision scipy scikit-learn xgboost --quiet\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.linear_model    import LogisticRegression\n",
        "from sklearn.neural_network  import MLPClassifier\n",
        "from sklearn.ensemble        import RandomForestClassifier\n",
        "from xgboost                 import XGBClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from scipy.spatial.distance  import jensenshannon\n",
        "from scipy.stats             import wasserstein_distance\n",
        "\n",
        "# -------------------- 1. Hyperparameters & Paths --------------------\n",
        "PREPROCESSED_PATH = \"/content/drive/MyDrive/Katabatic/Data/Nursery/preprocessed_nursery.csv\"\n",
        "LATENT_DIM        = 100\n",
        "BATCH_SIZE        = 64\n",
        "EPOCHS            = 100    # per spec for small/medium\n",
        "REPEATS           = 3\n",
        "FOLDS             = 2\n",
        "SYN_RATIO         = 0.5    # 50% synthetic of train size\n",
        "TARGET_COL        = \"class\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"▶ Running on\", device)\n",
        "\n",
        "# -------------------- 2. CR-GAN Model Definitions --------------------\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(z_dim,256), nn.ReLU(),\n",
        "            nn.Linear(256,512),   nn.ReLU(),\n",
        "            nn.Linear(512,256),   nn.ReLU(),\n",
        "            nn.Linear(256,out_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    def forward(self, z): return self.net(z)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim,512), nn.ReLU(),\n",
        "            nn.Linear(512,256),   nn.ReLU(),\n",
        "            nn.Linear(256,128),   nn.ReLU(),\n",
        "            nn.Linear(128,1),     nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "def train_cramer_gan(G, D, loader, epochs):\n",
        "    G, D = G.to(device), D.to(device)\n",
        "    opt_g = optim.Adam(G.parameters(), lr=2e-4)\n",
        "    opt_d = optim.Adam(D.parameters(), lr=2e-4)\n",
        "    loss_fn = nn.BCELoss()\n",
        "    for ep in range(1, epochs+1):\n",
        "        ld, lg = 0.0, 0.0\n",
        "        for xb, _ in loader:\n",
        "            xb = xb.to(device); bsz = xb.size(0)\n",
        "            # — D step\n",
        "            opt_d.zero_grad()\n",
        "            z     = torch.randn(bsz, LATENT_DIM, device=device)\n",
        "            fake  = G(z).detach()\n",
        "            ld    = loss_fn(D(xb),   torch.ones_like(D(xb))) + \\\n",
        "                    loss_fn(D(fake), torch.zeros_like(D(fake)))\n",
        "            ld.backward(); opt_d.step()\n",
        "            # — G step\n",
        "            opt_g.zero_grad()\n",
        "            z2    = torch.randn(bsz, LATENT_DIM, device=device)\n",
        "            fake2 = G(z2)\n",
        "            lg    = loss_fn(D(fake2), torch.ones_like(D(fake2)))\n",
        "            lg.backward(); opt_g.step()\n",
        "        if ep==1 or ep%20==0 or ep==epochs:\n",
        "            print(f\"  Epoch {ep}/{epochs}  D_loss={ld:.4f}  G_loss={lg:.4f}\")\n",
        "    return G\n",
        "\n",
        "def generate_synthetic(G, n):\n",
        "    G = G.to(device).eval()\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(n, LATENT_DIM, device=device)\n",
        "        return G(z).cpu().numpy()\n",
        "\n",
        "# -------------------- 3. Metrics --------------------\n",
        "def compute_tstr_all(Xr, yr, Xs, ys):\n",
        "    out = {}\n",
        "    lr  = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=5000)\n",
        "    mlp = MLPClassifier(hidden_layer_sizes=(128,64), max_iter=1000)\n",
        "    rf  = RandomForestClassifier(n_estimators=200)\n",
        "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "\n",
        "    for name, clf in [(\"LR\",lr), (\"MLP\",mlp), (\"RF\",rf), (\"XGB\",xgb)]:\n",
        "        clf.fit(Xs, ys)\n",
        "        out[name] = clf.score(Xr, yr)*100\n",
        "    return out\n",
        "\n",
        "def compute_jsd_wd(Xr, Xs):\n",
        "    # fully categorical → return zero\n",
        "    return 0.0, 0.0\n",
        "\n",
        "def ensure_all_classes(Xs, ys, classes, Xtr, ytr):\n",
        "    missing = set(classes) - set(np.unique(ys))\n",
        "    if missing:\n",
        "        for c in missing:\n",
        "            idx = np.where(ytr==c)[0][0]\n",
        "            Xs  = np.vstack([Xs, Xtr[idx:idx+1]])\n",
        "            ys  = np.hstack([ys, [c]])\n",
        "    return Xs, ys\n",
        "\n",
        "# -------------------- 4. Load Preprocessed Nursery --------------------\n",
        "df     = pd.read_csv(PREPROCESSED_PATH)\n",
        "X_full = df.drop(columns=[TARGET_COL]).values.astype(np.float32)\n",
        "y_full = df[TARGET_COL].values.astype(int)\n",
        "all_cls = np.unique(y_full)\n",
        "print(f\"▶ Loaded {len(df)} rows, {X_full.shape[1]} features, classes = {all_cls}\")\n",
        "\n",
        "# -------------------- 5. 3×(2-Fold StratifiedCV) --------------------\n",
        "skf   = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=0)\n",
        "tstrs = {m:[] for m in ['LR','MLP','RF','XGB']}\n",
        "jsds, wds = [], []\n",
        "\n",
        "for rep in range(1, REPEATS+1):\n",
        "    for fold, (tr, te) in enumerate(skf.split(X_full, y_full), start=1):\n",
        "        print(f\"\\n▷ Rep {rep}/{REPEATS} · Fold {fold}/{FOLDS}\")\n",
        "        Xtr, Xte = X_full[tr], X_full[te]\n",
        "        ytr, yte = y_full[tr], y_full[te]\n",
        "\n",
        "        loader = DataLoader(\n",
        "            TensorDataset(torch.from_numpy(Xtr), torch.from_numpy(ytr)),\n",
        "            batch_size=BATCH_SIZE, shuffle=True\n",
        "        )\n",
        "\n",
        "        G = train_cramer_gan(\n",
        "            Generator(LATENT_DIM, Xtr.shape[1]),\n",
        "            Discriminator(Xtr.shape[1]),\n",
        "            loader, epochs=EPOCHS\n",
        "        )\n",
        "\n",
        "        n_syn = int(SYN_RATIO * len(Xtr))\n",
        "        Xs    = generate_synthetic(G, n_syn)\n",
        "        ys    = np.random.choice(ytr, size=n_syn, replace=True)\n",
        "\n",
        "        # *** This is the only new line ***\n",
        "        Xs, ys = ensure_all_classes(Xs, ys, all_cls, Xtr, ytr)\n",
        "\n",
        "        t_res = compute_tstr_all(Xte, yte, Xs, ys)\n",
        "        for m, sc in t_res.items(): tstrs[m].append(sc)\n",
        "\n",
        "        js, wd = compute_jsd_wd(Xte, Xs)\n",
        "        jsds.append(js); wds.append(wd)\n",
        "\n",
        "# -------------------- 6. Report CV Results --------------------\n",
        "print(\"\\n=== CV Results (mean ± std) ===\")\n",
        "for m in ['LR','MLP','RF','XGB']:\n",
        "    arr = np.array(tstrs[m])\n",
        "    print(f\" • {m:4s} TSTR = {arr.mean():.2f}% ± {arr.std():.2f}%\")\n",
        "print(f\" • JSD = {np.mean(jsds):.4f} ± {np.std(jsds):.4f}\")\n",
        "print(f\" • WD  = {np.mean(wds):.4f} ± {np.std(wds):.4f}\")\n",
        "\n",
        "# -------------------- 7. Retrain Full & Save Synthetic --------------------\n",
        "full_loader = DataLoader(\n",
        "    TensorDataset(torch.from_numpy(X_full), torch.from_numpy(y_full)),\n",
        "    batch_size=BATCH_SIZE, shuffle=True\n",
        ")\n",
        "Gf = train_cramer_gan(\n",
        "    Generator(LATENT_DIM, X_full.shape[1]),\n",
        "    Discriminator(X_full.shape[1]),\n",
        "    full_loader, epochs=EPOCHS\n",
        ")\n",
        "\n",
        "n_final = int(SYN_RATIO * len(X_full))\n",
        "Xf      = generate_synthetic(Gf, n_final)\n",
        "yf      = np.random.choice(y_full, size=n_final, replace=True)\n",
        "Xf, yf  = ensure_all_classes(Xf, yf, all_cls, X_full, y_full)\n",
        "\n",
        "syn_df       = pd.DataFrame(Xf, columns=df.columns[:-1])\n",
        "syn_df[TARGET_COL] = yf\n",
        "out_path     = \"/content/drive/MyDrive/Katabatic/Data/Nursery/synthetic_nursery_final.csv\"\n",
        "syn_df.to_csv(out_path, index=False)\n",
        "print(f\"\\n✅ Final synthetic ({len(syn_df)} rows) saved to:\\n  {out_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_4H2bi6_fR9",
        "outputId": "2c10fd3f-e0ee-4f7e-e10f-18cf1f0b28e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▶ Running on cuda\n",
            "▶ Loaded 12960 rows, 27 features, classes = [0 1 2 3 4]\n",
            "\n",
            "▷ Rep 1/3 · Fold 1/2\n",
            "  Epoch 1/100  D_loss=0.1747  G_loss=2.6384\n",
            "  Epoch 20/100  D_loss=0.0964  G_loss=2.9975\n",
            "  Epoch 40/100  D_loss=0.6011  G_loss=1.9851\n",
            "  Epoch 60/100  D_loss=0.6976  G_loss=3.0092\n",
            "  Epoch 80/100  D_loss=0.9122  G_loss=2.0498\n",
            "  Epoch 100/100  D_loss=0.2935  G_loss=3.4371\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:18:52] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "▷ Rep 1/3 · Fold 2/2\n",
            "  Epoch 1/100  D_loss=0.1906  G_loss=2.1762\n",
            "  Epoch 20/100  D_loss=0.2836  G_loss=2.2121\n",
            "  Epoch 40/100  D_loss=0.8973  G_loss=2.0247\n",
            "  Epoch 60/100  D_loss=0.7665  G_loss=2.3363\n",
            "  Epoch 80/100  D_loss=1.4778  G_loss=1.9949\n",
            "  Epoch 100/100  D_loss=0.3122  G_loss=2.2812\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:20:11] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "▷ Rep 2/3 · Fold 1/2\n",
            "  Epoch 1/100  D_loss=1.0745  G_loss=0.6577\n",
            "  Epoch 20/100  D_loss=0.4465  G_loss=2.1971\n",
            "  Epoch 40/100  D_loss=0.7571  G_loss=1.3199\n",
            "  Epoch 60/100  D_loss=0.4052  G_loss=2.2853\n",
            "  Epoch 80/100  D_loss=0.4184  G_loss=3.8349\n",
            "  Epoch 100/100  D_loss=0.3619  G_loss=2.5207\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:21:30] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "▷ Rep 2/3 · Fold 2/2\n",
            "  Epoch 1/100  D_loss=0.7249  G_loss=1.5297\n",
            "  Epoch 20/100  D_loss=1.6203  G_loss=3.3977\n",
            "  Epoch 40/100  D_loss=0.4617  G_loss=2.6178\n",
            "  Epoch 60/100  D_loss=0.5842  G_loss=1.4953\n",
            "  Epoch 80/100  D_loss=0.4449  G_loss=1.7838\n",
            "  Epoch 100/100  D_loss=0.3360  G_loss=2.4956\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:22:50] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "▷ Rep 3/3 · Fold 1/2\n",
            "  Epoch 1/100  D_loss=0.7856  G_loss=1.2085\n",
            "  Epoch 20/100  D_loss=0.5127  G_loss=1.6985\n",
            "  Epoch 40/100  D_loss=0.6554  G_loss=1.5560\n",
            "  Epoch 60/100  D_loss=1.1353  G_loss=2.5727\n",
            "  Epoch 80/100  D_loss=0.9417  G_loss=1.3404\n",
            "  Epoch 100/100  D_loss=0.2031  G_loss=2.7692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:24:13] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "▷ Rep 3/3 · Fold 2/2\n",
            "  Epoch 1/100  D_loss=0.4087  G_loss=2.1599\n",
            "  Epoch 20/100  D_loss=0.6336  G_loss=1.8926\n",
            "  Epoch 40/100  D_loss=0.6927  G_loss=3.3008\n",
            "  Epoch 60/100  D_loss=0.5921  G_loss=2.1852\n",
            "  Epoch 80/100  D_loss=0.3510  G_loss=1.8082\n",
            "  Epoch 100/100  D_loss=0.5509  G_loss=2.8592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:25:31] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== CV Results (mean ± std) ===\n",
            " • LR   TSTR = 32.64% ± 5.69%\n",
            " • MLP  TSTR = 32.44% ± 3.28%\n",
            " • RF   TSTR = 31.01% ± 2.59%\n",
            " • XGB  TSTR = 34.54% ± 4.47%\n",
            " • JSD = 0.0000 ± 0.0000\n",
            " • WD  = 0.0000 ± 0.0000\n",
            "  Epoch 1/100  D_loss=0.6030  G_loss=2.3094\n",
            "  Epoch 20/100  D_loss=0.4812  G_loss=2.3373\n",
            "  Epoch 40/100  D_loss=0.5238  G_loss=2.9237\n",
            "  Epoch 60/100  D_loss=0.0130  G_loss=4.9876\n",
            "  Epoch 80/100  D_loss=0.0036  G_loss=6.6024\n",
            "  Epoch 100/100  D_loss=0.0005  G_loss=7.9298\n",
            "\n",
            "✅ Final synthetic (6481 rows) saved to:\n",
            "  /content/drive/MyDrive/Katabatic/Data/Nursery/synthetic_nursery_final.csv\n"
          ]
        }
      ]
    }
  ]
}