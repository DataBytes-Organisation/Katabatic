{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOdEumVEQOhx",
        "outputId": "6f40d427-17d5-4796-e9db-b1971334fdf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Saved CSV to /content/drive/MyDrive/Katabatic/Data/Credit/credit.csv\n"
          ]
        }
      ],
      "source": [
        "# 1. Install the ARFF parser\n",
        "!pip install liac-arff --quiet\n",
        "\n",
        "# 2. Load and convert\n",
        "import arff\n",
        "import pandas as pd\n",
        "\n",
        "# Adjust the path if needed\n",
        "arff_path = \"/content/drive/MyDrive/Katabatic/Data/Credit/credit-a.arff\"\n",
        "csv_path  = \"/content/drive/MyDrive/Katabatic/Data/Credit/credit.csv\"\n",
        "\n",
        "# Parse the ARFF\n",
        "with open(arff_path, 'r') as f:\n",
        "    arff_data = arff.load(f)\n",
        "\n",
        "# Build a DataFrame\n",
        "columns = [attr[0] for attr in arff_data['attributes']]\n",
        "df = pd.DataFrame(arff_data['data'], columns=columns)\n",
        "\n",
        "# Save out as CSV\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(f\"Saved CSV to {csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def interval_to_mid(x):\n",
        "    \"\"\"\n",
        "    Parses strings like '10-30', '-inf-100', '' into a single float:\n",
        "     - If two numbers: returns their average\n",
        "     - If one number: returns it\n",
        "     - If no numbers or blank: returns np.nan\n",
        "    \"\"\"\n",
        "    s = str(x)\n",
        "    if pd.isna(x) or not s.strip():\n",
        "        return np.nan\n",
        "    nums = re.findall(r\"-?\\d+\\.?\\d*\", s)\n",
        "    if not nums:\n",
        "        return np.nan\n",
        "    vals = list(map(float, nums))\n",
        "    return vals[0] if len(vals) == 1 else sum(vals) / len(vals)\n",
        "\n",
        "# Load your CSV\n",
        "in_path = \"/content/drive/MyDrive/Katabatic/Data/Credit/credit.csv\"\n",
        "df = pd.read_csv(in_path)\n",
        "print(\"Raw shape:\", df.shape)\n",
        "\n",
        "# Columns that hold intervals\n",
        "interval_cols = ['A2', 'A3', 'A8', 'A11', 'A14', 'A15']\n",
        "\n",
        "# Apply midpoint conversion, now .astype(float) will work\n",
        "for col in interval_cols:\n",
        "    df[col] = df[col].apply(interval_to_mid).astype(float)\n",
        "print(\"After interval‚Üímidpoint:\\n\", df[interval_cols].head())\n",
        "\n",
        "# Map the target to 0/1 (adjust if your actual labels differ)\n",
        "df['class'] = df['class'].map({'n': 0, 'y': 1})\n",
        "\n",
        "# One-hot encode the rest\n",
        "cat_cols = [c for c in df.columns if c not in interval_cols + ['class']]\n",
        "df_cat  = pd.get_dummies(df[cat_cols], drop_first=False).astype(int)\n",
        "\n",
        "# Recombine\n",
        "df_processed = pd.concat([\n",
        "    df[interval_cols].reset_index(drop=True),\n",
        "    df_cat.reset_index(drop=True),\n",
        "    df[['class']].reset_index(drop=True)\n",
        "], axis=1)\n",
        "\n",
        "print(\"Processed shape:\", df_processed.shape)\n",
        "\n",
        "# Save\n",
        "out_path = \"/content/drive/MyDrive/Katabatic/Data/Credit/credit_preprocessed.csv\"\n",
        "df_processed.to_csv(out_path, index=False)\n",
        "print(\"‚úÖ Saved preprocessed data to:\", out_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsPOOBqMW1wX",
        "outputId": "f25360ef-a3d0-4096-ba1f-5372ea7cddc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw shape: (690, 16)\n",
            "After interval‚Üímidpoint:\n",
            "       A2      A3    A8  A11    A14    A15\n",
            "0 -38.96 -4.2075  1.02 -1.0  105.0 -492.0\n",
            "1  38.96  4.2075  1.02  2.5 -105.0  492.0\n",
            "2 -38.96 -4.2075  1.02 -0.5  105.0  492.0\n",
            "3 -38.96 -4.2075  1.02  2.5 -105.0 -492.0\n",
            "4 -38.96  4.2075  1.02 -0.5  105.0 -492.0\n",
            "Processed shape: (690, 52)\n",
            "‚úÖ Saved preprocessed data to: /content/drive/MyDrive/Katabatic/Data/Credit/credit_preprocessed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Katabatic/Data/Credit/credit_preprocessed.csv\")\n",
        "\n",
        "# 1) Features are everything except the final 'class' column\n",
        "X = df.drop(columns=[\"class\"]).values.astype(np.float32)\n",
        "\n",
        "# 2) Labels are the single integer column\n",
        "y = df[\"class\"].astype(int).values\n",
        "\n",
        "print(\"X shape:\", X.shape)   # e.g. (690, 51)\n",
        "print(\"y unique:\", np.unique(y))  # should be [0,1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUpq4y_1jrnQ",
        "outputId": "65caca53-0798-4e57-8ed1-bc7da4bfb322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (690, 51)\n",
            "y unique: [0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) Installs & Imports\n",
        "!pip install torch torchvision scipy scikit-learn xgboost --quiet\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.linear_model    import LogisticRegression\n",
        "from sklearn.neural_network  import MLPClassifier\n",
        "from sklearn.ensemble        import RandomForestClassifier\n",
        "from xgboost                 import XGBClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from scipy.stats             import wasserstein_distance\n",
        "from scipy.spatial.distance  import jensenshannon\n",
        "\n",
        "# 1) Hyperparameters & Paths\n",
        "PREPROCESSED_PATH = \"/content/drive/MyDrive/Katabatic/Data/Credit/credit_preprocessed.csv\"\n",
        "LATENT_DIM        = 100\n",
        "BATCH_SIZE        = 64\n",
        "EPOCHS            = 100    # ‚Äúmedium‚Äù dataset\n",
        "REPEATS           = 3\n",
        "FOLDS             = 2\n",
        "SYN_RATIO         = 0.5    # synthetic = 50% of train fold\n",
        "TARGET_COL        = \"class\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"üñ•Ô∏è  Running on\", device)\n",
        "\n",
        "\n",
        "# 2) CR-GAN Definitions (no final Sigmoid; we‚Äôll use BCEWithLogitsLoss)\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(z_dim,256), nn.ReLU(),\n",
        "            nn.Linear(256,512),   nn.ReLU(),\n",
        "            nn.Linear(512,256),   nn.ReLU(),\n",
        "            nn.Linear(256,out_dim),\n",
        "            nn.Tanh()             # keep [-1,1] range if you like\n",
        "        )\n",
        "    def forward(self, z):\n",
        "        return self.net(z)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim,512), nn.ReLU(),\n",
        "            nn.Linear(512,256),   nn.ReLU(),\n",
        "            nn.Linear(256,128),   nn.ReLU(),\n",
        "            nn.Linear(128,1)      # **no** Sigmoid here\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x).view(-1,1)  # raw logits\n",
        "\n",
        "\n",
        "def train_cramer_gan(G, D, loader, epochs):\n",
        "    G, D = G.to(device), D.to(device)\n",
        "    opt_g = optim.Adam(G.parameters(), lr=1e-4)   # lower LR for stability\n",
        "    opt_d = optim.Adam(D.parameters(), lr=2e-4)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        last_d, last_g = 0.0, 0.0\n",
        "        for xb, _ in loader:\n",
        "            xb = xb.to(device)\n",
        "            bsz = xb.size(0)\n",
        "\n",
        "            # Discriminator step\n",
        "            opt_d.zero_grad()\n",
        "            z      = torch.randn(bsz, LATENT_DIM, device=device)\n",
        "            fake   = G(z).detach()\n",
        "            d_real = D(xb)\n",
        "            d_fake = D(fake)\n",
        "            # real‚Üí1, fake‚Üí0\n",
        "            ld = loss_fn(d_real, torch.ones_like(d_real)) + \\\n",
        "                 loss_fn(d_fake, torch.zeros_like(d_fake))\n",
        "            ld.backward(); opt_d.step()\n",
        "\n",
        "            # Generator step\n",
        "            opt_g.zero_grad()\n",
        "            z2     = torch.randn(bsz, LATENT_DIM, device=device)\n",
        "            fake2  = G(z2)\n",
        "            dg     = D(fake2)\n",
        "            lg = loss_fn(dg, torch.ones_like(dg))\n",
        "            lg.backward(); opt_g.step()\n",
        "\n",
        "            last_d, last_g = ld.item(), lg.item()\n",
        "\n",
        "        if ep==1 or ep%20==0 or ep==epochs:\n",
        "            print(f\"Epoch {ep:3d}/{epochs}  D_loss={last_d:.4f}  G_loss={last_g:.4f}\")\n",
        "\n",
        "    return G\n",
        "\n",
        "\n",
        "def generate_synthetic(G, n_samples):\n",
        "    G = G.to(device).eval()\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(n_samples, LATENT_DIM, device=device)\n",
        "        return G(z).cpu().numpy()\n",
        "\n",
        "\n",
        "# 3) Metrics\n",
        "def compute_tstr_all(Xr, yr, Xs, ys, n_classes):\n",
        "    out = {}\n",
        "    lr  = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=5000)\n",
        "    lr .fit(Xs, ys); out['LR']  = lr.score(Xr, yr)*100\n",
        "\n",
        "    mlp = MLPClassifier(hidden_layer_sizes=(128,64), max_iter=1000)\n",
        "    mlp.fit(Xs, ys); out['MLP'] = mlp.score(Xr, yr)*100\n",
        "\n",
        "    rf  = RandomForestClassifier(n_estimators=200)\n",
        "    rf .fit(Xs, ys); out['RF']  = rf.score(Xr, yr)*100\n",
        "\n",
        "    xgb = XGBClassifier(\n",
        "        objective = 'binary:logistic' if n_classes==2 else 'multi:softprob',\n",
        "        num_class = n_classes if n_classes>2 else None,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss'\n",
        "    )\n",
        "    xgb.fit(Xs, ys); out['XGB'] = xgb.score(Xr, yr)*100\n",
        "\n",
        "    return out\n",
        "\n",
        "def compute_jsd_wd(Xr, Xs):\n",
        "    jsd_list, wd_list = [], []\n",
        "    for i in range(Xr.shape[1]):\n",
        "        p_real,_ = np.histogram(Xr[:,i], bins=50, density=True)\n",
        "        p_syn,_  = np.histogram(Xs[:,i], bins=50, density=True)\n",
        "        jsd_list.append(jensenshannon(p_real, p_syn))\n",
        "        wd_list .append(wasserstein_distance(Xr[:,i], Xs[:,i]))\n",
        "    return np.mean(jsd_list), np.mean(wd_list)\n",
        "\n",
        "\n",
        "# 4) Load Preprocessed Credit\n",
        "df      = pd.read_csv(PREPROCESSED_PATH)\n",
        "X_full  = df.drop(columns=[TARGET_COL]).values.astype(np.float32)\n",
        "y_full  = df[TARGET_COL].values.astype(int)\n",
        "n_classes = len(np.unique(y_full))\n",
        "print(f\"‚Üí {len(df)} rows, {X_full.shape[1]} feats, {n_classes} classes\")\n",
        "\n",
        "\n",
        "# 5) 3√ó(2-Fold CV)\n",
        "kf     = KFold(n_splits=FOLDS, shuffle=True, random_state=0)\n",
        "tstrs  = {m:[] for m in ['LR','MLP','RF','XGB']}\n",
        "jsds, wds = [], []\n",
        "\n",
        "for rep in range(1, REPEATS+1):\n",
        "    for fold, (tr, te) in enumerate(kf.split(X_full), start=1):\n",
        "        print(f\"\\n‚ñ∫ Rep {rep}/{REPEATS} ¬∑ Fold {fold}/{FOLDS}\")\n",
        "        Xtr, Xte = X_full[tr], X_full[te]\n",
        "        ytr, yte = y_full[tr], y_full[te]\n",
        "\n",
        "        loader = DataLoader(\n",
        "            TensorDataset(torch.from_numpy(Xtr), torch.from_numpy(ytr)),\n",
        "            batch_size=BATCH_SIZE, shuffle=True\n",
        "        )\n",
        "\n",
        "        # train GAN\n",
        "        G = train_cramer_gan(\n",
        "            Generator(LATENT_DIM, Xtr.shape[1]),\n",
        "            Discriminator(Xtr.shape[1]),\n",
        "            loader, epochs=EPOCHS\n",
        "        )\n",
        "\n",
        "        # synth 50% of train\n",
        "        n_syn = int(SYN_RATIO * len(Xtr))\n",
        "        Xs    = generate_synthetic(G, n_syn)\n",
        "        ys    = np.random.choice(ytr, size=n_syn, replace=True)\n",
        "\n",
        "        # zero-impute any NaN/‚àû\n",
        "        Xs = np.nan_to_num(Xs, nan=0., posinf=0., neginf=0.)\n",
        "        Xte= np.nan_to_num(Xte,nan=0., posinf=0., neginf=0.)\n",
        "\n",
        "        # TSTR\n",
        "        res = compute_tstr_all(Xte, yte, Xs, ys, n_classes)\n",
        "        for m,sc in res.items():\n",
        "            tstrs[m].append(sc)\n",
        "\n",
        "        # JSD & WD\n",
        "        js, wd = compute_jsd_wd(Xte, Xs)\n",
        "        jsds.append(js); wds.append(wd)\n",
        "\n",
        "\n",
        "# 6) Report CV\n",
        "print(\"\\n=== CV Results (mean ¬± std) ===\")\n",
        "for m in ['LR','MLP','RF','XGB']:\n",
        "    arr = np.array(tstrs[m])\n",
        "    print(f\" ‚Ä¢ {m:4s} TSTR = {arr.mean():.2f}% ¬± {arr.std():.2f}%\")\n",
        "print(f\" ‚Ä¢ JSD = {np.mean(jsds):.4f} ¬± {np.std(jsds):.4f}\")\n",
        "print(f\" ‚Ä¢ WD  = {np.mean(wds):.4f} ¬± {np.std(wds):.4f}\")\n",
        "\n",
        "\n",
        "# 7) Retrain on Full & Save 50%-sized Final Synthetic\n",
        "full_loader = DataLoader(\n",
        "    TensorDataset(torch.from_numpy(X_full), torch.from_numpy(y_full)),\n",
        "    batch_size=BATCH_SIZE, shuffle=True\n",
        ")\n",
        "Gf = train_cramer_gan(\n",
        "    Generator(LATENT_DIM, X_full.shape[1]),\n",
        "    Discriminator(X_full.shape[1]),\n",
        "    full_loader, epochs=EPOCHS\n",
        ")\n",
        "\n",
        "n_final = int(SYN_RATIO * len(X_full))\n",
        "Xf_syn  = generate_synthetic(Gf, n_final)\n",
        "yf_syn  = np.random.choice(y_full, size=n_final, replace=True)\n",
        "Xf_syn  = np.nan_to_num(Xf_syn, nan=0., posinf=0., neginf=0.)\n",
        "\n",
        "syn_df       = pd.DataFrame(Xf_syn, columns=df.columns[:-1])\n",
        "syn_df[TARGET_COL] = yf_syn\n",
        "outp = \"/content/drive/MyDrive/Katabatic/Data/Credit/credit_synthetic_final.csv\"\n",
        "syn_df.to_csv(outp, index=False)\n",
        "print(f\"\\n‚úÖ Final synthetic ({n_final} rows) saved to:\\n  {outp}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vgjy-_SZha8o",
        "outputId": "9f20094b-7ef1-43e6-8d82-e9d305a9ba25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üñ•Ô∏è  Running on cpu\n",
            "‚Üí 690 rows, 51 feats, 2 classes\n",
            "\n",
            "‚ñ∫ Rep 1/3 ¬∑ Fold 1/2\n",
            "Epoch   1/100  D_loss=nan  G_loss=nan\n",
            "Epoch  20/100  D_loss=nan  G_loss=nan\n",
            "Epoch  40/100  D_loss=nan  G_loss=nan\n",
            "Epoch  60/100  D_loss=nan  G_loss=nan\n",
            "Epoch  80/100  D_loss=nan  G_loss=nan\n",
            "Epoch 100/100  D_loss=nan  G_loss=nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:39:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ñ∫ Rep 1/3 ¬∑ Fold 2/2\n",
            "Epoch   1/100  D_loss=nan  G_loss=nan\n",
            "Epoch  20/100  D_loss=nan  G_loss=nan\n",
            "Epoch  40/100  D_loss=nan  G_loss=nan\n",
            "Epoch  60/100  D_loss=nan  G_loss=nan\n",
            "Epoch  80/100  D_loss=nan  G_loss=nan\n",
            "Epoch 100/100  D_loss=nan  G_loss=nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:39:50] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ñ∫ Rep 2/3 ¬∑ Fold 1/2\n",
            "Epoch   1/100  D_loss=nan  G_loss=nan\n",
            "Epoch  20/100  D_loss=nan  G_loss=nan\n",
            "Epoch  40/100  D_loss=nan  G_loss=nan\n",
            "Epoch  60/100  D_loss=nan  G_loss=nan\n",
            "Epoch  80/100  D_loss=nan  G_loss=nan\n",
            "Epoch 100/100  D_loss=nan  G_loss=nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:40:06] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ñ∫ Rep 2/3 ¬∑ Fold 2/2\n",
            "Epoch   1/100  D_loss=nan  G_loss=nan\n",
            "Epoch  20/100  D_loss=nan  G_loss=nan\n",
            "Epoch  40/100  D_loss=nan  G_loss=nan\n",
            "Epoch  60/100  D_loss=nan  G_loss=nan\n",
            "Epoch  80/100  D_loss=nan  G_loss=nan\n",
            "Epoch 100/100  D_loss=nan  G_loss=nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:40:22] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ñ∫ Rep 3/3 ¬∑ Fold 1/2\n",
            "Epoch   1/100  D_loss=nan  G_loss=nan\n",
            "Epoch  20/100  D_loss=nan  G_loss=nan\n",
            "Epoch  40/100  D_loss=nan  G_loss=nan\n",
            "Epoch  60/100  D_loss=nan  G_loss=nan\n",
            "Epoch  80/100  D_loss=nan  G_loss=nan\n",
            "Epoch 100/100  D_loss=nan  G_loss=nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:40:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ñ∫ Rep 3/3 ¬∑ Fold 2/2\n",
            "Epoch   1/100  D_loss=nan  G_loss=nan\n",
            "Epoch  20/100  D_loss=nan  G_loss=nan\n",
            "Epoch  40/100  D_loss=nan  G_loss=nan\n",
            "Epoch  60/100  D_loss=nan  G_loss=nan\n",
            "Epoch  80/100  D_loss=nan  G_loss=nan\n",
            "Epoch 100/100  D_loss=nan  G_loss=nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:40:56] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== CV Results (mean ¬± std) ===\n",
            " ‚Ä¢ LR   TSTR = 55.51% ¬± 0.43%\n",
            " ‚Ä¢ MLP  TSTR = 52.51% ¬± 6.90%\n",
            " ‚Ä¢ RF   TSTR = 55.51% ¬± 0.43%\n",
            " ‚Ä¢ XGB  TSTR = 55.51% ¬± 0.43%\n",
            " ‚Ä¢ JSD = 0.8152 ¬± 0.0165\n",
            " ‚Ä¢ WD  = 12.7183 ¬± 0.0004\n",
            "Epoch   1/100  D_loss=nan  G_loss=nan\n",
            "Epoch  20/100  D_loss=nan  G_loss=nan\n",
            "Epoch  40/100  D_loss=nan  G_loss=nan\n",
            "Epoch  60/100  D_loss=nan  G_loss=nan\n",
            "Epoch  80/100  D_loss=nan  G_loss=nan\n",
            "Epoch 100/100  D_loss=nan  G_loss=nan\n",
            "\n",
            "‚úÖ Final synthetic (345 rows) saved to:\n",
            "  /content/drive/MyDrive/Katabatic/Data/Credit/credit_synthetic_final.csv\n"
          ]
        }
      ]
    }
  ]
}