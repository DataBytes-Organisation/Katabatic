{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqMjJ8JCeot0",
        "outputId": "7c465131-7ec8-40cc-ddb4-47dabd9bdca2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚îÄ‚îÄ‚îÄ DATASET SANITY CHECK ‚îÄ‚îÄ‚îÄ\n",
            "Shape: 48842 rows √ó 15 cols\n",
            "\n",
            "Column dtypes:\n",
            "object    15 \n",
            "\n",
            "No missing values.\n",
            "\n",
            "Target distribution (class):\n",
            "class\n",
            "<=50K    76.07%\\n\n",
            ">50K     23.93%\\n\n",
            "Name: proportion, dtype: object\n",
            "Suggested numeric (interval) cols (5): ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
            "Suggested categorical cols (9): ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "\n",
        "def dataset_sanity_check(\n",
        "    df: pd.DataFrame,\n",
        "    target_col: str,\n",
        "    interval_cols: Optional[List[str]] = None,\n",
        "    max_unique_for_cat: int = 50\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Prints a summary of df:\n",
        "     - shape, dtype counts\n",
        "     - missing values per column\n",
        "     - target distribution\n",
        "     - suggested interval vs categorical splits\n",
        "    Args:\n",
        "      df: raw DataFrame\n",
        "      target_col: name of the target column\n",
        "      interval_cols: optional list of numeric feature names;\n",
        "                     if None, they'll be inferred by dtype and unique count\n",
        "      max_unique_for_cat: if dtype==object but unique<=this, treat as cat\n",
        "    \"\"\"\n",
        "    print(\"‚îÄ‚îÄ‚îÄ DATASET SANITY CHECK ‚îÄ‚îÄ‚îÄ\")\n",
        "    print(f\"Shape: {df.shape[0]} rows √ó {df.shape[1]} cols\")\n",
        "    print(\"\\nColumn dtypes:\")\n",
        "    print(df.dtypes.value_counts().to_string(), \"\\n\")\n",
        "\n",
        "    # Missing\n",
        "    missing = df.isna().sum()\n",
        "    if missing.any():\n",
        "        print(\"Missing values:\")\n",
        "        print(missing[missing>0].sort_values(), \"\\n\")\n",
        "    else:\n",
        "        print(\"No missing values.\\n\")\n",
        "\n",
        "    # Target distribution\n",
        "    if target_col not in df.columns:\n",
        "        raise ValueError(f\"Target column '{target_col}' not found in DataFrame!\")\n",
        "    print(f\"Target distribution ({target_col}):\")\n",
        "    print(df[target_col].value_counts(normalize=True).mul(100).round(2).astype(str) + \"%\\n\")\n",
        "\n",
        "    # Feature type suggestions\n",
        "    if interval_cols is None:\n",
        "        # infer numeric by dtype\n",
        "        num = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        # treat low‚Äêcard object cols as categorical too\n",
        "        obj = [\n",
        "            c for c in df.select_dtypes(include=[\"object\"]).columns\n",
        "            if df[c].nunique() <= max_unique_for_cat and c != target_col\n",
        "        ]\n",
        "        interval_cols = num\n",
        "        cat_cols = [c for c in df.columns if c not in interval_cols + [target_col]]\n",
        "    else:\n",
        "        # user‚Äêprovided\n",
        "        cat_cols = [c for c in df.columns if c not in interval_cols + [target_col]]\n",
        "\n",
        "    print(f\"Suggested numeric (interval) cols ({len(interval_cols)}): {interval_cols}\")\n",
        "    print(f\"Suggested categorical cols ({len(cat_cols)}): {cat_cols}\\n\")\n",
        "\n",
        "    # Warn about very small or very large datasets\n",
        "    if df.shape[0] < 100:\n",
        "        print(\"  Warning: fewer than 100 samples‚ÄîGANs may overfit or collapse.\")\n",
        "    elif df.shape[0] > 200_000:\n",
        "        print(\" Warning: very large dataset‚Äîtraining may be slow.\")\n",
        "\n",
        "    print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\\n\")\n",
        "\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ Example usage ‚îÄ‚îÄ‚îÄ\n",
        "if __name__ == \"__main__\":\n",
        "    # Load any dataset\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/Katabatic/Data/Adult/adult-official.csv\")\n",
        "\n",
        "    # Run the check\n",
        "    dataset_sanity_check(\n",
        "      df,\n",
        "      target_col=\"class\",\n",
        "      # you can also explicitly tell it which interval cols to use:\n",
        "      interval_cols=[\"age\",\"education-num\",\"capital-gain\",\"capital-loss\",\"hours-per-week\"]\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import os\n",
        "\n",
        "def interval_to_mid(x):\n",
        "    \"\"\"Convert interval strings to midpoints, e.g., '20-30' => 25.0\"\"\"\n",
        "    if pd.isna(x): return x\n",
        "    parts = re.findall(r'-?[\\d\\.]+|inf', str(x))\n",
        "    if len(parts) == 2:\n",
        "        lo, hi = parts\n",
        "        lo = float(lo) if lo not in (\"-inf\", \"inf\") else float(hi)\n",
        "        hi = float(hi) if hi not in (\"-inf\", \"inf\") else float(lo)\n",
        "        return (lo + hi) / 2\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return pd.NA\n",
        "\n",
        "def preprocess_dataset(file_path, target_column, interval_cols=None, output_path=\"processed_data.csv\"):\n",
        "    # Load dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "\n",
        "    # Auto-detect interval columns if not provided\n",
        "    if interval_cols is None:\n",
        "        interval_cols = df.select_dtypes(include=['object']).columns[\n",
        "            df.select_dtypes(include=['object']).apply(\n",
        "                lambda col: col.astype(str).str.contains(r'\\d+\\s*-\\s*\\d+').any()\n",
        "            )\n",
        "        ].tolist()\n",
        "\n",
        "    # Convert intervals to midpoints\n",
        "    for col in interval_cols:\n",
        "        df[col] = df[col].apply(interval_to_mid)\n",
        "\n",
        "    # Separate types\n",
        "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    if target_column in numeric_cols:\n",
        "        numeric_cols.remove(target_column)\n",
        "\n",
        "    # Encode target\n",
        "    le = LabelEncoder()\n",
        "    df[target_column] = le.fit_transform(df[target_column])\n",
        "\n",
        "    # Scale numeric columns if they exist\n",
        "    if numeric_cols:\n",
        "        scaler = StandardScaler()\n",
        "        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "        df_numeric = df[numeric_cols].reset_index(drop=True)\n",
        "    else:\n",
        "        df_numeric = pd.DataFrame()\n",
        "\n",
        "    # Identify categorical columns\n",
        "    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    # One-hot encode categorical columns if they exist\n",
        "    if cat_cols:\n",
        "        df_cat = pd.get_dummies(df[cat_cols], drop_first=False).astype(int).reset_index(drop=True)\n",
        "    else:\n",
        "        df_cat = pd.DataFrame()\n",
        "\n",
        "    # Target column\n",
        "    df_target = df[[target_column]].reset_index(drop=True)\n",
        "\n",
        "    # Final dataset\n",
        "    df_processed = pd.concat([df_numeric, df_cat, df_target], axis=1)\n",
        "\n",
        "    # Hyperparameter adjustment\n",
        "    num_rows = df_processed.shape[0]\n",
        "    epochs = 100 if num_rows < 50000 else 150\n",
        "    print(f\"üìä Dataset size: {num_rows} rows ‚Üí EPOCHS = {epochs}\")\n",
        "\n",
        "    return df_processed, numeric_cols, cat_cols, epochs, target_column,numeric_cols\n",
        "# Example usage:\n",
        "data_path = \"/content/drive/MyDrive/Katabatic/Data/Adult/adult-official.csv\"\n",
        "target = \"class\"\n",
        "interval_columns = []\n",
        "output_file = \"processed_data.csv\"\n",
        "#[\"age\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
        "\n",
        "# TARGET_COL        = \"class\"\n",
        "# TARGET_COL        = \"class\"\n",
        "df_processed, numeric_cols, cat_cols, EPOCHS,TARGET_COL,NUMERIC_COLS  = preprocess_dataset(\n",
        "    file_path=data_path,\n",
        "    target_column=target,\n",
        "    interval_cols=interval_columns,\n",
        "    output_path=output_file\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwAOE1LY73bT",
        "outputId": "4981dba5-8009-4d82-8cb1-e6de3eaee1df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Dataset size: 48842 rows ‚Üí EPOCHS = 100\n"
          ]
        }
      ]
    }
  ]
}