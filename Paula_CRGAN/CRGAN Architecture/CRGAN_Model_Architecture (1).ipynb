{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CRGAN Model Architecture**"
      ],
      "metadata": {
        "id": "IMbbMxl2zlNC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FHnY3M0yXEV"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "#Device configuration\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "# Generator definition\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim: int, output_dim: int):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            latent_dim: size of the input noise vector\n",
        "            output_dim: dimensionality of each generated sample\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, output_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        z: noise tensor of shape (batch_size, latent_dim)\n",
        "        returns: generated samples of shape (batch_size, output_dim)\n",
        "        \"\"\"\n",
        "        return self.net(z)\n",
        "\n",
        "\n",
        "\n",
        "#Discriminator definition\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim: int):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim: dimensionality of each sample (real or fake)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: tensor of shape (batch_size, input_dim)\n",
        "        returns: probability of realness, shape (batch_size, 1)\n",
        "        \"\"\"\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "#Training loop for Cramér GAN\n",
        "def train_cramer_gan(\n",
        "    G: Generator,\n",
        "    D: Discriminator,\n",
        "    loader: torch.utils.data.DataLoader,\n",
        "    *,\n",
        "    latent_dim: int = 100,\n",
        "    epochs: int = 100,\n",
        "    lr: float = 2e-4\n",
        ") -> Generator:\n",
        "    \"\"\"\n",
        "    Train the Generator G and Discriminator D adversarially.\n",
        "\n",
        "    Args:\n",
        "      G: Generator instance\n",
        "      D: Discriminator instance\n",
        "      loader: DataLoader yielding (real_batch, _)\n",
        "      latent_dim: dimension of noise vector\n",
        "      epochs: number of training epochs\n",
        "      lr: learning rate for both optimizers\n",
        "\n",
        "    Returns:\n",
        "      The trained Generator (and Discriminator, if you need it).\n",
        "    \"\"\"\n",
        "    G.to(device)\n",
        "    D.to(device)\n",
        "    opt_g = optim.Adam(G.parameters(), lr=lr)\n",
        "    opt_d = optim.Adam(D.parameters(), lr=lr)\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        for real_batch, _ in loader:\n",
        "            real_batch = real_batch.to(device).float()\n",
        "            bsz = real_batch.size(0)\n",
        "\n",
        "            # --- Discriminator step ---\n",
        "            opt_d.zero_grad()\n",
        "            # Real\n",
        "            d_real = D(real_batch)\n",
        "            # Fake\n",
        "            z = torch.randn(bsz, latent_dim, device=device)\n",
        "            fake = G(z).detach()\n",
        "            d_fake = D(fake)\n",
        "            loss_d = criterion(d_real, torch.ones_like(d_real)) + \\\n",
        "                     criterion(d_fake, torch.zeros_like(d_fake))\n",
        "            loss_d.backward()\n",
        "            opt_d.step()\n",
        "\n",
        "            # --- Generator step ---\n",
        "            opt_g.zero_grad()\n",
        "            z2 = torch.randn(bsz, latent_dim, device=device)\n",
        "            fake2 = G(z2)\n",
        "            d_fake2 = D(fake2)\n",
        "            loss_g = criterion(d_fake2, torch.ones_like(d_fake2))\n",
        "            loss_g.backward()\n",
        "            opt_g.step()\n",
        "\n",
        "        # optional logging\n",
        "        if ep == 1 or ep % 20 == 0 or ep == epochs:\n",
        "            print(f\"Epoch {ep}/{epochs}  D_loss={loss_d.item():.4f}  G_loss={loss_g.item():.4f}\")\n",
        "\n",
        "    return G\n",
        "\n",
        "\n",
        "\n",
        "#Synthetic data generation\n",
        "\n",
        "def generate_synthetic(\n",
        "    G: Generator,\n",
        "    n_samples: int,\n",
        "    *,\n",
        "    latent_dim: int = 100\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Sample new data points from a trained Generator.\n",
        "\n",
        "    Args:\n",
        "      G: trained Generator\n",
        "      n_samples: number of synthetic samples to generate\n",
        "      latent_dim: dimension of noise vector\n",
        "\n",
        "    Returns:\n",
        "      A tensor of shape (n_samples, output_dim) on CPU.\n",
        "    \"\"\"\n",
        "    G.to(device).eval()\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(n_samples, latent_dim, device=device)\n",
        "        synth = G(z).cpu()\n",
        "    return synth\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CRGAN Training Procedure**"
      ],
      "metadata": {
        "id": "Cd2BI0tY0CqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "from cr_gan import Generator, Discriminator, train_cramer_gan, generate_synthetic\n",
        "\n",
        "# Load your preprocessed DataFrame df (must include 'target' column)\n",
        "df = pd.read_csv(\"preprocessed_data.csv\")\n",
        "\n",
        "# Create DataLoader over real features (ignore target)\n",
        "X = df.drop(columns=\"target\").values.astype(\"float32\")\n",
        "loader = DataLoader(TensorDataset(torch.from_numpy(X), torch.zeros(len(X))),\n",
        "                    batch_size=64, shuffle=True)\n",
        "\n",
        "#Instantiate models\n",
        "G = Generator(latent_dim=100, output_dim=X.shape[1])\n",
        "D = Discriminator(input_dim=X.shape[1])\n",
        "\n",
        "# Train GAN\n",
        "G_trained = train_cramer_gan(G, D, loader, latent_dim=100, epochs=100)\n",
        "\n",
        "# Generate synthetic samples\n",
        "n_syn = int(0.5 * len(X))\n",
        "X_synth = generate_synthetic(G_trained, n_samples=n_syn, latent_dim=100).numpy()\n",
        "\n",
        "# Reattach target\n",
        "y_real = df[\"target\"].values\n",
        "y_synth = np.random.choice(y_real, size=n_syn, replace=True)\n",
        "synth_df = pd.DataFrame(X_synth, columns=df.columns[:-1])\n",
        "synth_df[\"target\"] = y_synth\n",
        "\n",
        "# 7) Save or pass to downstream code\n",
        "synth_df.to_csv(\"synthetic_data.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "4XZpvR080Vnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CRGAN Model Evaluation**"
      ],
      "metadata": {
        "id": "yRY1PBHR4GFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.linear_model    import LogisticRegression\n",
        "from sklearn.neural_network  import MLPClassifier\n",
        "from sklearn.ensemble        import RandomForestClassifier\n",
        "from xgboost                 import XGBClassifier\n",
        "from sklearn.metrics         import accuracy_score, roc_auc_score\n",
        "from scipy.stats             import entropy, wasserstein_distance\n",
        "\n",
        "\n",
        "#Helper Functions for JSD & Wasserstein Distance\n",
        "def compute_jsd(real: pd.DataFrame, synth: pd.DataFrame, bins=20) -> float:\n",
        "    \"\"\"Average Jensen–Shannon divergence over all columns.\"\"\"\n",
        "    jsd_vals = []\n",
        "    for col in real.columns:\n",
        "        r = real[col].to_numpy()\n",
        "        s = synth[col].to_numpy()\n",
        "        # build histograms\n",
        "        mn, mx = min(r.min(), s.min()), max(r.max(), s.max())\n",
        "        edges = np.linspace(mn, mx, bins+1)\n",
        "        p_r, _ = np.histogram(r, bins=edges, density=True)\n",
        "        p_s, _ = np.histogram(s, bins=edges, density=True)\n",
        "        # avoid zero‐bins\n",
        "        p_r += 1e-8; p_s += 1e-8\n",
        "        m = 0.5 * (p_r + p_s)\n",
        "        jsd_vals.append(0.5 * (entropy(p_r, m) + entropy(p_s, m)))\n",
        "    return float(np.mean(jsd_vals))\n",
        "\n",
        "def compute_wd(real: pd.DataFrame, synth: pd.DataFrame) -> float:\n",
        "    \"\"\"Average 1D Wasserstein (Earth‐Mover) distance over all columns.\"\"\"\n",
        "    wd_vals = []\n",
        "    for col in real.columns:\n",
        "        r = real[col].to_numpy()\n",
        "        s = synth[col].to_numpy()\n",
        "        wd_vals.append(wasserstein_distance(r, s))\n",
        "    return float(np.mean(wd_vals))\n",
        "\n",
        "\n",
        "#Load the real & synthetic datasets\n",
        "\n",
        "real_path  = \"adult_preprocessed.csv\"           # original preprocessed file\n",
        "synth_path = \"adult_tabpfn_synthetic.csv\"      # synthetic data generated\n",
        "\n",
        "df_real  = pd.read_csv(real_path)\n",
        "df_synth = pd.read_csv(synth_path)\n",
        "\n",
        "# Separate features & target\n",
        "X_real, y_real   = df_real.drop(\"target\", axis=1), df_real[\"target\"]\n",
        "X_synth, y_synth = df_synth.drop(\"target\", axis=1), df_synth[\"target\"]\n",
        "\n",
        "#Distributional fidelity: JSD & WD\n",
        "\n",
        "jsd_score = compute_jsd(X_real, X_synth)\n",
        "wd_score  = compute_wd(X_real, X_synth)\n",
        "\n",
        "# TSTR: train‐on‐synthetic, test‐on‐real with 4 classifiers\n",
        "\n",
        "classifiers = {\n",
        "    \"LR\":   LogisticRegression(max_iter=1000),\n",
        "    \"MLP\":  MLPClassifier(hidden_layer_sizes=(128,64), max_iter=500),\n",
        "    \"RF\":   RandomForestClassifier(n_estimators=200),\n",
        "    \"XGBT\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
        "}\n",
        "\n",
        "tstr_results = {}\n",
        "for name, clf in classifiers.items():\n",
        "    clf.fit(X_synth, y_synth)                         # train on synthetic\n",
        "    y_pred = clf.predict(X_real)                      # test on real\n",
        "    acc = accuracy_score(y_real, y_pred)\n",
        "    auc = roc_auc_score(y_real, clf.predict_proba(X_real)[:,1])\n",
        "    tstr_results[name] = {\"Accuracy\": acc, \"AUC\": auc}\n",
        "\n",
        "\n",
        "print(\"\\nDistributional Metrics:\")\n",
        "print(f\"  • Jensen–Shannon Divergence (JSD): {jsd_score:.4f}\")\n",
        "print(f\"  • Wasserstein Distance (WD):      {wd_score:.4f}\")\n",
        "\n",
        "print(\"\\nTSTR Results (train on synthetic → test on real):\")\n",
        "print(f\"{'Model':<6}  {'Accuracy':>8}   {'AUC':>6}\")\n",
        "print(\"-\" * 26)\n",
        "for name, metrics in tstr_results.items():\n",
        "    print(f\"{name:<6}  {metrics['Accuracy']*100:8.2f}%   {metrics['AUC']:6.3f}\")\n"
      ],
      "metadata": {
        "id": "QGz3RUlx4FYk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}