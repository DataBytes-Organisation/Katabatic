{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q5McNOdYtPU",
        "outputId": "6fe54c86-bd11-4c2e-c765-1688b753a44e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas scipy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available(), torch.cuda.get_device_name(0)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMDrabE0ZXlC",
        "outputId": "84ef79aa-fbab-47c5-ad5c-eb87ac8eec72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, 'Tesla T4')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXDMCUIfZdaJ",
        "outputId": "5e6e1061-bfd4-4d6a-80c9-a39b7b898209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1jnHZJwnk4Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVVA2LN0lDAf",
        "outputId": "057a0473-2430-48a0-f6c0-79bddc495d7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "\n",
        "def dataset_sanity_check(\n",
        "    df: pd.DataFrame,\n",
        "    target_col: str,\n",
        "    interval_cols: Optional[List[str]] = None,\n",
        "    max_unique_for_cat: int = 50\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Prints a summary of df:\n",
        "     - shape, dtype counts\n",
        "     - missing values per column\n",
        "     - target distribution\n",
        "     - suggested interval vs categorical splits\n",
        "    Args:\n",
        "      df: raw DataFrame\n",
        "      target_col: name of the target column\n",
        "      interval_cols: optional list of numeric feature names;\n",
        "                     if None, they'll be inferred by dtype and unique count\n",
        "      max_unique_for_cat: if dtype==object but unique<=this, treat as cat\n",
        "    \"\"\"\n",
        "    print(\"‚îÄ‚îÄ‚îÄ DATASET SANITY CHECK ‚îÄ‚îÄ‚îÄ\")\n",
        "    print(f\"Shape: {df.shape[0]} rows √ó {df.shape[1]} cols\")\n",
        "    print(\"\\nColumn dtypes:\")\n",
        "    print(df.dtypes.value_counts().to_string(), \"\\n\")\n",
        "\n",
        "    # Missing\n",
        "    missing = df.isna().sum()\n",
        "    if missing.any():\n",
        "        print(\"Missing values:\")\n",
        "        print(missing[missing>0].sort_values(), \"\\n\")\n",
        "    else:\n",
        "        print(\"No missing values.\\n\")\n",
        "\n",
        "    # Target distribution\n",
        "    if target_col not in df.columns:\n",
        "        raise ValueError(f\"Target column '{target_col}' not found in DataFrame!\")\n",
        "    print(f\"Target distribution ({target_col}):\")\n",
        "    print(df[target_col].value_counts(normalize=True).mul(100).round(2).astype(str) + \"%\\n\")\n",
        "\n",
        "    # Feature type suggestions\n",
        "    if interval_cols is None:\n",
        "        # infer numeric by dtype\n",
        "        num = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        # treat low‚Äêcard object cols as categorical too\n",
        "        obj = [\n",
        "            c for c in df.select_dtypes(include=[\"object\"]).columns\n",
        "            if df[c].nunique() <= max_unique_for_cat and c != target_col\n",
        "        ]\n",
        "        interval_cols = num\n",
        "        cat_cols = [c for c in df.columns if c not in interval_cols + [target_col]]\n",
        "    else:\n",
        "        # user‚Äêprovided\n",
        "        cat_cols = [c for c in df.columns if c not in interval_cols + [target_col]]\n",
        "\n",
        "    print(f\"Suggested numeric (interval) cols ({len(interval_cols)}): {interval_cols}\")\n",
        "    print(f\"Suggested categorical cols ({len(cat_cols)}): {cat_cols}\\n\")\n",
        "\n",
        "    # Warn about very small or very large datasets\n",
        "    if df.shape[0] < 100:\n",
        "        print(\"  Warning: fewer than 100 samples‚ÄîGANs may overfit or collapse.\")\n",
        "    elif df.shape[0] > 200_000:\n",
        "        print(\"  Warning: very large dataset‚Äîtraining may be slow.\")\n",
        "\n",
        "    print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\\n\")\n",
        "\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ Example usage ‚îÄ‚îÄ‚îÄ\n",
        "if __name__ == \"__main__\":\n",
        "    # Load any dataset\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/Katabatic/Data/Adult/adult-official.csv\")\n",
        "\n",
        "    # Run the check\n",
        "    dataset_sanity_check(\n",
        "      df,\n",
        "      target_col=\"class\",\n",
        "      # you can also explicitly tell it which interval cols to use:\n",
        "      interval_cols=[\"age\",\"education-num\",\"capital-gain\",\"capital-loss\",\"hours-per-week\"]\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8cG-RPJlPE-",
        "outputId": "83d761f8-55e7-42ce-fd48-ac0ac099dee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚îÄ‚îÄ‚îÄ DATASET SANITY CHECK ‚îÄ‚îÄ‚îÄ\n",
            "Shape: 48842 rows √ó 15 cols\n",
            "\n",
            "Column dtypes:\n",
            "object    15 \n",
            "\n",
            "No missing values.\n",
            "\n",
            "Target distribution (class):\n",
            "class\n",
            "<=50K    76.07%\\n\n",
            ">50K     23.93%\\n\n",
            "Name: proportion, dtype: object\n",
            "Suggested numeric (interval) cols (5): ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
            "Suggested categorical cols (9): ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import os\n",
        "\n",
        "def interval_to_mid(x):\n",
        "    \"\"\"Convert interval strings to midpoints, e.g., '20-30' => 25.0\"\"\"\n",
        "    if pd.isna(x): return x\n",
        "    parts = re.findall(r'-?[\\d\\.]+|inf', str(x))\n",
        "    if len(parts) == 2:\n",
        "        lo, hi = parts\n",
        "        lo = float(lo) if lo not in (\"-inf\", \"inf\") else float(hi)\n",
        "        hi = float(hi) if hi not in (\"-inf\", \"inf\") else float(lo)\n",
        "        return (lo + hi) / 2\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return pd.NA\n",
        "\n",
        "def preprocess_dataset(file_path, target_column, interval_cols=None, output_path=\"processed_data.csv\"):\n",
        "    # Load dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "\n",
        "    # Auto-detect interval columns if not provided\n",
        "    if interval_cols is None:\n",
        "        interval_cols = df.select_dtypes(include=['object']).columns[\n",
        "            df.select_dtypes(include=['object']).apply(\n",
        "                lambda col: col.astype(str).str.contains(r'\\d+\\s*-\\s*\\d+').any()\n",
        "            )\n",
        "        ].tolist()\n",
        "\n",
        "    # Convert intervals to midpoints\n",
        "    for col in interval_cols:\n",
        "        df[col] = df[col].apply(interval_to_mid)\n",
        "\n",
        "    # Separate types\n",
        "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    if target_column in numeric_cols:\n",
        "        numeric_cols.remove(target_column)\n",
        "\n",
        "    # Encode target\n",
        "    le = LabelEncoder()\n",
        "    df[target_column] = le.fit_transform(df[target_column])\n",
        "\n",
        "    # Scale numeric columns if they exist\n",
        "    if numeric_cols:\n",
        "        scaler = StandardScaler()\n",
        "        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "        df_numeric = df[numeric_cols].reset_index(drop=True)\n",
        "    else:\n",
        "        df_numeric = pd.DataFrame()\n",
        "\n",
        "    # Identify categorical columns\n",
        "    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    # One-hot encode categorical columns if they exist\n",
        "    if cat_cols:\n",
        "        df_cat = pd.get_dummies(df[cat_cols], drop_first=False).astype(int).reset_index(drop=True)\n",
        "    else:\n",
        "        df_cat = pd.DataFrame()\n",
        "\n",
        "    # Target column\n",
        "    df_target = df[[target_column]].reset_index(drop=True)\n",
        "\n",
        "    # Final dataset\n",
        "    df_processed = pd.concat([df_numeric, df_cat, df_target], axis=1)\n",
        "\n",
        "    # Hyperparameter adjustment\n",
        "    num_rows = df_processed.shape[0]\n",
        "    epochs = 100 if num_rows < 50000 else 150\n",
        "    print(f\"üìä Dataset size: {num_rows} rows ‚Üí EPOCHS = {epochs}\")\n",
        "\n",
        "    return df_processed, numeric_cols, cat_cols, epochs, target_column,numeric_cols\n",
        "# Example usage:\n",
        "data_path = \"/content/drive/MyDrive/Katabatic/Data/Adult/adult-official.csv\"\n",
        "target = \"class\"\n",
        "interval_columns = []\n",
        "output_file = \"processed_data.csv\"\n",
        "#[\"age\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
        "\n",
        "# TARGET_COL        = \"class\"\n",
        "# TARGET_COL        = \"class\"\n",
        "df_processed, numeric_cols, cat_cols, EPOCHS,TARGET_COL,NUMERIC_COLS  = preprocess_dataset(\n",
        "    file_path=data_path,\n",
        "    target_column=target,\n",
        "    interval_cols=interval_columns,\n",
        "    output_path=output_file\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsidQC-xlT-4",
        "outputId": "deaeb290-fd64-4db8-d71c-975d5260a42f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Dataset size: 48842 rows ‚Üí EPOCHS = 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Katabatic/Data/Adult/adult-official.csv\")\n",
        "\n",
        "#Interval ‚Üí midpoint\n",
        "interval_cols = ['age','education-num','capital-gain','capital-loss','hours-per-week']\n",
        "def interval_to_mid(x):\n",
        "    if pd.isna(x): return x\n",
        "    parts = re.findall(r'-?[\\d\\.]+|inf', str(x))\n",
        "    if len(parts)==2:\n",
        "        lo, hi = parts\n",
        "        lo = float(lo) if lo not in (\"-inf\",\"inf\") else float(hi)\n",
        "        hi = float(hi) if hi not in (\"-inf\",\"inf\") else float(lo)\n",
        "        return (lo+hi)/2\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return pd.NA\n",
        "\n",
        "for c in interval_cols:\n",
        "    df[c] = df[c].apply(interval_to_mid)\n",
        "\n",
        "#Encode target\n",
        "le = LabelEncoder()\n",
        "df['class'] = le.fit_transform(df['class'])\n",
        "\n",
        "#scale numeric features\n",
        "scaler = StandardScaler()\n",
        "df[interval_cols] = scaler.fit_transform(df[interval_cols])\n",
        "\n",
        "#all interval_cols are floats, class is 0/1\n",
        "print(df[interval_cols].dtypes)\n",
        "print(df['class'].value_counts())\n",
        "print(df[interval_cols].head())\n",
        "print(df['class'].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyEWa4TeZm_J",
        "outputId": "a8339f06-bbdf-40fb-cc11-928d56013172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "age               float64\n",
            "education-num     float64\n",
            "capital-gain      float64\n",
            "capital-loss      float64\n",
            "hours-per-week    float64\n",
            "dtype: object\n",
            "class\n",
            "0    37155\n",
            "1    11687\n",
            "Name: count, dtype: int64\n",
            "        age  education-num  capital-gain  capital-loss  hours-per-week\n",
            "0  0.024301       0.169881     -1.185489     -0.206255        0.274596\n",
            "1 -0.242654       0.169881     -0.189421     -0.206255       -1.632827\n",
            "2  0.024301       0.169881     -0.189421     -0.206255        0.274596\n",
            "3 -0.242654      -1.925737     -0.189421     -0.206255        0.274596\n",
            "4  0.138711       0.169881     -0.189421     -0.206255        0.274596\n",
            "0    0\n",
            "1    0\n",
            "2    0\n",
            "3    0\n",
            "4    0\n",
            "Name: class, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#One-hot encode the remaining categorical columns\n",
        "cat_cols = ['workclass','education','marital-status',\n",
        "            'occupation','relationship','race','sex',\n",
        "            'native-country']\n",
        "\n",
        "# Expand into dummy columns these will be bools\n",
        "df_cat = pd.get_dummies(df[cat_cols], drop_first=False)\n",
        "\n",
        "#  Cast to 0 / 1 so downstream code are purely numeric\n",
        "df_cat = df_cat.astype(int)\n",
        "\n",
        "#Keep our numeric and target\n",
        "df_numeric = df[interval_cols]               # five float columns\n",
        "df_target  = df[['class']]                   # 0/1 target\n",
        "\n",
        "#Record group sizes so GAN knows how to split its outputs\n",
        "cat_group_sizes = [\n",
        "    len(pd.get_dummies(df[col], drop_first=False).columns)\n",
        "    for col in cat_cols\n",
        "]\n",
        "\n",
        "# Build the final feature matrix for modeling\n",
        "X = np.hstack([df_numeric.values, df_cat.values])\n",
        "y = df_target.values.ravel()\n",
        "\n",
        "df_numeric = df_numeric.reset_index(drop=True)\n",
        "df_cat     = df_cat.reset_index(drop=True)\n",
        "df_target  = df_target.reset_index(drop=True)\n",
        "\n",
        "df_processed = pd.concat([df_numeric, df_cat, df_target], axis=1)\n",
        "out_path = \"/content/drive/MyDrive/Katabatic/Data/Adult/preprocessed4_adult.csv\"\n",
        "df_processed.to_csv(out_path, index=False)\n",
        "print(\"Preprocessed data saved to:\", out_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMnXfb1JZtbW",
        "outputId": "847ec9ce-37a7-4589-ff7a-733a37f88e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed data saved to: /content/drive/MyDrive/Katabatic/Data/Adult/preprocessed4_adult.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installs & Imports\n",
        "!pip install torch torchvision scipy scikit-learn xgboost --quiet\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.linear_model    import LogisticRegression\n",
        "from sklearn.neural_network  import MLPClassifier\n",
        "from sklearn.ensemble        import RandomForestClassifier\n",
        "from xgboost                 import XGBClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from scipy.stats             import wasserstein_distance\n",
        "from scipy.spatial.distance  import jensenshannon\n",
        "\n",
        "# Hyperparameters\n",
        "PREPROCESSED_PATH = \"/content/drive/MyDrive/Katabatic/Data/Adult/preprocessed4_adult.csv\"\n",
        "LATENT_DIM        = 100\n",
        "BATCH_SIZE        = 64\n",
        "EPOCHS            = 100    # medium dataset\n",
        "REPEATS           = 3\n",
        "FOLDS             = 2\n",
        "SYN_RATIO         = 0.5    # synthetic / real train size\n",
        "NUMERIC_COLS      = [\"age\",\"education-num\",\"capital-gain\",\"capital-loss\",\"hours-per-week\"]\n",
        "TARGET_COL        = \"class\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# CR-GAN Definitions\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(z_dim,256), nn.ReLU(),\n",
        "            nn.Linear(256,512),   nn.ReLU(),\n",
        "            nn.Linear(512,256),   nn.ReLU(),\n",
        "            nn.Linear(256,out_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    def forward(self, z):\n",
        "        return self.net(z)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim,512), nn.ReLU(),\n",
        "            nn.Linear(512,256),   nn.ReLU(),\n",
        "            nn.Linear(256,128),   nn.ReLU(),\n",
        "            nn.Linear(128,1),     nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def train_cramer_gan(G, D, loader, epochs):\n",
        "    G, D = G.to(device), D.to(device)\n",
        "    optg = optim.Adam(G.parameters(), lr=2e-4)\n",
        "    optd = optim.Adam(D.parameters(), lr=2e-4)\n",
        "    loss_fn = nn.BCELoss()\n",
        "    for ep in range(1, epochs+1):\n",
        "        for real_batch, _ in loader:\n",
        "            real_batch = real_batch.to(device)\n",
        "            bsz = real_batch.size(0)\n",
        "            # ‚Äî D step\n",
        "            optd.zero_grad()\n",
        "            z      = torch.randn(bsz, LATENT_DIM, device=device)\n",
        "            fake   = G(z).detach()\n",
        "            d_real = D(real_batch)\n",
        "            d_fake = D(fake)\n",
        "            lossd  = loss_fn(d_real, torch.ones_like(d_real)) + \\\n",
        "                     loss_fn(d_fake, torch.zeros_like(d_fake))\n",
        "            lossd.backward();  optd.step()\n",
        "            # ‚Äî G step\n",
        "            optg.zero_grad()\n",
        "            z     = torch.randn(bsz, LATENT_DIM, device=device)\n",
        "            fake2 = G(z)\n",
        "            dg    = D(fake2)\n",
        "            lossg = loss_fn(dg, torch.ones_like(dg))\n",
        "            lossg.backward(); optg.step()\n",
        "        if ep%20==0 or ep==1 or ep==epochs:\n",
        "            print(f\" Ep {ep}/{epochs}  D_loss={lossd.item():.4f}  G_loss={lossg.item():.4f}\")\n",
        "    return G, D\n",
        "\n",
        "def generate_synthetic(G, n_samples):\n",
        "    G = G.to(device).eval()\n",
        "    with torch.no_grad():\n",
        "        z    = torch.randn(n_samples, LATENT_DIM, device=device)\n",
        "        data = G(z).cpu().numpy()\n",
        "    return data\n",
        "\n",
        "def compute_tstr_all(X_real, y_real, X_syn, y_syn):\n",
        "    # Train each classifier on synthetic ‚Üí score on real\n",
        "    results = {}\n",
        "    for name, clf in [\n",
        "        (\"LR\",  LogisticRegression(max_iter=5000)),\n",
        "        (\"MLP\", MLPClassifier(hidden_layer_sizes=(128,64), max_iter=1000)),\n",
        "        (\"RF\",  RandomForestClassifier(n_estimators=200)),\n",
        "        (\"XGB\", XGBClassifier(eval_metric=\"logloss\"))\n",
        "    ]:\n",
        "        clf.fit(X_syn, y_syn)\n",
        "        results[name] = clf.score(X_real, y_real)*100.0\n",
        "    return results\n",
        "\n",
        "def compute_jsd_wd(X_real, X_syn, num_idx):\n",
        "    jsd_list, wd_list = [], []\n",
        "    for i in num_idx:\n",
        "        p_real, _  = np.histogram(X_real[:,i], bins=50, density=True)\n",
        "        p_syn,  _  = np.histogram(X_syn[:,i], bins=50, density=True)\n",
        "        jsd_list.append( jensenshannon(p_real, p_syn) )\n",
        "        wd_list .append( wasserstein_distance(X_real[:,i], X_syn[:,i]) )\n",
        "    return np.mean(jsd_list), np.mean(wd_list)\n",
        "\n",
        "# Load Preprocessed Data\n",
        "df = pd.read_csv(PREPROCESSED_PATH)\n",
        "X_full = df.drop(columns=[TARGET_COL]).values.astype(np.float32)\n",
        "y_full = df[TARGET_COL].values.astype(int)\n",
        "num_idx = [df.columns.get_loc(c) for c in NUMERIC_COLS]\n",
        "\n",
        "# 3√ó(2-Fold CV)\n",
        "tstr_scores = {m:[] for m in [\"LR\",\"MLP\",\"RF\",\"XGB\"]}\n",
        "jsd_scores, wd_scores = [], []\n",
        "\n",
        "kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
        "for rep in range(1, REPEATS+1):\n",
        "    for fold,(train_idx, test_idx) in enumerate(kf.split(X_full),1):\n",
        "        print(f\"\\n‚Äî Rep {rep}/{REPEATS}  Fold {fold}/{FOLDS} ‚Äî\")\n",
        "        X_tr, X_te = X_full[train_idx], X_full[test_idx]\n",
        "        y_tr, y_te = y_full[train_idx], y_full[test_idx]\n",
        "        loader = DataLoader(\n",
        "            TensorDataset(torch.from_numpy(X_tr), torch.from_numpy(y_tr)),\n",
        "            batch_size=BATCH_SIZE, shuffle=True\n",
        "        )\n",
        "        # train\n",
        "        G = Generator(LATENT_DIM, X_tr.shape[1])\n",
        "        D = Discriminator(X_tr.shape[1])\n",
        "        G, D = train_cramer_gan(G, D, loader, epochs=EPOCHS)\n",
        "        # synth\n",
        "        n_syn = int(SYN_RATIO * len(X_tr))\n",
        "        X_syn = generate_synthetic(G, n_syn)\n",
        "        y_syn = np.random.choice(y_tr, size=n_syn, replace=True)\n",
        "        # metrics\n",
        "        tstrs = compute_tstr_all(X_te, y_te, X_syn, y_syn)\n",
        "        for m,sc in tstrs.items(): tstr_scores[m].append(sc)\n",
        "        js, wd = compute_jsd_wd(X_te, X_syn, num_idx)\n",
        "        jsd_scores.append(js);  wd_scores.append(wd)\n",
        "\n",
        "# Report CV Results\n",
        "print(\"\\n=== CV Results (mean ¬± std) ===\")\n",
        "for m in [\"LR\",\"MLP\",\"RF\",\"XGB\"]:\n",
        "    arr = np.array(tstr_scores[m])\n",
        "    print(f\" ‚Ä¢ {m:4s} TSTR = {arr.mean():.2f}% ¬± {arr.std():.2f}%\")\n",
        "print(f\" ‚Ä¢ JSD = {np.mean(jsd_scores):.4f} ¬± {np.std(jsd_scores):.4f}\")\n",
        "print(f\" ‚Ä¢ WD  = {np.mean(wd_scores):.4f} ¬± {np.std(wd_scores):.4f}\")\n",
        "\n",
        "# Train on Full & Save Final Synthetic\n",
        "# retrain on all data\n",
        "full_loader = DataLoader(\n",
        "    TensorDataset(torch.from_numpy(X_full), torch.from_numpy(y_full)),\n",
        "    batch_size=BATCH_SIZE, shuffle=True\n",
        ")\n",
        "Gf = Generator(LATENT_DIM, X_full.shape[1])\n",
        "Df = Discriminator(X_full.shape[1])\n",
        "Gf, Df = train_cramer_gan(Gf, Df, full_loader, epochs=EPOCHS)\n",
        "\n",
        "# generate 50% synthetic\n",
        "n_final = int(SYN_RATIO * len(X_full))\n",
        "Xf_syn = generate_synthetic(Gf, n_final)\n",
        "yf_syn = np.random.choice(y_full, size=n_final, replace=True)\n",
        "\n",
        "\n",
        "cols = df.columns[:-1]\n",
        "syn_df = pd.DataFrame(Xf_syn, columns=cols)\n",
        "syn_df[TARGET_COL] = yf_syn\n",
        "out_path = \"/content/drive/MyDrive/Katabatic/Data/Adult/synthetic4_adult_final.csv\"\n",
        "syn_df.to_csv(out_path, index=False)\n",
        "print(f\"\\nSaved final synthetic dataset ({n_final} rows) to:\\n  {out_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWfz7hYAZzW8",
        "outputId": "8b17ac83-f7ac-49d2-85b6-beab765c9396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing device: cuda\n",
            "\n",
            "‚Äî Rep 1/3  Fold 1/2 ‚Äî\n",
            " Ep 1/100  D_loss=0.1596  G_loss=3.2518\n",
            " Ep 20/100  D_loss=0.5681  G_loss=4.9457\n",
            " Ep 40/100  D_loss=0.0166  G_loss=4.1547\n",
            " Ep 60/100  D_loss=0.0047  G_loss=6.6931\n",
            " Ep 80/100  D_loss=0.0002  G_loss=9.0192\n",
            " Ep 100/100  D_loss=0.0017  G_loss=6.3655\n",
            "\n",
            "‚Äî Rep 1/3  Fold 2/2 ‚Äî\n",
            " Ep 1/100  D_loss=1.2078  G_loss=2.4916\n",
            " Ep 20/100  D_loss=0.0528  G_loss=3.3392\n",
            " Ep 40/100  D_loss=0.0191  G_loss=6.5382\n",
            " Ep 60/100  D_loss=0.0046  G_loss=6.8651\n",
            " Ep 80/100  D_loss=0.0011  G_loss=6.9110\n",
            " Ep 100/100  D_loss=0.0001  G_loss=10.2168\n",
            "\n",
            "‚Äî Rep 2/3  Fold 1/2 ‚Äî\n",
            " Ep 1/100  D_loss=0.5602  G_loss=3.3097\n",
            " Ep 20/100  D_loss=0.2377  G_loss=4.5787\n",
            " Ep 40/100  D_loss=0.1952  G_loss=5.7681\n",
            " Ep 60/100  D_loss=0.0017  G_loss=6.5880\n",
            " Ep 80/100  D_loss=0.0001  G_loss=9.5342\n",
            " Ep 100/100  D_loss=0.2141  G_loss=5.9897\n",
            "\n",
            "‚Äî Rep 2/3  Fold 2/2 ‚Äî\n",
            " Ep 1/100  D_loss=0.6909  G_loss=2.6459\n",
            " Ep 20/100  D_loss=0.5083  G_loss=4.3086\n",
            " Ep 40/100  D_loss=0.0033  G_loss=5.7765\n",
            " Ep 60/100  D_loss=0.0008  G_loss=7.1197\n",
            " Ep 80/100  D_loss=0.0025  G_loss=7.5396\n",
            " Ep 100/100  D_loss=0.0001  G_loss=9.1010\n",
            "\n",
            "‚Äî Rep 3/3  Fold 1/2 ‚Äî\n",
            " Ep 1/100  D_loss=0.8419  G_loss=2.3351\n",
            " Ep 20/100  D_loss=0.1355  G_loss=3.5229\n",
            " Ep 40/100  D_loss=0.0106  G_loss=5.4312\n",
            " Ep 60/100  D_loss=0.0003  G_loss=9.0636\n",
            " Ep 80/100  D_loss=0.0004  G_loss=7.9589\n",
            " Ep 100/100  D_loss=0.0007  G_loss=8.7764\n",
            "\n",
            "‚Äî Rep 3/3  Fold 2/2 ‚Äî\n",
            " Ep 1/100  D_loss=0.3122  G_loss=2.7396\n",
            " Ep 20/100  D_loss=0.0146  G_loss=4.2542\n",
            " Ep 40/100  D_loss=0.0238  G_loss=5.2142\n",
            " Ep 60/100  D_loss=0.0026  G_loss=8.7214\n",
            " Ep 80/100  D_loss=0.0028  G_loss=9.9117\n",
            " Ep 100/100  D_loss=0.0001  G_loss=10.3224\n",
            "\n",
            "=== CV Results (mean ¬± std) ===\n",
            " ‚Ä¢ LR   TSTR = 73.16% ¬± 4.33%\n",
            " ‚Ä¢ MLP  TSTR = 76.07% ¬± 0.11%\n",
            " ‚Ä¢ RF   TSTR = 24.18% ¬± 0.61%\n",
            " ‚Ä¢ XGB  TSTR = 65.44% ¬± 10.90%\n",
            " ‚Ä¢ JSD = 0.7866 ¬± 0.0197\n",
            " ‚Ä¢ WD  = 0.4111 ¬± 0.0667\n",
            " Ep 1/100  D_loss=0.3321  G_loss=3.7209\n",
            " Ep 20/100  D_loss=0.5612  G_loss=6.6578\n",
            " Ep 40/100  D_loss=0.0004  G_loss=8.2200\n",
            " Ep 60/100  D_loss=0.0032  G_loss=5.7825\n",
            " Ep 80/100  D_loss=0.0014  G_loss=6.9951\n",
            " Ep 100/100  D_loss=0.0022  G_loss=6.3560\n",
            "\n",
            "Saved final synthetic dataset (24421 rows) to:\n",
            "  /content/drive/MyDrive/Katabatic/Data/Adult/synthetic4_adult_final.csv\n"
          ]
        }
      ]
    }
  ]
}