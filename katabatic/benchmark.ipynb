{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from ganblr import GANBLR  # Assuming GANBLR is saved in a module named ganblr\n",
    "\n",
    "# Utility function to compute descriptive statistics comparison\n",
    "def compute_statistics(real_data, synthetic_data):\n",
    "    real_mean = np.mean(real_data, axis=0)\n",
    "    real_var = np.var(real_data, axis=0)\n",
    "    synthetic_mean = np.mean(synthetic_data, axis=0)\n",
    "    synthetic_var = np.var(synthetic_data, axis=0)\n",
    "    \n",
    "    mean_diff = np.abs(real_mean - synthetic_mean)\n",
    "    var_diff = np.abs(real_var - synthetic_var)\n",
    "    \n",
    "    print(\"\\nMean Differences between Real and Synthetic Data:\")\n",
    "    print(mean_diff)\n",
    "    print(\"\\nVariance Differences between Real and Synthetic Data:\")\n",
    "    print(var_diff)\n",
    "    \n",
    "    return mean_diff, var_diff\n",
    "\n",
    "# Utility function to compute distribution similarity using Kolmogorov-Smirnov test\n",
    "def compute_distribution_similarity(real_data, synthetic_data):\n",
    "    ks_results = [ks_2samp(real_data[:, i], synthetic_data[:, i]) for i in range(real_data.shape[1])]\n",
    "    ks_statistic = np.array([result.statistic for result in ks_results])\n",
    "    ks_pvalue = np.array([result.pvalue for result in ks_results])\n",
    "    \n",
    "    print(\"\\nKolmogorov-Smirnov Test Results:\")\n",
    "    print(\"KS Statistic:\", ks_statistic)\n",
    "    print(\"P-Value:\", ks_pvalue)\n",
    "    \n",
    "    return ks_statistic, ks_pvalue\n",
    "\n",
    "# Utility function to compute Jensen-Shannon divergence\n",
    "def compute_js_divergence(real_data, synthetic_data):\n",
    "    js_divergence = []\n",
    "    for i in range(real_data.shape[1]):\n",
    "        real_dist = np.histogram(real_data[:, i], bins=20, density=True)[0]\n",
    "        synthetic_dist = np.histogram(synthetic_data[:, i], bins=20, density=True)[0]\n",
    "        jsd = jensenshannon(real_dist, synthetic_dist)\n",
    "        js_divergence.append(jsd)\n",
    "    \n",
    "    js_divergence = np.array(js_divergence)\n",
    "    \n",
    "    print(\"\\nJensen-Shannon Divergence between Real and Synthetic Data:\")\n",
    "    print(js_divergence)\n",
    "    \n",
    "    return js_divergence\n",
    "\n",
    "# Privacy evaluation: Membership Inference Attack\n",
    "def membership_inference_attack(real_data, synthetic_data, model, test_size=0.2):\n",
    "    real_data = shuffle(real_data, random_state=42)\n",
    "    synthetic_data = shuffle(synthetic_data, random_state=42)\n",
    "\n",
    "    # Train a binary classifier to distinguish between real and synthetic data\n",
    "    labels = np.concatenate([np.ones(len(real_data)), np.zeros(len(synthetic_data))])\n",
    "    combined_data = np.vstack([real_data, synthetic_data])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(combined_data, labels, test_size=test_size, random_state=42)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    auc = roc_auc_score(y_test, predictions)\n",
    "    \n",
    "    print(\"\\nMembership Inference Attack Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    \n",
    "    return accuracy, auc\n",
    "\n",
    "# Load Dataset (e.g., UCI Adult Dataset)\n",
    "from sklearn.datasets import fetch_openml\n",
    "dataset = fetch_openml(name='adult', version=2)\n",
    "X, y = dataset.data, dataset.target\n",
    "\n",
    "# Encode categorical data\n",
    "ordinal_encoder = OrdinalEncoder(dtype=int, handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "X = ordinal_encoder.fit_transform(X)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y).astype(int)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize GANBLR model and fit on training data\n",
    "ganblr = GANBLR()\n",
    "ganblr.fit(X_train, y_train, k=2, batch_size=32, epochs=50, warmup_epochs=10, verbose=1)\n",
    "\n",
    "# Generate synthetic data\n",
    "synthetic_data = ganblr.sample(size=len(X_train))\n",
    "synthetic_X, synthetic_y = synthetic_data[:, :-1], synthetic_data[:, -1]\n",
    "\n",
    "# Data Quality Evaluation\n",
    "mean_diff, var_diff = compute_statistics(X_train, synthetic_X)\n",
    "ks_statistic, ks_pvalue = compute_distribution_similarity(X_train, synthetic_X)\n",
    "js_divergence = compute_js_divergence(X_train, synthetic_X)\n",
    "\n",
    "# Privacy Evaluation: Membership Inference Attack\n",
    "logistic_model = LogisticRegression()\n",
    "mia_accuracy, mia_auc = membership_inference_attack(X_train, synthetic_X, logistic_model)\n",
    "\n",
    "# Plotting results for better visualization\n",
    "def plot_results(mean_diff, var_diff, ks_statistic, js_divergence):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    axs[0, 0].bar(range(len(mean_diff)), mean_diff)\n",
    "    axs[0, 0].set_title('Mean Differences')\n",
    "    \n",
    "    axs[0, 1].bar(range(len(var_diff)), var_diff)\n",
    "    axs[0, 1].set_title('Variance Differences')\n",
    "    \n",
    "    axs[1, 0].bar(range(len(ks_statistic)), ks_statistic)\n",
    "    axs[1, 0].set_title('KS Statistic')\n",
    "    \n",
    "    axs[1, 1].bar(range(len(js_divergence)), js_divergence)\n",
    "    axs[1, 1].set_title('Jensen-Shannon Divergence')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_results(mean_diff, var_diff, ks_statistic, js_divergence)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
