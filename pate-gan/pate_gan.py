import tensorflow as tf
import numpy as np
import warnings
warnings.filterwarnings("ignore")

from sklearn.linear_model import LogisticRegression

###############################################################################
# PATE-lambda (teacher voting with noise)
###############################################################################
def pate_lambda(x, teacher_models, lamda):
    """Returns PATE_lambda(x).
    
    Args:
        x: Single sample (1D NumPy array).
        teacher_models: List of trained teacher models.
        lamda: Noise scale for Laplace mechanism.
    
    Returns:
        n0, n1: the number of teacher votes for class 0 and class 1, respectively.
        out: final (noisy) label.
    """
    votes = []
    # Ask each teacher model for a prediction
    for teacher in teacher_models:
        temp_y = teacher.predict(x.reshape(1, -1))  # logistic predict expects 2D
        votes.append(temp_y[0])
    votes = np.array(votes)
    
    n0 = np.sum(votes == 0)
    n1 = np.sum(votes == 1)
    
    lap_noise = np.random.laplace(loc=0.0, scale=lamda)
    noisy_ratio = (n1 + lap_noise) / float(n0 + n1)
    out = int(noisy_ratio > 0.5)
    return n0, n1, out

###############################################################################
# Generator Model (Keras)
###############################################################################
class Generator(tf.keras.Model):
    def __init__(self, z_dim, generator_h_dim, x_dim):
        super(Generator, self).__init__()
        self.dense1 = tf.keras.layers.Dense(generator_h_dim, activation='tanh')
        self.dense2 = tf.keras.layers.Dense(generator_h_dim, activation='tanh')
        self.dense3 = tf.keras.layers.Dense(x_dim, activation='sigmoid')
        
    def call(self, z):
        x = self.dense1(z)
        x = self.dense2(x)
        x = self.dense3(x)
        return x

###############################################################################
# Student Model (Keras)
###############################################################################
class Student(tf.keras.Model):
    def __init__(self, x_dim, student_h_dim):
        super(Student, self).__init__()
        self.dense1 = tf.keras.layers.Dense(student_h_dim, activation='relu')
        self.dense2 = tf.keras.layers.Dense(1)  # no activation; raw logits
        
    def call(self, x):
        h = self.dense1(x)
        out = self.dense2(h)
        return out

###############################################################################
# Sample random Z from uniform distribution
###############################################################################
def sample_Z(m, n):
    return np.random.uniform(-1., 1., size=[m, n])

###############################################################################
# Main PATE-GAN Function
###############################################################################
def pategan(x_train, parameters):
    """
    Basic PATE-GAN framework, refactored for TensorFlow 2.x.

    Args:
      x_train: Training data (NumPy array, shape [n, dim]).
      parameters (dict): PATE-GAN parameters
          - n_s (int): number of student training iterations per round
          - batch_size (int): batch size for student and generator updates
          - k (int): number of teachers
          - epsilon (float): target epsilon for (epsilon, delta)-DP
          - delta (float): target delta for (epsilon, delta)-DP
          - lamda (float): Laplace noise scale
          
    Returns:
      x_train_hat (NumPy array): DP-synthetic data generated by the final generator.
    """
    # Unpack parameters
    n_s = parameters['n_s']
    batch_size = parameters['batch_size']
    k = parameters['k']
    epsilon = parameters['epsilon']
    delta = parameters['delta']
    lamda = parameters['lamda']
    
    # Data shape
    n, dim = x_train.shape
    
    # Dimensions
    z_dim = dim
    student_h_dim = dim
    generator_h_dim = 4 * dim
    
    # Partition data for teacher training
    partition_data_no = n // k
    shuffled_idx = np.random.permutation(n)
    x_partitions = []
    k = int(k)
    partition_data_no = int(partition_data_no)
    for i in range(k):
        part_idx = shuffled_idx[i * partition_data_no:(i + 1) * partition_data_no]
        x_partitions.append(x_train[part_idx])
        
    # Prepare the student & generator models
    student = Student(x_dim=dim, student_h_dim=student_h_dim)
    generator = Generator(z_dim=z_dim, generator_h_dim=generator_h_dim, x_dim=dim)
    
    # Define optimizers
    student_opt = tf.keras.optimizers.RMSprop(learning_rate=1e-4)
    generator_opt = tf.keras.optimizers.RMSprop(learning_rate=1e-4)
    
    # We keep track of alpha for the moments accountant
    L = 20
    alpha = np.zeros(L)
    epsilon_hat = 0.0
    
    ###########################################################################
    # Utility: Clip Student weights after each gradient update (WGAN-style)
    ###########################################################################
    def clip_student_weights(student_model, cmin=-0.01, cmax=0.01):
        for var in student_model.trainable_variables:
            var.assign(tf.clip_by_value(var, cmin, cmax))
    
    ###########################################################################
    # Main training loop: keep training until we hit the privacy budget
    ###########################################################################
    while epsilon_hat < epsilon:
        #######################################################################
        # 1. Train teacher models on real data vs. generator samples
        #######################################################################
        teacher_models = []
        for i in range(k):
            # Draw same number of generator samples as real samples
            Z_mb = sample_Z(partition_data_no, z_dim)
            G_mb = generator(Z_mb).numpy()
            
            # Real data from partition
            real_x = x_partitions[i]
            
            # In case real_x is smaller than partition_data_no (edge case)
            # here we just slice, or if you want exactly partition_data_no, do:
            idx_r = np.random.permutation(len(real_x))[:partition_data_no]
            X_mb = real_x[idx_r]
            
            # Build a dataset of size 2*partition_data_no: half real, half fake
            X_comb = np.concatenate((X_mb, G_mb), axis=0)
            Y_comb = np.concatenate((
                np.ones(partition_data_no),
                np.zeros(partition_data_no)
            ), axis=0)
            
            # Fit a logistic regression teacher
            model = LogisticRegression()
            model.fit(X_comb, Y_comb)
            teacher_models.append(model)
        
        #######################################################################
        # 2. Student training
        #######################################################################
        for _ in range(n_s):
            # Sample generator data
            Z_mb = sample_Z(batch_size, z_dim)
            G_mb = generator(Z_mb).numpy()
            
            # Get PATE labels (noisy votes from teachers)
            Y_mb_list = []
            for j in range(batch_size):
                n0, n1, r_j = pate_lambda(G_mb[j], teacher_models, lamda)
                Y_mb_list.append(r_j)
                
                # Update moments accountant for this sample
                # (This is the simplified logic from the original code.)
                q = np.log(2 + lamda * abs(n0 - n1)) - np.log(4.0) - (lamda * abs(n0 - n1))
                q = np.exp(q)
                
                for l in range(L):
                    term1 = 2 * (lamda ** 2) * (l + 1) * (l + 2)
                    term2 = ((1 - q) * (((1 - q) / (1 - q * np.exp(2 * lamda))) ** (l + 1))) \
                            + (q * np.exp(2 * lamda * (l + 1)))
                    alpha[l] += np.minimum(term1, np.log(term2))
            
            Y_mb = np.array(Y_mb_list).reshape(-1, 1).astype(np.float32)
            
            # One step of student update
            with tf.GradientTape() as tape:
                S_fake = student(G_mb)
                # S_loss = mean(y * S_fake) - mean((1-y)*S_fake)
                #  -> rewriting: E[y * S_fake] - E[(1-y) * S_fake]
                #  -> = E[y*S_fake] - E[S_fake - y*S_fake] = E[2*y*S_fake - S_fake]
                #  but we'll just do it literally
                s_loss = tf.reduce_mean(Y_mb * S_fake) - tf.reduce_mean((1 - Y_mb) * S_fake)
            
            grads_S = tape.gradient(s_loss, student.trainable_variables)
            student_opt.apply_gradients(zip(grads_S, student.trainable_variables))
            
            # Clip student weights
            clip_student_weights(student)
        
        #######################################################################
        # 3. Generator update
        #######################################################################
        Z_mb = sample_Z(batch_size, z_dim)
        with tf.GradientTape() as tape:
            G_sample = generator(Z_mb)
            S_fake = student(G_sample)
            g_loss = -tf.reduce_mean(S_fake)
        
        grads_G = tape.gradient(g_loss, generator.trainable_variables)
        generator_opt.apply_gradients(zip(grads_G, generator.trainable_variables))
        
        #######################################################################
        # 4. Update epsilon_hat via moments accountant
        #######################################################################
        eps_candidates = []
        for l in range(L):
            temp_alpha = (alpha[l] + np.log(1 / delta)) / float(l + 1)
            eps_candidates.append(temp_alpha)
        epsilon_hat = np.min(eps_candidates)
        
        # (Optional) Print progress
        # print(f"Current epsilon_hat: {epsilon_hat:.4f}, target epsilon: {epsilon}")
    
    ###########################################################################
    # Once we exit the loop, we produce DP-synthetic data 
    ###########################################################################
    x_train_hat = generator(sample_Z(n, z_dim)).numpy()
    return x_train_hat
