{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "#          IMPORTS             #\n",
    "# ----------------------------- #\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# ----------------------------- #\n",
    "#      CW-GAN PIPELINE         #\n",
    "# ----------------------------- #\n",
    "\n",
    "def run_cwgan_pipeline(csv_file, target_column, epochs=300, num_samples=2000):\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Encode categorical variables\n",
    "    df_encoded = df.copy()\n",
    "    encoders = {}\n",
    "    for col in df_encoded.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "        encoders[col] = le\n",
    "\n",
    "    X = df_encoded.drop(columns=[target_column])\n",
    "    y = df_encoded[target_column]\n",
    "    num_classes = y.nunique()\n",
    "    input_dim = X.shape[1]\n",
    "\n",
    "    # Create tensor dataset\n",
    "    X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    # Generator model\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(32 + num_classes, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, input_dim)\n",
    "            )\n",
    "        def forward(self, z, labels):\n",
    "            c = self.label_emb(labels)\n",
    "            x = torch.cat((z, c), dim=1)\n",
    "            return self.model(x)\n",
    "\n",
    "    # Critic model\n",
    "    class Critic(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(input_dim + num_classes, 128),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(128, 64),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(64, 1)\n",
    "            )\n",
    "        def forward(self, x, labels):\n",
    "            c = self.label_emb(labels)\n",
    "            d_in = torch.cat((x, c), dim=1)\n",
    "            return self.model(d_in)\n",
    "\n",
    "    def compute_gp(critic, real_samples, fake_samples, labels):\n",
    "        alpha = torch.rand(real_samples.size(0), 1).to(device)\n",
    "        interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "        d_interpolates = critic(interpolates, labels)\n",
    "        fake = torch.ones_like(d_interpolates)\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        return ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "    # Training setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    generator = Generator().to(device)\n",
    "    critic = Critic().to(device)\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "    optimizer_C = torch.optim.Adam(critic.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "    # Train GAN\n",
    "    for epoch in range(epochs):\n",
    "        for i, (real_samples, labels) in enumerate(loader):\n",
    "            real_samples = real_samples.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer_C.zero_grad()\n",
    "            z = torch.randn(real_samples.size(0), 32).to(device)\n",
    "            fake_samples = generator(z, labels)\n",
    "            real_validity = critic(real_samples, labels)\n",
    "            fake_validity = critic(fake_samples.detach(), labels)\n",
    "            gp = compute_gp(critic, real_samples.data, fake_samples.data, labels)\n",
    "            c_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + 10 * gp\n",
    "            c_loss.backward()\n",
    "            optimizer_C.step()\n",
    "\n",
    "            if i % 5 == 0:\n",
    "                optimizer_G.zero_grad()\n",
    "                gen_samples = generator(z, labels)\n",
    "                g_loss = -torch.mean(critic(gen_samples, labels))\n",
    "                g_loss.backward()\n",
    "                optimizer_G.step()\n",
    "\n",
    "         # Print every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "             print(f\"[{csv_file}] Epoch {epoch+1}/{epochs} | Critic Loss: {c_loss.item():.4f} | Generator Loss: {g_loss.item():.4f}\")\n",
    "        \n",
    "    # Generate synthetic data\n",
    "    z = torch.randn(num_samples, 32).to(device)\n",
    "    labels = torch.randint(0, num_classes, (num_samples,), dtype=torch.long).to(device)\n",
    "    gen_data = generator(z, labels).detach().cpu().numpy()\n",
    "    label_arr = labels.cpu().numpy()\n",
    "\n",
    "    gen_df = pd.DataFrame(gen_data, columns=X.columns)\n",
    "    decoded_df = gen_df.copy()\n",
    "    for col in decoded_df.columns:\n",
    "        valid_indices = np.arange(len(encoders[col].classes_))\n",
    "        rounded = np.round(decoded_df[col]).clip(min(valid_indices), max(valid_indices)).astype(int)\n",
    "        decoded_df[col] = encoders[col].inverse_transform(rounded)\n",
    "    decoded_df[target_column] = encoders[target_column].inverse_transform(label_arr)\n",
    "\n",
    "    # Re-encode synthetic data\n",
    "    X_synth = decoded_df.drop(columns=[target_column])\n",
    "    y_synth = decoded_df[target_column]\n",
    "    X_encoded = pd.DataFrame()\n",
    "    for col in X_synth.columns:\n",
    "        X_encoded[col] = encoders[col].transform(X_synth[col])\n",
    "    y_encoded = encoders[target_column].transform(y_synth)\n",
    "\n",
    "    # Define classifiers\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(),\n",
    "        \"MLPClassifier\": MLPClassifier(max_iter=500),\n",
    "        \"LogisticRegression\": LogisticRegression(max_iter=300),\n",
    "        \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "    }\n",
    "\n",
    "    # ==== 10-chunk classifier evaluation on synthetic data ====\n",
    "    full_df = X_encoded.copy()\n",
    "    full_df['target'] = y_encoded\n",
    "    total_samples = len(full_df)\n",
    "    chunk_size = total_samples // 5\n",
    "    accuracy_results = defaultdict(list)\n",
    "\n",
    "    for repeat in range(2):  # Before and after shuffle\n",
    "        print(f\"\\nüîÅ Starting round {repeat + 1} (Shuffle: {'Yes' if repeat == 1 else 'No'})\")\n",
    "\n",
    "        if repeat == 1:\n",
    "            full_df = shuffle(full_df, random_state=42).reset_index(drop=True)\n",
    "\n",
    "        for chunk in range(5):\n",
    "            start = chunk * chunk_size\n",
    "            end = start + chunk_size if chunk < 4 else total_samples\n",
    "            chunk_df = full_df.iloc[start:end]\n",
    "\n",
    "            X_chunk = chunk_df.drop(columns=['target'])\n",
    "            y_chunk = chunk_df['target']\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_chunk, y_chunk, test_size=0.2, random_state=42)\n",
    "\n",
    "            print(f\"\\nüß™ Training on Chunk {chunk + 1}/5 of Round {repeat + 1} (Rows {start} to {end})\")\n",
    "\n",
    "            for name, model in models.items():\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "                accuracy_results[name].append(acc)\n",
    "                print(f\"{name} - Accuracy: {acc:.4f}\")\n",
    "\n",
    "    print(\"\\nüìä Final Average Accuracy on SYNTHETIC data:\")\n",
    "    for name, scores in accuracy_results.items():\n",
    "        avg_acc = np.mean(scores)\n",
    "        print(f\"{name}: Average Accuracy = {avg_acc:.4f}\")\n",
    "\n",
    "\n",
    "# ----------------------------- #\n",
    "#  BASELINE ON REAL DATASET    #\n",
    "# ----------------------------- #\n",
    "\n",
    "# def evaluate_on_real_data(csv_file, target_column):\n",
    "#     df = pd.read_csv(csv_file)\n",
    "\n",
    "#     # Encode\n",
    "#     df_encoded = df.copy()\n",
    "#     encoders = {}\n",
    "#     for col in df_encoded.columns:\n",
    "#         le = LabelEncoder()\n",
    "#         df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "#         encoders[col] = le\n",
    "\n",
    "#     X = df_encoded.drop(columns=[target_column])\n",
    "#     y = df_encoded[target_column]\n",
    "\n",
    "#     # Stratified split to avoid class mismatch\n",
    "#     sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "#     for train_idx, test_idx in sss.split(X, y):\n",
    "#         X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "#         y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "#     # Define models\n",
    "#     models = {\n",
    "#         \"RandomForest\": RandomForestClassifier(),\n",
    "#         \"MLPClassifier\": MLPClassifier(max_iter=500),\n",
    "#         \"LogisticRegression\": LogisticRegression(max_iter=300),\n",
    "#         \"XGBoost\": XGBClassifier(eval_metric=\"mlogloss\")\n",
    "#     }\n",
    "\n",
    "#     print(\"\\nüìä Accuracy of classifiers trained on REAL data:\")\n",
    "\n",
    "#     for name, model in models.items():\n",
    "#         model.fit(X_train, y_train)\n",
    "#         y_pred = model.predict(X_test)\n",
    "#         acc = accuracy_score(y_test, y_pred)\n",
    "#         print(f\"{name}: Accuracy = {acc:.4f}\")\n",
    "\n",
    "# ----------------------------- #\n",
    "#           RUN IT             #\n",
    "# ----------------------------- #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nursery.csv] Epoch 10/300 | Critic Loss: -2.2653 | Generator Loss: -0.6611\n",
      "[nursery.csv] Epoch 20/300 | Critic Loss: -1.7459 | Generator Loss: 0.5883\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnursery.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mrun_cwgan_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#evaluate_on_real_data(csv_path, target)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 118\u001b[0m, in \u001b[0;36mrun_cwgan_pipeline\u001b[0;34m(csv_file, target_column, epochs, num_samples)\u001b[0m\n\u001b[1;32m    116\u001b[0m c_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(real_validity) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(fake_validity) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m gp\n\u001b[1;32m    117\u001b[0m c_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 118\u001b[0m \u001b[43moptimizer_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    121\u001b[0m     optimizer_G\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/optim/adam.py:244\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    235\u001b[0m         group,\n\u001b[1;32m    236\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m         state_steps,\n\u001b[1;32m    242\u001b[0m     )\n\u001b[0;32m--> 244\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/optim/adam.py:876\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    874\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 876\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/optim/adam.py:395\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    390\u001b[0m         param\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m step_t\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01min\u001b[39;00m capturable_supported_devices\n\u001b[1;32m    392\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf capturable=True, params and state_steps must be on supported devices: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcapturable_supported_devices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# update step\u001b[39;00m\n\u001b[0;32m--> 395\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    398\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Adjust CSV path as needed\n",
    "csv_path = \"nursery.csv\"\n",
    "target = \"Target\"\n",
    "\n",
    "run_cwgan_pipeline(csv_path, target)\n",
    "#evaluate_on_real_data(csv_path, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[letter-recognition-2.csv] Epoch 10/300 | Critic Loss: -18.5876 | Generator Loss: -17.0938\n",
      "[letter-recognition-2.csv] Epoch 20/300 | Critic Loss: -7.4032 | Generator Loss: -2.3815\n",
      "[letter-recognition-2.csv] Epoch 30/300 | Critic Loss: -6.5988 | Generator Loss: -1.1215\n",
      "[letter-recognition-2.csv] Epoch 40/300 | Critic Loss: -5.4533 | Generator Loss: -1.4681\n",
      "[letter-recognition-2.csv] Epoch 50/300 | Critic Loss: -2.8873 | Generator Loss: -0.4209\n",
      "[letter-recognition-2.csv] Epoch 60/300 | Critic Loss: -2.8446 | Generator Loss: 3.0356\n",
      "[letter-recognition-2.csv] Epoch 70/300 | Critic Loss: -3.5078 | Generator Loss: 1.4259\n",
      "[letter-recognition-2.csv] Epoch 80/300 | Critic Loss: -1.7420 | Generator Loss: -1.0205\n",
      "[letter-recognition-2.csv] Epoch 90/300 | Critic Loss: -0.5450 | Generator Loss: 0.1197\n",
      "[letter-recognition-2.csv] Epoch 100/300 | Critic Loss: -2.6525 | Generator Loss: 2.2407\n",
      "[letter-recognition-2.csv] Epoch 110/300 | Critic Loss: -1.7055 | Generator Loss: 0.8824\n",
      "[letter-recognition-2.csv] Epoch 120/300 | Critic Loss: -1.2968 | Generator Loss: 2.6802\n",
      "[letter-recognition-2.csv] Epoch 130/300 | Critic Loss: -1.2356 | Generator Loss: -1.3367\n",
      "[letter-recognition-2.csv] Epoch 140/300 | Critic Loss: -1.1487 | Generator Loss: 1.6382\n",
      "[letter-recognition-2.csv] Epoch 150/300 | Critic Loss: -1.6868 | Generator Loss: 0.3318\n",
      "[letter-recognition-2.csv] Epoch 160/300 | Critic Loss: -1.2257 | Generator Loss: -0.3062\n",
      "[letter-recognition-2.csv] Epoch 170/300 | Critic Loss: -1.4047 | Generator Loss: 3.6352\n",
      "[letter-recognition-2.csv] Epoch 180/300 | Critic Loss: -1.8331 | Generator Loss: 4.0715\n",
      "[letter-recognition-2.csv] Epoch 190/300 | Critic Loss: -2.2797 | Generator Loss: 0.9145\n",
      "[letter-recognition-2.csv] Epoch 200/300 | Critic Loss: -1.4144 | Generator Loss: 2.9009\n",
      "[letter-recognition-2.csv] Epoch 210/300 | Critic Loss: -1.5882 | Generator Loss: 2.6595\n",
      "[letter-recognition-2.csv] Epoch 220/300 | Critic Loss: -1.2689 | Generator Loss: 1.7919\n",
      "[letter-recognition-2.csv] Epoch 230/300 | Critic Loss: -1.9164 | Generator Loss: 0.5025\n",
      "[letter-recognition-2.csv] Epoch 240/300 | Critic Loss: -1.5397 | Generator Loss: 1.3219\n",
      "[letter-recognition-2.csv] Epoch 250/300 | Critic Loss: -1.1120 | Generator Loss: 0.4810\n",
      "[letter-recognition-2.csv] Epoch 260/300 | Critic Loss: -1.9632 | Generator Loss: 1.5633\n",
      "[letter-recognition-2.csv] Epoch 270/300 | Critic Loss: -1.3326 | Generator Loss: 0.4006\n",
      "[letter-recognition-2.csv] Epoch 280/300 | Critic Loss: -0.9757 | Generator Loss: 3.1060\n",
      "[letter-recognition-2.csv] Epoch 290/300 | Critic Loss: -1.0649 | Generator Loss: 0.3927\n",
      "[letter-recognition-2.csv] Epoch 300/300 | Critic Loss: -1.3851 | Generator Loss: 4.8305\n",
      "\n",
      "üîÅ Starting round 1 (Shuffle: No)\n",
      "\n",
      "üß™ Training on Chunk 1/5 of Round 1 (Rows 0 to 400)\n",
      "RandomForest - Accuracy: 0.6125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [12:58:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.7000\n",
      "LogisticRegression - Accuracy: 0.6875\n",
      "XGBoost - Accuracy: 0.6375\n",
      "\n",
      "üß™ Training on Chunk 2/5 of Round 1 (Rows 400 to 800)\n",
      "RandomForest - Accuracy: 0.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [12:58:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.6125\n",
      "LogisticRegression - Accuracy: 0.5625\n",
      "XGBoost - Accuracy: 0.5500\n",
      "\n",
      "üß™ Training on Chunk 3/5 of Round 1 (Rows 800 to 1200)\n",
      "RandomForest - Accuracy: 0.6125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [12:58:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.6625\n",
      "LogisticRegression - Accuracy: 0.6750\n",
      "XGBoost - Accuracy: 0.5875\n",
      "\n",
      "üß™ Training on Chunk 4/5 of Round 1 (Rows 1200 to 1600)\n",
      "RandomForest - Accuracy: 0.5750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [12:58:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.6625\n",
      "LogisticRegression - Accuracy: 0.6875\n",
      "XGBoost - Accuracy: 0.6000\n",
      "\n",
      "üß™ Training on Chunk 5/5 of Round 1 (Rows 1600 to 2000)\n",
      "RandomForest - Accuracy: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [12:58:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.5500\n",
      "LogisticRegression - Accuracy: 0.6625\n",
      "XGBoost - Accuracy: 0.5625\n",
      "\n",
      "üîÅ Starting round 2 (Shuffle: Yes)\n",
      "\n",
      "üß™ Training on Chunk 1/5 of Round 2 (Rows 0 to 400)\n",
      "RandomForest - Accuracy: 0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [12:58:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.6500\n",
      "LogisticRegression - Accuracy: 0.6375\n",
      "XGBoost - Accuracy: 0.6000\n",
      "\n",
      "üß™ Training on Chunk 2/5 of Round 2 (Rows 400 to 800)\n",
      "RandomForest - Accuracy: 0.6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [12:58:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.7125\n",
      "LogisticRegression - Accuracy: 0.6875\n",
      "XGBoost - Accuracy: 0.4875\n",
      "\n",
      "üß™ Training on Chunk 3/5 of Round 2 (Rows 800 to 1200)\n",
      "RandomForest - Accuracy: 0.6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [12:58:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.6250\n",
      "LogisticRegression - Accuracy: 0.6125\n",
      "XGBoost - Accuracy: 0.5750\n",
      "\n",
      "üß™ Training on Chunk 4/5 of Round 2 (Rows 1200 to 1600)\n",
      "RandomForest - Accuracy: 0.5250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [12:58:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.6375\n",
      "LogisticRegression - Accuracy: 0.6875\n",
      "XGBoost - Accuracy: 0.4875\n",
      "\n",
      "üß™ Training on Chunk 5/5 of Round 2 (Rows 1600 to 2000)\n",
      "RandomForest - Accuracy: 0.6125\n",
      "MLPClassifier - Accuracy: 0.6875\n",
      "LogisticRegression - Accuracy: 0.6750\n",
      "XGBoost - Accuracy: 0.5500\n",
      "\n",
      "üìä Final Average Accuracy on SYNTHETIC data:\n",
      "RandomForest: Average Accuracy = 0.5900\n",
      "MLPClassifier: Average Accuracy = 0.6500\n",
      "LogisticRegression: Average Accuracy = 0.6575\n",
      "XGBoost: Average Accuracy = 0.5637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [12:58:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "run_cwgan_pipeline(\"letter-recognition-2.csv\", \"letter\")\n",
    "#evaluate_on_real_data(\"letter-recognition-2.csv\", \"letter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[satellite.csv] Epoch 10/300 | Critic Loss: -474.2088 | Generator Loss: -0.0243\n",
      "[satellite.csv] Epoch 20/300 | Critic Loss: -1489.9869 | Generator Loss: -0.4603\n",
      "[satellite.csv] Epoch 30/300 | Critic Loss: -3141.7512 | Generator Loss: -2.6643\n",
      "[satellite.csv] Epoch 40/300 | Critic Loss: -4848.3130 | Generator Loss: -9.9609\n",
      "[satellite.csv] Epoch 50/300 | Critic Loss: -8936.9209 | Generator Loss: -27.1223\n",
      "[satellite.csv] Epoch 60/300 | Critic Loss: -13003.3477 | Generator Loss: -59.3526\n",
      "[satellite.csv] Epoch 70/300 | Critic Loss: -17558.8945 | Generator Loss: -112.6599\n",
      "[satellite.csv] Epoch 80/300 | Critic Loss: -19841.2422 | Generator Loss: -207.1697\n",
      "[satellite.csv] Epoch 90/300 | Critic Loss: -28666.8379 | Generator Loss: -335.4250\n",
      "[satellite.csv] Epoch 100/300 | Critic Loss: -39275.1719 | Generator Loss: -527.4099\n",
      "[satellite.csv] Epoch 110/300 | Critic Loss: -44569.0781 | Generator Loss: -834.6372\n",
      "[satellite.csv] Epoch 120/300 | Critic Loss: -44989.5938 | Generator Loss: -1155.3904\n",
      "[satellite.csv] Epoch 130/300 | Critic Loss: -64286.6484 | Generator Loss: -1552.8215\n",
      "[satellite.csv] Epoch 140/300 | Critic Loss: -49725.4648 | Generator Loss: -2087.4375\n",
      "[satellite.csv] Epoch 150/300 | Critic Loss: -53480.8594 | Generator Loss: -2455.0679\n",
      "[satellite.csv] Epoch 160/300 | Critic Loss: -45323.2266 | Generator Loss: -3117.2048\n",
      "[satellite.csv] Epoch 170/300 | Critic Loss: -52109.1055 | Generator Loss: -3440.1401\n",
      "[satellite.csv] Epoch 180/300 | Critic Loss: -46841.4062 | Generator Loss: -4083.8301\n",
      "[satellite.csv] Epoch 190/300 | Critic Loss: -50246.5273 | Generator Loss: -4452.4824\n",
      "[satellite.csv] Epoch 200/300 | Critic Loss: -53682.2266 | Generator Loss: -5279.5903\n",
      "[satellite.csv] Epoch 210/300 | Critic Loss: -60961.3281 | Generator Loss: -5817.5078\n",
      "[satellite.csv] Epoch 220/300 | Critic Loss: -58912.9219 | Generator Loss: -7033.1821\n",
      "[satellite.csv] Epoch 230/300 | Critic Loss: -55535.8828 | Generator Loss: -7420.2930\n",
      "[satellite.csv] Epoch 240/300 | Critic Loss: -41272.7578 | Generator Loss: -7458.3867\n",
      "[satellite.csv] Epoch 250/300 | Critic Loss: -54362.8164 | Generator Loss: -9168.9023\n",
      "[satellite.csv] Epoch 260/300 | Critic Loss: -54136.9961 | Generator Loss: -10113.7012\n",
      "[satellite.csv] Epoch 270/300 | Critic Loss: -42554.7930 | Generator Loss: -10529.5391\n",
      "[satellite.csv] Epoch 280/300 | Critic Loss: -43304.1016 | Generator Loss: -11322.9648\n",
      "[satellite.csv] Epoch 290/300 | Critic Loss: -34387.8672 | Generator Loss: -12147.0977\n",
      "[satellite.csv] Epoch 300/300 | Critic Loss: -64212.2734 | Generator Loss: -12463.8848\n",
      "\n",
      "üîÅ Starting round 1 (Shuffle: No)\n",
      "\n",
      "üß™ Training on Chunk 1/5 of Round 1 (Rows 0 to 400)\n",
      "RandomForest - Accuracy: 0.3250\n",
      "MLPClassifier - Accuracy: 0.1875\n",
      "LogisticRegression - Accuracy: 0.3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:07:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - Accuracy: 0.3375\n",
      "\n",
      "üß™ Training on Chunk 2/5 of Round 1 (Rows 400 to 800)\n",
      "RandomForest - Accuracy: 0.2500\n",
      "MLPClassifier - Accuracy: 0.1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:07:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression - Accuracy: 0.3625\n",
      "XGBoost - Accuracy: 0.2250\n",
      "\n",
      "üß™ Training on Chunk 3/5 of Round 1 (Rows 800 to 1200)\n",
      "RandomForest - Accuracy: 0.3625\n",
      "MLPClassifier - Accuracy: 0.2125\n",
      "LogisticRegression - Accuracy: 0.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:07:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - Accuracy: 0.3500\n",
      "\n",
      "üß™ Training on Chunk 4/5 of Round 1 (Rows 1200 to 1600)\n",
      "RandomForest - Accuracy: 0.2750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:07:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.3000\n",
      "LogisticRegression - Accuracy: 0.3750\n",
      "XGBoost - Accuracy: 0.3000\n",
      "\n",
      "üß™ Training on Chunk 5/5 of Round 1 (Rows 1600 to 2000)\n",
      "RandomForest - Accuracy: 0.3625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:07:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.4125\n",
      "LogisticRegression - Accuracy: 0.3500\n",
      "XGBoost - Accuracy: 0.3000\n",
      "\n",
      "üîÅ Starting round 2 (Shuffle: Yes)\n",
      "\n",
      "üß™ Training on Chunk 1/5 of Round 2 (Rows 0 to 400)\n",
      "RandomForest - Accuracy: 0.2750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:07:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.2125\n",
      "LogisticRegression - Accuracy: 0.3375\n",
      "XGBoost - Accuracy: 0.2750\n",
      "\n",
      "üß™ Training on Chunk 2/5 of Round 2 (Rows 400 to 800)\n",
      "RandomForest - Accuracy: 0.3250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:07:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.3125\n",
      "LogisticRegression - Accuracy: 0.3625\n",
      "XGBoost - Accuracy: 0.2500\n",
      "\n",
      "üß™ Training on Chunk 3/5 of Round 2 (Rows 800 to 1200)\n",
      "RandomForest - Accuracy: 0.3625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:07:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.3000\n",
      "LogisticRegression - Accuracy: 0.4000\n",
      "XGBoost - Accuracy: 0.3750\n",
      "\n",
      "üß™ Training on Chunk 4/5 of Round 2 (Rows 1200 to 1600)\n",
      "RandomForest - Accuracy: 0.2500\n",
      "MLPClassifier - Accuracy: 0.1750\n",
      "LogisticRegression - Accuracy: 0.3625\n",
      "XGBoost - Accuracy: 0.3125\n",
      "\n",
      "üß™ Training on Chunk 5/5 of Round 2 (Rows 1600 to 2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:07:39] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Accuracy: 0.2750\n",
      "MLPClassifier - Accuracy: 0.2250\n",
      "LogisticRegression - Accuracy: 0.4000\n",
      "XGBoost - Accuracy: 0.2500\n",
      "\n",
      "üìä Final Average Accuracy on SYNTHETIC data:\n",
      "RandomForest: Average Accuracy = 0.3062\n",
      "MLPClassifier: Average Accuracy = 0.2487\n",
      "LogisticRegression: Average Accuracy = 0.3612\n",
      "XGBoost: Average Accuracy = 0.2975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:07:39] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"satellite.csv\"\n",
    "target = \"Purpose\"  # <-- Change this if your target column has a different name\n",
    "\n",
    "run_cwgan_pipeline(csv_path, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Bank_Personal_Loan.csv] Epoch 10/300 | Critic Loss: -5447.5850 | Generator Loss: -2.0383\n",
      "[Bank_Personal_Loan.csv] Epoch 20/300 | Critic Loss: -20116.5215 | Generator Loss: -19.5606\n",
      "[Bank_Personal_Loan.csv] Epoch 30/300 | Critic Loss: -43128.8359 | Generator Loss: -70.6840\n",
      "[Bank_Personal_Loan.csv] Epoch 40/300 | Critic Loss: -77634.7812 | Generator Loss: -188.8972\n",
      "[Bank_Personal_Loan.csv] Epoch 50/300 | Critic Loss: -102706.7734 | Generator Loss: -427.1605\n",
      "[Bank_Personal_Loan.csv] Epoch 60/300 | Critic Loss: -242935.9844 | Generator Loss: -863.9886\n",
      "[Bank_Personal_Loan.csv] Epoch 70/300 | Critic Loss: -124384.5781 | Generator Loss: -1562.5706\n",
      "[Bank_Personal_Loan.csv] Epoch 80/300 | Critic Loss: -179174.8281 | Generator Loss: -2125.1792\n",
      "[Bank_Personal_Loan.csv] Epoch 90/300 | Critic Loss: -237937.1875 | Generator Loss: -2707.3740\n",
      "[Bank_Personal_Loan.csv] Epoch 100/300 | Critic Loss: -178342.6094 | Generator Loss: -3252.7529\n",
      "[Bank_Personal_Loan.csv] Epoch 110/300 | Critic Loss: -19687.3750 | Generator Loss: -4223.9033\n",
      "[Bank_Personal_Loan.csv] Epoch 120/300 | Critic Loss: 4387.7031 | Generator Loss: -5110.6274\n",
      "[Bank_Personal_Loan.csv] Epoch 130/300 | Critic Loss: -205532.0938 | Generator Loss: -6302.4668\n",
      "[Bank_Personal_Loan.csv] Epoch 140/300 | Critic Loss: -190680.9688 | Generator Loss: -7235.0396\n",
      "[Bank_Personal_Loan.csv] Epoch 150/300 | Critic Loss: -244407.9062 | Generator Loss: -8704.3760\n",
      "[Bank_Personal_Loan.csv] Epoch 160/300 | Critic Loss: -135464.3750 | Generator Loss: -10066.7012\n",
      "[Bank_Personal_Loan.csv] Epoch 170/300 | Critic Loss: -198746.7500 | Generator Loss: -11495.5039\n",
      "[Bank_Personal_Loan.csv] Epoch 180/300 | Critic Loss: -183174.4688 | Generator Loss: -13040.1709\n",
      "[Bank_Personal_Loan.csv] Epoch 190/300 | Critic Loss: -57609.2344 | Generator Loss: -14793.7197\n",
      "[Bank_Personal_Loan.csv] Epoch 200/300 | Critic Loss: -148975.9688 | Generator Loss: -16461.7285\n",
      "[Bank_Personal_Loan.csv] Epoch 210/300 | Critic Loss: -124508.2500 | Generator Loss: -18430.5449\n",
      "[Bank_Personal_Loan.csv] Epoch 220/300 | Critic Loss: -130862.9062 | Generator Loss: -20907.3359\n",
      "[Bank_Personal_Loan.csv] Epoch 230/300 | Critic Loss: -5025.5312 | Generator Loss: -23012.1934\n",
      "[Bank_Personal_Loan.csv] Epoch 240/300 | Critic Loss: -233836.0469 | Generator Loss: -25279.3164\n",
      "[Bank_Personal_Loan.csv] Epoch 250/300 | Critic Loss: -136813.9062 | Generator Loss: -27336.7422\n",
      "[Bank_Personal_Loan.csv] Epoch 260/300 | Critic Loss: -173964.4688 | Generator Loss: -30197.4062\n",
      "[Bank_Personal_Loan.csv] Epoch 270/300 | Critic Loss: -154686.9531 | Generator Loss: -32888.8086\n",
      "[Bank_Personal_Loan.csv] Epoch 280/300 | Critic Loss: -129941.1328 | Generator Loss: -36216.3281\n",
      "[Bank_Personal_Loan.csv] Epoch 290/300 | Critic Loss: -188879.7812 | Generator Loss: -38746.2031\n",
      "[Bank_Personal_Loan.csv] Epoch 300/300 | Critic Loss: -195860.0156 | Generator Loss: -42963.1406\n",
      "\n",
      "üîÅ Starting round 1 (Shuffle: No)\n",
      "\n",
      "üß™ Training on Chunk 1/5 of Round 1 (Rows 0 to 400)\n",
      "RandomForest - Accuracy: 0.6250\n",
      "MLPClassifier - Accuracy: 0.5625\n",
      "LogisticRegression - Accuracy: 0.6250\n",
      "XGBoost - Accuracy: 0.6000\n",
      "\n",
      "üß™ Training on Chunk 2/5 of Round 1 (Rows 400 to 800)\n",
      "RandomForest - Accuracy: 0.6875\n",
      "MLPClassifier - Accuracy: 0.6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:17:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:17:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression - Accuracy: 0.7125\n",
      "XGBoost - Accuracy: 0.6750\n",
      "\n",
      "üß™ Training on Chunk 3/5 of Round 1 (Rows 800 to 1200)\n",
      "RandomForest - Accuracy: 0.6000\n",
      "MLPClassifier - Accuracy: 0.5000\n",
      "LogisticRegression - Accuracy: 0.6500\n",
      "XGBoost - Accuracy: 0.5125\n",
      "\n",
      "üß™ Training on Chunk 4/5 of Round 1 (Rows 1200 to 1600)\n",
      "RandomForest - Accuracy: 0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:17:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:17:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.5500\n",
      "LogisticRegression - Accuracy: 0.6250\n",
      "XGBoost - Accuracy: 0.5750\n",
      "\n",
      "üß™ Training on Chunk 5/5 of Round 1 (Rows 1600 to 2000)\n",
      "RandomForest - Accuracy: 0.6000\n",
      "MLPClassifier - Accuracy: 0.5875\n",
      "LogisticRegression - Accuracy: 0.6500\n",
      "XGBoost - Accuracy: 0.6500\n",
      "\n",
      "üîÅ Starting round 2 (Shuffle: Yes)\n",
      "\n",
      "üß™ Training on Chunk 1/5 of Round 2 (Rows 0 to 400)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:17:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:17:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Accuracy: 0.6000\n",
      "MLPClassifier - Accuracy: 0.4625\n",
      "LogisticRegression - Accuracy: 0.5375\n",
      "XGBoost - Accuracy: 0.6250\n",
      "\n",
      "üß™ Training on Chunk 2/5 of Round 2 (Rows 400 to 800)\n",
      "RandomForest - Accuracy: 0.6000\n",
      "MLPClassifier - Accuracy: 0.5500\n",
      "LogisticRegression - Accuracy: 0.6250\n",
      "XGBoost - Accuracy: 0.6000\n",
      "\n",
      "üß™ Training on Chunk 3/5 of Round 2 (Rows 800 to 1200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:17:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:17:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Accuracy: 0.5500\n",
      "MLPClassifier - Accuracy: 0.5375\n",
      "LogisticRegression - Accuracy: 0.5875\n",
      "XGBoost - Accuracy: 0.4750\n",
      "\n",
      "üß™ Training on Chunk 4/5 of Round 2 (Rows 1200 to 1600)\n",
      "RandomForest - Accuracy: 0.5250\n",
      "MLPClassifier - Accuracy: 0.5250\n",
      "LogisticRegression - Accuracy: 0.6375\n",
      "XGBoost - Accuracy: 0.5125\n",
      "\n",
      "üß™ Training on Chunk 5/5 of Round 2 (Rows 1600 to 2000)\n",
      "RandomForest - Accuracy: 0.6875\n",
      "MLPClassifier - Accuracy: 0.5875\n",
      "LogisticRegression - Accuracy: 0.6250\n",
      "XGBoost - Accuracy: 0.5875\n",
      "\n",
      "üìä Final Average Accuracy on SYNTHETIC data:\n",
      "RandomForest: Average Accuracy = 0.6038\n",
      "MLPClassifier: Average Accuracy = 0.5513\n",
      "LogisticRegression: Average Accuracy = 0.6275\n",
      "XGBoost: Average Accuracy = 0.5813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:17:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:17:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"Bank_Personal_Loan.csv\"\n",
    "target = \"Personal Loan\"  # <-- Confirm exact spelling\n",
    "\n",
    "run_cwgan_pipeline(csv_path, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[car.csv] Epoch 10/300 | Critic Loss: 0.1892 | Generator Loss: 0.0870\n",
      "[car.csv] Epoch 20/300 | Critic Loss: -2.6257 | Generator Loss: -0.1677\n",
      "[car.csv] Epoch 30/300 | Critic Loss: -2.6099 | Generator Loss: -0.4955\n",
      "[car.csv] Epoch 40/300 | Critic Loss: -2.6364 | Generator Loss: -0.9037\n",
      "[car.csv] Epoch 50/300 | Critic Loss: -2.5980 | Generator Loss: -1.2068\n",
      "[car.csv] Epoch 60/300 | Critic Loss: -2.1969 | Generator Loss: -1.3192\n",
      "[car.csv] Epoch 70/300 | Critic Loss: -1.8108 | Generator Loss: -1.1663\n",
      "[car.csv] Epoch 80/300 | Critic Loss: -1.8877 | Generator Loss: -0.4858\n",
      "[car.csv] Epoch 90/300 | Critic Loss: -1.7775 | Generator Loss: 0.2218\n",
      "[car.csv] Epoch 100/300 | Critic Loss: -1.8255 | Generator Loss: 0.2935\n",
      "[car.csv] Epoch 110/300 | Critic Loss: -1.6230 | Generator Loss: 0.4517\n",
      "[car.csv] Epoch 120/300 | Critic Loss: -1.6370 | Generator Loss: 0.3291\n",
      "[car.csv] Epoch 130/300 | Critic Loss: -1.5434 | Generator Loss: 0.4971\n",
      "[car.csv] Epoch 140/300 | Critic Loss: -1.4574 | Generator Loss: 0.2909\n",
      "[car.csv] Epoch 150/300 | Critic Loss: -1.4485 | Generator Loss: 0.0993\n",
      "[car.csv] Epoch 160/300 | Critic Loss: -1.3215 | Generator Loss: 0.0894\n",
      "[car.csv] Epoch 170/300 | Critic Loss: -1.1948 | Generator Loss: -0.0114\n",
      "[car.csv] Epoch 180/300 | Critic Loss: -1.0334 | Generator Loss: -0.0653\n",
      "[car.csv] Epoch 190/300 | Critic Loss: -0.9280 | Generator Loss: -0.1036\n",
      "[car.csv] Epoch 200/300 | Critic Loss: -0.9831 | Generator Loss: -0.1661\n",
      "[car.csv] Epoch 210/300 | Critic Loss: -0.7897 | Generator Loss: -0.1217\n",
      "[car.csv] Epoch 220/300 | Critic Loss: -0.7970 | Generator Loss: -0.1008\n",
      "[car.csv] Epoch 230/300 | Critic Loss: -0.7060 | Generator Loss: -0.0888\n",
      "[car.csv] Epoch 240/300 | Critic Loss: -0.3655 | Generator Loss: -0.0262\n",
      "[car.csv] Epoch 250/300 | Critic Loss: -0.3482 | Generator Loss: -0.0321\n",
      "[car.csv] Epoch 260/300 | Critic Loss: -0.3556 | Generator Loss: -0.4975\n",
      "[car.csv] Epoch 270/300 | Critic Loss: -0.1040 | Generator Loss: -0.4891\n",
      "[car.csv] Epoch 280/300 | Critic Loss: 0.0891 | Generator Loss: 0.3486\n",
      "[car.csv] Epoch 290/300 | Critic Loss: -0.1161 | Generator Loss: -1.5241\n",
      "[car.csv] Epoch 300/300 | Critic Loss: 0.4316 | Generator Loss: -0.7765\n",
      "\n",
      "üîÅ Starting round 1 (Shuffle: No)\n",
      "\n",
      "üß™ Training on Chunk 1/5 of Round 1 (Rows 0 to 400)\n",
      "RandomForest - Accuracy: 0.2625\n",
      "MLPClassifier - Accuracy: 0.3875\n",
      "LogisticRegression - Accuracy: 0.3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:44:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - Accuracy: 0.2375\n",
      "\n",
      "üß™ Training on Chunk 2/5 of Round 1 (Rows 400 to 800)\n",
      "RandomForest - Accuracy: 0.3375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:44:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.3000\n",
      "LogisticRegression - Accuracy: 0.3250\n",
      "XGBoost - Accuracy: 0.2875\n",
      "\n",
      "üß™ Training on Chunk 3/5 of Round 1 (Rows 800 to 1200)\n",
      "RandomForest - Accuracy: 0.3250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:44:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.2875\n",
      "LogisticRegression - Accuracy: 0.3625\n",
      "XGBoost - Accuracy: 0.2750\n",
      "\n",
      "üß™ Training on Chunk 4/5 of Round 1 (Rows 1200 to 1600)\n",
      "RandomForest - Accuracy: 0.2875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:44:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.3250\n",
      "LogisticRegression - Accuracy: 0.2625\n",
      "XGBoost - Accuracy: 0.2500\n",
      "\n",
      "üß™ Training on Chunk 5/5 of Round 1 (Rows 1600 to 2000)\n",
      "RandomForest - Accuracy: 0.3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:44:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.3750\n",
      "LogisticRegression - Accuracy: 0.3625\n",
      "XGBoost - Accuracy: 0.3250\n",
      "\n",
      "üîÅ Starting round 2 (Shuffle: Yes)\n",
      "\n",
      "üß™ Training on Chunk 1/5 of Round 2 (Rows 0 to 400)\n",
      "RandomForest - Accuracy: 0.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:44:39] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.2625\n",
      "LogisticRegression - Accuracy: 0.2500\n",
      "XGBoost - Accuracy: 0.3250\n",
      "\n",
      "üß™ Training on Chunk 2/5 of Round 2 (Rows 400 to 800)\n",
      "RandomForest - Accuracy: 0.2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:44:39] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.3000\n",
      "LogisticRegression - Accuracy: 0.3125\n",
      "XGBoost - Accuracy: 0.2250\n",
      "\n",
      "üß™ Training on Chunk 3/5 of Round 2 (Rows 800 to 1200)\n",
      "RandomForest - Accuracy: 0.3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:44:39] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.3375\n",
      "LogisticRegression - Accuracy: 0.3750\n",
      "XGBoost - Accuracy: 0.3250\n",
      "\n",
      "üß™ Training on Chunk 4/5 of Round 2 (Rows 1200 to 1600)\n",
      "RandomForest - Accuracy: 0.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:44:40] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.3625\n",
      "LogisticRegression - Accuracy: 0.4000\n",
      "XGBoost - Accuracy: 0.3125\n",
      "\n",
      "üß™ Training on Chunk 5/5 of Round 2 (Rows 1600 to 2000)\n",
      "RandomForest - Accuracy: 0.3750\n",
      "MLPClassifier - Accuracy: 0.3250\n",
      "LogisticRegression - Accuracy: 0.4125\n",
      "XGBoost - Accuracy: 0.3750\n",
      "\n",
      "üìä Final Average Accuracy on SYNTHETIC data:\n",
      "RandomForest: Average Accuracy = 0.3112\n",
      "MLPClassifier: Average Accuracy = 0.3263\n",
      "LogisticRegression: Average Accuracy = 0.3412\n",
      "XGBoost: Average Accuracy = 0.2938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:44:40] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"car.csv\"\n",
    "target = \"Class\"  # Or possibly \"Acceptability\" ‚Äì depends on exact column names\n",
    "\n",
    "run_cwgan_pipeline(csv_path, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[games.csv] Epoch 10/300 | Critic Loss: -1135931.1250 | Generator Loss: -871.4750\n",
      "[games.csv] Epoch 20/300 | Critic Loss: -5138733.5000 | Generator Loss: -20126.5742\n",
      "[games.csv] Epoch 30/300 | Critic Loss: -7885935.0000 | Generator Loss: -102314.1484\n",
      "[games.csv] Epoch 40/300 | Critic Loss: -7948184.5000 | Generator Loss: -205883.0156\n",
      "[games.csv] Epoch 50/300 | Critic Loss: -7926738.5000 | Generator Loss: -370425.0312\n",
      "[games.csv] Epoch 60/300 | Critic Loss: -7225608.5000 | Generator Loss: -607534.6250\n",
      "[games.csv] Epoch 70/300 | Critic Loss: -6899740.5000 | Generator Loss: -902128.0625\n",
      "[games.csv] Epoch 80/300 | Critic Loss: -6264729.0000 | Generator Loss: -1211364.7500\n",
      "[games.csv] Epoch 90/300 | Critic Loss: -6226793.5000 | Generator Loss: -1767550.5000\n",
      "[games.csv] Epoch 100/300 | Critic Loss: -5446475.5000 | Generator Loss: -2187105.5000\n",
      "[games.csv] Epoch 110/300 | Critic Loss: -4845916.0000 | Generator Loss: -2678300.2500\n",
      "[games.csv] Epoch 120/300 | Critic Loss: -3995768.2500 | Generator Loss: -2940709.5000\n",
      "[games.csv] Epoch 130/300 | Critic Loss: -3702144.7500 | Generator Loss: -3769214.5000\n",
      "[games.csv] Epoch 140/300 | Critic Loss: -2399135.2500 | Generator Loss: -3438935.5000\n",
      "[games.csv] Epoch 150/300 | Critic Loss: -1572571.0000 | Generator Loss: -3650752.5000\n",
      "[games.csv] Epoch 160/300 | Critic Loss: -1342958.0000 | Generator Loss: -2927942.5000\n",
      "[games.csv] Epoch 170/300 | Critic Loss: -553035.0000 | Generator Loss: -2038601.8750\n",
      "[games.csv] Epoch 180/300 | Critic Loss: -309674.7500 | Generator Loss: -888674.3750\n",
      "[games.csv] Epoch 190/300 | Critic Loss: -368711.1562 | Generator Loss: -25488.6621\n",
      "[games.csv] Epoch 200/300 | Critic Loss: -289136.4688 | Generator Loss: 18527.9219\n",
      "[games.csv] Epoch 210/300 | Critic Loss: -286070.4062 | Generator Loss: 409001.1250\n",
      "[games.csv] Epoch 220/300 | Critic Loss: -433106.3125 | Generator Loss: 541624.8750\n",
      "[games.csv] Epoch 230/300 | Critic Loss: -702305.0000 | Generator Loss: -93568.1562\n",
      "[games.csv] Epoch 240/300 | Critic Loss: -1164983.5000 | Generator Loss: -1091773.3750\n",
      "[games.csv] Epoch 250/300 | Critic Loss: -1200964.5000 | Generator Loss: 29139.7793\n",
      "[games.csv] Epoch 260/300 | Critic Loss: -1505681.6250 | Generator Loss: 35807.5703\n",
      "[games.csv] Epoch 270/300 | Critic Loss: -1593882.1250 | Generator Loss: 63177.8711\n",
      "[games.csv] Epoch 280/300 | Critic Loss: -1804741.3750 | Generator Loss: 41541.2070\n",
      "[games.csv] Epoch 290/300 | Critic Loss: -1646876.0000 | Generator Loss: 40242.4375\n",
      "[games.csv] Epoch 300/300 | Critic Loss: -1634941.5000 | Generator Loss: -21222.2227\n",
      "\n",
      "üîÅ Starting round 1 (Shuffle: No)\n",
      "\n",
      "üß™ Training on Chunk 1/5 of Round 1 (Rows 0 to 400)\n",
      "RandomForest - Accuracy: 0.7500\n",
      "MLPClassifier - Accuracy: 0.4000\n",
      "LogisticRegression - Accuracy: 0.7500\n",
      "XGBoost - Accuracy: 0.8125\n",
      "\n",
      "üß™ Training on Chunk 2/5 of Round 1 (Rows 400 to 800)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:25:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:25:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Accuracy: 0.8125\n",
      "MLPClassifier - Accuracy: 0.3125\n",
      "LogisticRegression - Accuracy: 0.7750\n",
      "XGBoost - Accuracy: 0.7875\n",
      "\n",
      "üß™ Training on Chunk 3/5 of Round 1 (Rows 800 to 1200)\n",
      "RandomForest - Accuracy: 0.7750\n",
      "MLPClassifier - Accuracy: 0.2875\n",
      "LogisticRegression - Accuracy: 0.7375\n",
      "XGBoost - Accuracy: 0.7625\n",
      "\n",
      "üß™ Training on Chunk 4/5 of Round 1 (Rows 1200 to 1600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:25:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:25:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Accuracy: 0.8625\n",
      "MLPClassifier - Accuracy: 0.3625\n",
      "LogisticRegression - Accuracy: 0.6625\n",
      "XGBoost - Accuracy: 0.8125\n",
      "\n",
      "üß™ Training on Chunk 5/5 of Round 1 (Rows 1600 to 2000)\n",
      "RandomForest - Accuracy: 0.7375\n",
      "MLPClassifier - Accuracy: 0.3750\n",
      "LogisticRegression - Accuracy: 0.6875\n",
      "XGBoost - Accuracy: 0.7250\n",
      "\n",
      "üîÅ Starting round 2 (Shuffle: Yes)\n",
      "\n",
      "üß™ Training on Chunk 1/5 of Round 2 (Rows 0 to 400)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:25:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:25:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Accuracy: 0.7375\n",
      "MLPClassifier - Accuracy: 0.3375\n",
      "LogisticRegression - Accuracy: 0.6250\n",
      "XGBoost - Accuracy: 0.7500\n",
      "\n",
      "üß™ Training on Chunk 2/5 of Round 2 (Rows 400 to 800)\n",
      "RandomForest - Accuracy: 0.7000\n",
      "MLPClassifier - Accuracy: 0.2750\n",
      "LogisticRegression - Accuracy: 0.6375\n",
      "XGBoost - Accuracy: 0.7250\n",
      "\n",
      "üß™ Training on Chunk 3/5 of Round 2 (Rows 800 to 1200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:25:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:25:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Accuracy: 0.8500\n",
      "MLPClassifier - Accuracy: 0.4875\n",
      "LogisticRegression - Accuracy: 0.7125\n",
      "XGBoost - Accuracy: 0.7750\n",
      "\n",
      "üß™ Training on Chunk 4/5 of Round 2 (Rows 1200 to 1600)\n",
      "RandomForest - Accuracy: 0.8625\n",
      "MLPClassifier - Accuracy: 0.3625\n",
      "LogisticRegression - Accuracy: 0.7250\n",
      "XGBoost - Accuracy: 0.8500\n",
      "\n",
      "üß™ Training on Chunk 5/5 of Round 2 (Rows 1600 to 2000)\n",
      "RandomForest - Accuracy: 0.8125\n",
      "MLPClassifier - Accuracy: 0.3625\n",
      "LogisticRegression - Accuracy: 0.6625\n",
      "XGBoost - Accuracy: 0.7875\n",
      "\n",
      "üìä Final Average Accuracy on SYNTHETIC data:\n",
      "RandomForest: Average Accuracy = 0.7900\n",
      "MLPClassifier: Average Accuracy = 0.3562\n",
      "LogisticRegression: Average Accuracy = 0.6975\n",
      "XGBoost: Average Accuracy = 0.7787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:25:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [13:25:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"games.csv\"\n",
    "target = \"winner\"  # Best choice for this dataset\n",
    "\n",
    "run_cwgan_pipeline(csv_path, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[default_of_credit_card_clients.csv] Epoch 10/300 | Critic Loss: -4020330.0000 | Generator Loss: -5117.8750\n",
      "[default_of_credit_card_clients.csv] Epoch 20/300 | Critic Loss: -15762214.0000 | Generator Loss: -129634.2031\n",
      "[default_of_credit_card_clients.csv] Epoch 30/300 | Critic Loss: -16205158.0000 | Generator Loss: -440307.1875\n",
      "[default_of_credit_card_clients.csv] Epoch 40/300 | Critic Loss: -17066320.0000 | Generator Loss: -871953.1875\n",
      "[default_of_credit_card_clients.csv] Epoch 50/300 | Critic Loss: -18630356.0000 | Generator Loss: -1609416.0000\n",
      "[default_of_credit_card_clients.csv] Epoch 60/300 | Critic Loss: -14170787.0000 | Generator Loss: -2604200.7500\n",
      "[default_of_credit_card_clients.csv] Epoch 70/300 | Critic Loss: -16407600.0000 | Generator Loss: -3714174.5000\n",
      "[default_of_credit_card_clients.csv] Epoch 80/300 | Critic Loss: -13547940.0000 | Generator Loss: -4924727.0000\n",
      "[default_of_credit_card_clients.csv] Epoch 90/300 | Critic Loss: -10586430.0000 | Generator Loss: -5643571.0000\n",
      "[default_of_credit_card_clients.csv] Epoch 100/300 | Critic Loss: -5488232.0000 | Generator Loss: -7336805.0000\n",
      "[default_of_credit_card_clients.csv] Epoch 110/300 | Critic Loss: -5532743.0000 | Generator Loss: -7661354.0000\n",
      "[default_of_credit_card_clients.csv] Epoch 120/300 | Critic Loss: -155927.0000 | Generator Loss: -7521015.5000\n",
      "[default_of_credit_card_clients.csv] Epoch 130/300 | Critic Loss: -2169469.0000 | Generator Loss: -5735309.0000\n",
      "[default_of_credit_card_clients.csv] Epoch 140/300 | Critic Loss: -986871.5000 | Generator Loss: -2976674.7500\n",
      "[default_of_credit_card_clients.csv] Epoch 150/300 | Critic Loss: -1087632.1250 | Generator Loss: -464313.8750\n",
      "[default_of_credit_card_clients.csv] Epoch 160/300 | Critic Loss: -2301839.0000 | Generator Loss: -62782.2500\n",
      "[default_of_credit_card_clients.csv] Epoch 170/300 | Critic Loss: -720303.7500 | Generator Loss: -6464.6348\n",
      "[default_of_credit_card_clients.csv] Epoch 180/300 | Critic Loss: -1042434.5000 | Generator Loss: -1386.8881\n",
      "[default_of_credit_card_clients.csv] Epoch 190/300 | Critic Loss: -2045998.7500 | Generator Loss: -5845.6758\n",
      "[default_of_credit_card_clients.csv] Epoch 200/300 | Critic Loss: -1840175.7500 | Generator Loss: 1692291.2500\n",
      "[default_of_credit_card_clients.csv] Epoch 210/300 | Critic Loss: -2352019.2500 | Generator Loss: 3525787.0000\n",
      "[default_of_credit_card_clients.csv] Epoch 220/300 | Critic Loss: -2936019.5000 | Generator Loss: -1054487.3750\n",
      "[default_of_credit_card_clients.csv] Epoch 230/300 | Critic Loss: -2355551.7500 | Generator Loss: -62802.9219\n",
      "[default_of_credit_card_clients.csv] Epoch 240/300 | Critic Loss: -1925105.5000 | Generator Loss: -17408.8086\n",
      "[default_of_credit_card_clients.csv] Epoch 250/300 | Critic Loss: -2673354.5000 | Generator Loss: -66923.9297\n",
      "[default_of_credit_card_clients.csv] Epoch 260/300 | Critic Loss: -2648383.2500 | Generator Loss: 1931.4142\n",
      "[default_of_credit_card_clients.csv] Epoch 270/300 | Critic Loss: -2426560.5000 | Generator Loss: -145243.2344\n",
      "[default_of_credit_card_clients.csv] Epoch 280/300 | Critic Loss: -2893777.2500 | Generator Loss: -202545.3750\n",
      "[default_of_credit_card_clients.csv] Epoch 290/300 | Critic Loss: -2378108.5000 | Generator Loss: -373759.6562\n",
      "[default_of_credit_card_clients.csv] Epoch 300/300 | Critic Loss: -1355857.8750 | Generator Loss: -399599.8125\n",
      "\n",
      "üîÅ Starting round 1 (Shuffle: No)\n",
      "\n",
      "üß™ Training on Chunk 1/5 of Round 1 (Rows 0 to 400)\n",
      "RandomForest - Accuracy: 0.9750\n",
      "MLPClassifier - Accuracy: 0.5125\n",
      "LogisticRegression - Accuracy: 0.9125\n",
      "XGBoost - Accuracy: 0.9625\n",
      "\n",
      "üß™ Training on Chunk 2/5 of Round 1 (Rows 400 to 800)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:21:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:21:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - Accuracy: 0.9875\n",
      "MLPClassifier - Accuracy: 0.5375\n",
      "LogisticRegression - Accuracy: 0.8750\n",
      "XGBoost - Accuracy: 0.9750\n",
      "\n",
      "üß™ Training on Chunk 3/5 of Round 1 (Rows 800 to 1200)\n",
      "RandomForest - Accuracy: 0.9625\n",
      "MLPClassifier - Accuracy: 0.4375\n",
      "LogisticRegression - Accuracy: 0.9000\n",
      "XGBoost - Accuracy: 0.9500\n",
      "\n",
      "üß™ Training on Chunk 4/5 of Round 1 (Rows 1200 to 1600)\n",
      "RandomForest - Accuracy: 0.9750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:21:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:21:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:21:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier - Accuracy: 0.5625\n",
      "LogisticRegression - Accuracy: 0.8500\n",
      "XGBoost - Accuracy: 0.9750\n",
      "\n",
      "üß™ Training on Chunk 5/5 of Round 1 (Rows 1600 to 2000)\n",
      "RandomForest - Accuracy: 0.9625\n",
      "MLPClassifier - Accuracy: 0.5000\n",
      "LogisticRegression - Accuracy: 0.9250\n",
      "XGBoost - Accuracy: 0.9625\n",
      "\n",
      "üîÅ Starting round 2 (Shuffle: Yes)\n",
      "\n",
      "üß™ Training on Chunk 1/5 of Round 2 (Rows 0 to 400)\n",
      "RandomForest - Accuracy: 0.9875\n",
      "MLPClassifier - Accuracy: 0.5375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:21:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:21:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:21:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression - Accuracy: 0.9250\n",
      "XGBoost - Accuracy: 0.9875\n",
      "\n",
      "üß™ Training on Chunk 2/5 of Round 2 (Rows 400 to 800)\n",
      "RandomForest - Accuracy: 0.9875\n",
      "MLPClassifier - Accuracy: 0.4625\n",
      "LogisticRegression - Accuracy: 0.9250\n",
      "XGBoost - Accuracy: 0.9875\n",
      "\n",
      "üß™ Training on Chunk 3/5 of Round 2 (Rows 800 to 1200)\n",
      "RandomForest - Accuracy: 0.9750\n",
      "MLPClassifier - Accuracy: 0.4875\n",
      "LogisticRegression - Accuracy: 0.9000\n",
      "XGBoost - Accuracy: 0.9500\n",
      "\n",
      "üß™ Training on Chunk 4/5 of Round 2 (Rows 1200 to 1600)\n",
      "RandomForest - Accuracy: 0.9875\n",
      "MLPClassifier - Accuracy: 0.4875\n",
      "LogisticRegression - Accuracy: 0.8250\n",
      "XGBoost - Accuracy: 0.9750\n",
      "\n",
      "üß™ Training on Chunk 5/5 of Round 2 (Rows 1600 to 2000)\n",
      "RandomForest - Accuracy: 0.9875\n",
      "MLPClassifier - Accuracy: 0.4250\n",
      "LogisticRegression - Accuracy: 0.9750\n",
      "XGBoost - Accuracy: 0.9875\n",
      "\n",
      "üìä Final Average Accuracy on SYNTHETIC data:\n",
      "RandomForest: Average Accuracy = 0.9788\n",
      "MLPClassifier: Average Accuracy = 0.4950\n",
      "LogisticRegression: Average Accuracy = 0.9012\n",
      "XGBoost: Average Accuracy = 0.9713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:21:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:21:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"default_of_credit_card_clients.csv\"\n",
    "target = \"default payment next month\"\n",
    "\n",
    "run_cwgan_pipeline(csv_path, target)\n",
    "# evaluate_on_real_data(csv_path, target)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
