{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd7f3f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing car dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:82: UserWarning: The model does not have any trainable weights.\n",
      "  warnings.warn(\"The model does not have any trainable weights.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5000] | D Loss: 0.0036 | G Loss: 0.0000\n",
      "Epoch [50/5000] | D Loss: 0.0023 | G Loss: -0.0000\n",
      "Epoch [100/5000] | D Loss: 0.0022 | G Loss: -0.0001\n",
      "Epoch [150/5000] | D Loss: 0.0023 | G Loss: -0.0001\n",
      "Epoch [200/5000] | D Loss: 0.0023 | G Loss: -0.0002\n",
      "Epoch [250/5000] | D Loss: 0.0023 | G Loss: -0.0002\n",
      "Epoch [300/5000] | D Loss: 0.0024 | G Loss: -0.0003\n",
      "Epoch [350/5000] | D Loss: 0.0025 | G Loss: -0.0004\n",
      "Epoch [400/5000] | D Loss: 0.0025 | G Loss: -0.0005\n",
      "Epoch [450/5000] | D Loss: 0.0026 | G Loss: -0.0007\n",
      "Epoch [500/5000] | D Loss: 0.0027 | G Loss: -0.0009\n",
      "Epoch [550/5000] | D Loss: 0.0028 | G Loss: -0.0011\n",
      "Epoch [600/5000] | D Loss: 0.0029 | G Loss: -0.0013\n",
      "Epoch [650/5000] | D Loss: 0.0031 | G Loss: -0.0016\n",
      "Epoch [700/5000] | D Loss: 0.0032 | G Loss: -0.0019\n",
      "Epoch [750/5000] | D Loss: 0.0034 | G Loss: -0.0022\n",
      "Epoch [800/5000] | D Loss: 0.0036 | G Loss: -0.0026\n",
      "Epoch [850/5000] | D Loss: 0.0038 | G Loss: -0.0030\n",
      "Epoch [900/5000] | D Loss: 0.0040 | G Loss: -0.0034\n",
      "Epoch [950/5000] | D Loss: 0.0043 | G Loss: -0.0039\n",
      "Epoch [1000/5000] | D Loss: 0.0046 | G Loss: -0.0044\n",
      "Epoch [1050/5000] | D Loss: 0.0049 | G Loss: -0.0050\n",
      "Epoch [1100/5000] | D Loss: 0.0052 | G Loss: -0.0057\n",
      "Epoch [1150/5000] | D Loss: 0.0056 | G Loss: -0.0064\n",
      "Epoch [1200/5000] | D Loss: 0.0060 | G Loss: -0.0071\n",
      "Epoch [1250/5000] | D Loss: 0.0064 | G Loss: -0.0079\n",
      "Epoch [1300/5000] | D Loss: 0.0068 | G Loss: -0.0088\n",
      "Epoch [1350/5000] | D Loss: 0.0073 | G Loss: -0.0097\n",
      "Epoch [1400/5000] | D Loss: 0.0078 | G Loss: -0.0107\n",
      "Epoch [1450/5000] | D Loss: 0.0084 | G Loss: -0.0119\n",
      "Epoch [1500/5000] | D Loss: 0.0090 | G Loss: -0.0131\n",
      "Epoch [1550/5000] | D Loss: 0.0097 | G Loss: -0.0144\n",
      "Epoch [1600/5000] | D Loss: 0.0103 | G Loss: -0.0157\n",
      "Epoch [1650/5000] | D Loss: 0.0111 | G Loss: -0.0171\n",
      "Epoch [1700/5000] | D Loss: 0.0119 | G Loss: -0.0186\n",
      "Epoch [1750/5000] | D Loss: 0.0127 | G Loss: -0.0202\n",
      "Epoch [1800/5000] | D Loss: 0.0135 | G Loss: -0.0218\n",
      "Epoch [1850/5000] | D Loss: 0.0144 | G Loss: -0.0236\n",
      "Epoch [1900/5000] | D Loss: 0.0153 | G Loss: -0.0255\n",
      "Epoch [1950/5000] | D Loss: 0.0164 | G Loss: -0.0276\n",
      "Epoch [2000/5000] | D Loss: 0.0175 | G Loss: -0.0297\n",
      "Epoch [2050/5000] | D Loss: 0.0187 | G Loss: -0.0320\n",
      "Epoch [2100/5000] | D Loss: 0.0199 | G Loss: -0.0343\n",
      "Epoch [2150/5000] | D Loss: 0.0211 | G Loss: -0.0368\n",
      "Epoch [2200/5000] | D Loss: 0.0224 | G Loss: -0.0394\n",
      "Epoch [2250/5000] | D Loss: 0.0238 | G Loss: -0.0422\n",
      "Epoch [2300/5000] | D Loss: 0.0252 | G Loss: -0.0450\n",
      "Epoch [2350/5000] | D Loss: 0.0267 | G Loss: -0.0479\n",
      "Epoch [2400/5000] | D Loss: 0.0283 | G Loss: -0.0510\n",
      "Epoch [2450/5000] | D Loss: 0.0299 | G Loss: -0.0541\n",
      "Epoch [2500/5000] | D Loss: 0.0316 | G Loss: -0.0575\n",
      "Epoch [2550/5000] | D Loss: 0.0334 | G Loss: -0.0610\n",
      "Epoch [2600/5000] | D Loss: 0.0352 | G Loss: -0.0647\n",
      "Epoch [2650/5000] | D Loss: 0.0372 | G Loss: -0.0685\n",
      "Epoch [2700/5000] | D Loss: 0.0392 | G Loss: -0.0724\n",
      "Epoch [2750/5000] | D Loss: 0.0413 | G Loss: -0.0766\n",
      "Epoch [2800/5000] | D Loss: 0.0435 | G Loss: -0.0809\n",
      "Epoch [2850/5000] | D Loss: 0.0457 | G Loss: -0.0854\n",
      "Epoch [2900/5000] | D Loss: 0.0481 | G Loss: -0.0901\n",
      "Epoch [2950/5000] | D Loss: 0.0506 | G Loss: -0.0950\n",
      "Epoch [3000/5000] | D Loss: 0.0531 | G Loss: -0.1000\n",
      "Epoch [3050/5000] | D Loss: 0.0556 | G Loss: -0.1050\n",
      "Epoch [3100/5000] | D Loss: 0.0583 | G Loss: -0.1104\n",
      "Epoch [3150/5000] | D Loss: 0.0612 | G Loss: -0.1161\n",
      "Epoch [3200/5000] | D Loss: 0.0641 | G Loss: -0.1219\n",
      "Epoch [3250/5000] | D Loss: 0.0671 | G Loss: -0.1279\n",
      "Epoch [3300/5000] | D Loss: 0.0703 | G Loss: -0.1340\n",
      "Epoch [3350/5000] | D Loss: 0.0736 | G Loss: -0.1407\n",
      "Epoch [3400/5000] | D Loss: 0.0770 | G Loss: -0.1474\n",
      "Epoch [3450/5000] | D Loss: 0.0804 | G Loss: -0.1541\n",
      "Epoch [3500/5000] | D Loss: 0.0840 | G Loss: -0.1613\n",
      "Epoch [3550/5000] | D Loss: 0.0876 | G Loss: -0.1685\n",
      "Epoch [3600/5000] | D Loss: 0.0914 | G Loss: -0.1760\n",
      "Epoch [3650/5000] | D Loss: 0.0953 | G Loss: -0.1837\n",
      "Epoch [3700/5000] | D Loss: 0.0994 | G Loss: -0.1918\n",
      "Epoch [3750/5000] | D Loss: 0.1036 | G Loss: -0.2001\n",
      "Epoch [3800/5000] | D Loss: 0.1079 | G Loss: -0.2086\n",
      "Epoch [3850/5000] | D Loss: 0.1123 | G Loss: -0.2174\n",
      "Epoch [3900/5000] | D Loss: 0.1169 | G Loss: -0.2266\n",
      "Epoch [3950/5000] | D Loss: 0.1217 | G Loss: -0.2361\n",
      "Epoch [4000/5000] | D Loss: 0.1266 | G Loss: -0.2458\n",
      "Epoch [4050/5000] | D Loss: 0.1316 | G Loss: -0.2557\n",
      "Epoch [4100/5000] | D Loss: 0.1367 | G Loss: -0.2659\n",
      "Epoch [4150/5000] | D Loss: 0.1420 | G Loss: -0.2765\n",
      "Epoch [4200/5000] | D Loss: 0.1474 | G Loss: -0.2871\n",
      "Epoch [4250/5000] | D Loss: 0.1529 | G Loss: -0.2981\n",
      "Epoch [4300/5000] | D Loss: 0.1588 | G Loss: -0.3097\n",
      "Epoch [4350/5000] | D Loss: 0.1648 | G Loss: -0.3216\n",
      "Epoch [4400/5000] | D Loss: 0.1708 | G Loss: -0.3336\n",
      "Epoch [4450/5000] | D Loss: 0.1770 | G Loss: -0.3460\n",
      "Epoch [4500/5000] | D Loss: 0.1833 | G Loss: -0.3585\n",
      "Epoch [4550/5000] | D Loss: 0.1898 | G Loss: -0.3714\n",
      "Epoch [4600/5000] | D Loss: 0.1964 | G Loss: -0.3845\n",
      "Epoch [4650/5000] | D Loss: 0.2034 | G Loss: -0.3986\n",
      "Epoch [4700/5000] | D Loss: 0.2106 | G Loss: -0.4128\n",
      "Epoch [4750/5000] | D Loss: 0.2181 | G Loss: -0.4277\n",
      "Epoch [4800/5000] | D Loss: 0.2256 | G Loss: -0.4427\n",
      "Epoch [4850/5000] | D Loss: 0.2333 | G Loss: -0.4581\n",
      "Epoch [4900/5000] | D Loss: 0.2410 | G Loss: -0.4734\n",
      "Epoch [4950/5000] | D Loss: 0.2490 | G Loss: -0.4895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:03:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Evaluation Results on Real Test Set using GAN-Synthetic Data:\n",
      "       Accuracy  Precision    Recall  F1 Score\n",
      "Model                                         \n",
      "LR     0.031792   0.001011  0.031792  0.001959\n",
      "MLP    0.170520   0.724697  0.170520  0.243441\n",
      "RF     0.031792   0.001011  0.031792  0.001959\n",
      "XGBT   0.031792   0.001011  0.031792  0.001959\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Embedding, multiply, LeakyReLU, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# ---------------------- PREPROCESSING ----------------------\n",
    "print(\"Loading and preprocessing car dataset...\")\n",
    "data = pd.read_csv(\"car.csv\")  # Update path as needed\n",
    "target_column = 'Class'\n",
    "\n",
    "# Label encode all categorical columns including target\n",
    "encoders = {}\n",
    "df_encoded = data.copy()\n",
    "for col in df_encoded.columns:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "    encoders[col] = le\n",
    "\n",
    "X = df_encoded.drop(columns=[target_column])\n",
    "y = df_encoded[target_column]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ---------------------- IMPROVED WCGAN-GP MODEL ----------------------\n",
    "class ImprovedWCGANGP:\n",
    "    def __init__(self, data_dim, num_classes, latent_dim=32):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.data_dim = data_dim\n",
    "        self.num_classes = num_classes\n",
    "        optimizer = Adam(0.0001, beta_1=0.5, beta_2=0.9)\n",
    "        \n",
    "        self.generator = self.build_generator()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "        self.critic.compile(loss=self.wasserstein_loss, optimizer=optimizer)\n",
    "\n",
    "        self.critic.trainable = False\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        fake_data = self.generator([noise, label])\n",
    "        validity = self.critic([fake_data, label])\n",
    "        self.combined = Model([noise, label], validity)\n",
    "        self.combined.compile(loss=self.wasserstein_loss, optimizer=optimizer)\n",
    "\n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return tf.reduce_mean(y_true * y_pred)\n",
    "\n",
    "    def build_generator(self):\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
    "        model_input = multiply([noise, label_embedding])\n",
    "\n",
    "        x = Dense(64)(model_input)\n",
    "        x = LeakyReLU(negative_slope=0.2)(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = Dense(128)(x)\n",
    "        x = LeakyReLU(negative_slope=0.2)(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        output = Dense(self.data_dim, activation='linear')(x)\n",
    "\n",
    "        return Model([noise, label], output)\n",
    "\n",
    "    def build_critic(self):\n",
    "        data_input = Input(shape=(self.data_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.data_dim)(label))\n",
    "        model_input = multiply([data_input, label_embedding])\n",
    "\n",
    "        x = Dense(128)(model_input)\n",
    "        x = LeakyReLU(negative_slope=0.2)(x)\n",
    "        x = Dense(64)(x)\n",
    "        x = LeakyReLU(negative_slope=0.2)(x)\n",
    "        output = Dense(1, activation='linear')(x)\n",
    "\n",
    "        return Model([data_input, label], output)\n",
    "\n",
    "    def train(self, X_train, y_train, epochs=300, batch_size=64):\n",
    "        valid = -np.ones((batch_size, 1))\n",
    "        fake = np.ones((batch_size, 1))\n",
    "        for epoch in range(epochs):\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            real_samples, labels = X_train[idx], y_train.iloc[idx].values.reshape(-1, 1)\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_samples = self.generator.predict([noise, labels], verbose=0)\n",
    "\n",
    "            d_loss_real = self.critic.train_on_batch([real_samples, labels], valid)\n",
    "            d_loss_fake = self.critic.train_on_batch([gen_samples, labels], fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            g_loss = self.combined.train_on_batch([noise, labels], valid)\n",
    "\n",
    "            if epoch % 50 == 0:\n",
    "                print(f\"Epoch [{epoch}/{epochs}] | D Loss: {d_loss:.4f} | G Loss: {g_loss:.4f}\")\n",
    "\n",
    "\n",
    "# ---------------------- TRAIN THE GAN ----------------------\n",
    "wcgan = ImprovedWCGANGP(data_dim=X_train.shape[1], num_classes=len(np.unique(y_train)))\n",
    "wcgan.train(X_train, y_train, epochs=5000)\n",
    "\n",
    "# ---------------------- GENERATE SYNTHETIC DATA ----------------------\n",
    "def generate_samples(wcgan, n_samples):\n",
    "    noise = np.random.normal(0, 1, (n_samples, wcgan.latent_dim))\n",
    "    labels = np.random.randint(0, wcgan.num_classes, n_samples).reshape(-1, 1)\n",
    "    return wcgan.generator.predict([noise, labels], verbose=0), labels\n",
    "\n",
    "fake_samples, fake_labels = generate_samples(wcgan, X_train.shape[0])\n",
    "fake_df = pd.DataFrame(fake_samples)\n",
    "fake_labels = fake_labels.flatten()\n",
    "\n",
    "# ---------------------- EVALUATE CLASSIFIERS ----------------------\n",
    "models = {\n",
    "    'LR': LogisticRegression(max_iter=300),\n",
    "    'MLP': MLPClassifier(max_iter=300),\n",
    "    'RF': RandomForestClassifier(),\n",
    "    'XGBT': XGBClassifier(eval_metric='mlogloss', use_label_encoder=False)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(fake_df, fake_labels)\n",
    "    preds = model.predict(X_test)\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_test, preds),\n",
    "        \"Precision\": precision_score(y_test, preds, average='weighted', zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, preds, average='weighted'),\n",
    "        \"F1 Score\": f1_score(y_test, preds, average='weighted')\n",
    "    })\n",
    "\n",
    "# ---------------------- DISPLAY RESULTS ----------------------\n",
    "results_df = pd.DataFrame(results).set_index(\"Model\")\n",
    "print(\"\\nðŸ“Š Evaluation Results on Real Test Set using GAN-Synthetic Data:\")\n",
    "print(results_df.round(6).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45488727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/.pyenv/versions/3.9.18/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:82: UserWarning: The model does not have any trainable weights.\n",
      "  warnings.warn(\"The model does not have any trainable weights.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5000] | D Loss: 0.0013 | G Loss: 0.0000\n",
      "Epoch [100/5000] | D Loss: 0.0009 | G Loss: 0.0000\n",
      "Epoch [200/5000] | D Loss: 0.0009 | G Loss: 0.0000\n",
      "Epoch [300/5000] | D Loss: 0.0009 | G Loss: 0.0000\n",
      "Epoch [400/5000] | D Loss: 0.0009 | G Loss: 0.0000\n",
      "Epoch [500/5000] | D Loss: 0.0009 | G Loss: 0.0000\n",
      "Epoch [600/5000] | D Loss: 0.0009 | G Loss: 0.0000\n",
      "Epoch [700/5000] | D Loss: 0.0009 | G Loss: 0.0000\n",
      "Epoch [800/5000] | D Loss: 0.0009 | G Loss: 0.0000\n",
      "Epoch [900/5000] | D Loss: 0.0009 | G Loss: -0.0000\n",
      "Epoch [1000/5000] | D Loss: 0.0009 | G Loss: -0.0000\n",
      "Epoch [1100/5000] | D Loss: 0.0009 | G Loss: -0.0000\n",
      "Epoch [1200/5000] | D Loss: 0.0009 | G Loss: -0.0000\n",
      "Epoch [1300/5000] | D Loss: 0.0009 | G Loss: -0.0001\n",
      "Epoch [1400/5000] | D Loss: 0.0010 | G Loss: -0.0003\n",
      "Epoch [1500/5000] | D Loss: 0.0012 | G Loss: -0.0005\n",
      "Epoch [1600/5000] | D Loss: 0.0014 | G Loss: -0.0011\n",
      "Epoch [1700/5000] | D Loss: 0.0018 | G Loss: -0.0019\n",
      "Epoch [1800/5000] | D Loss: 0.0025 | G Loss: -0.0031\n",
      "Epoch [1900/5000] | D Loss: 0.0034 | G Loss: -0.0049\n",
      "Epoch [2000/5000] | D Loss: 0.0047 | G Loss: -0.0076\n",
      "Epoch [2100/5000] | D Loss: 0.0064 | G Loss: -0.0110\n",
      "Epoch [2200/5000] | D Loss: 0.0088 | G Loss: -0.0157\n",
      "Epoch [2300/5000] | D Loss: 0.0119 | G Loss: -0.0220\n",
      "Epoch [2400/5000] | D Loss: 0.0160 | G Loss: -0.0300\n",
      "Epoch [2500/5000] | D Loss: 0.0213 | G Loss: -0.0408\n",
      "Epoch [2600/5000] | D Loss: 0.0279 | G Loss: -0.0539\n",
      "Epoch [2700/5000] | D Loss: 0.0355 | G Loss: -0.0690\n",
      "Epoch [2800/5000] | D Loss: 0.0451 | G Loss: -0.0881\n",
      "Epoch [2900/5000] | D Loss: 0.0567 | G Loss: -0.1113\n",
      "Epoch [3000/5000] | D Loss: 0.0704 | G Loss: -0.1386\n",
      "Epoch [3100/5000] | D Loss: 0.0864 | G Loss: -0.1703\n",
      "Epoch [3200/5000] | D Loss: 0.1051 | G Loss: -0.2078\n",
      "Epoch [3300/5000] | D Loss: 0.1276 | G Loss: -0.2532\n",
      "Epoch [3400/5000] | D Loss: 0.1522 | G Loss: -0.3026\n",
      "Epoch [3500/5000] | D Loss: 0.1817 | G Loss: -0.3619\n",
      "Epoch [3600/5000] | D Loss: 0.2144 | G Loss: -0.4273\n",
      "Epoch [3700/5000] | D Loss: 0.2517 | G Loss: -0.5018\n",
      "Epoch [3800/5000] | D Loss: 0.2951 | G Loss: -0.5884\n",
      "Epoch [3900/5000] | D Loss: 0.3460 | G Loss: -0.6906\n",
      "Epoch [4000/5000] | D Loss: 0.3993 | G Loss: -0.7960\n",
      "Epoch [4100/5000] | D Loss: 0.4624 | G Loss: -0.9225\n",
      "Epoch [4200/5000] | D Loss: 0.5338 | G Loss: -1.0652\n",
      "Epoch [4300/5000] | D Loss: 0.6062 | G Loss: -1.2089\n",
      "Epoch [4400/5000] | D Loss: 0.6914 | G Loss: -1.3800\n",
      "Epoch [4500/5000] | D Loss: 0.7889 | G Loss: -1.5750\n",
      "Epoch [4600/5000] | D Loss: 0.8947 | G Loss: -1.7873\n",
      "Epoch [4700/5000] | D Loss: 1.0024 | G Loss: -2.0023\n",
      "Epoch [4800/5000] | D Loss: 1.1282 | G Loss: -2.2552\n",
      "Epoch [4900/5000] | D Loss: 1.2632 | G Loss: -2.5263\n",
      "\n",
      "--- Structured Evaluation Results (Synthetic Data) ---\n",
      "Model  Accuracy  Precision   Recall  F1 Score\n",
      "   LR  0.601156   0.180366 0.264603  0.210095\n",
      "  MLP  0.592486   0.175576 0.239749  0.200867\n",
      "   RF  0.187861   0.205487 0.285783  0.107586\n",
      " XGBT  0.476879   0.178571 0.175532  0.177039\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Embedding, multiply, LeakyReLU, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# ---------------------- PREPROCESSING ----------------------\n",
    "print(\"Loading and preprocessing dataset...\")\n",
    "data = pd.read_csv(\"car.csv\")  # Update path as needed\n",
    "\n",
    "# Encode all columns\n",
    "label_encoders = {}\n",
    "for col in data.columns:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "X = data.drop(\"Class\", axis=1)\n",
    "y = data[\"Class\"]\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ---------------------- WCGAN-GP MODEL ----------------------\n",
    "class ImprovedWCGANGP:\n",
    "    def __init__(self, data_dim, num_classes, latent_dim=32):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.data_dim = data_dim\n",
    "        self.num_classes = num_classes\n",
    "        optimizer = Adam(0.0001, beta_1=0.5, beta_2=0.9)\n",
    "\n",
    "        self.generator = self.build_generator()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "        self.critic.compile(loss=self.wasserstein_loss, optimizer=optimizer)\n",
    "\n",
    "        self.critic.trainable = False\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        fake_data = self.generator([noise, label])\n",
    "        validity = self.critic([fake_data, label])\n",
    "        self.combined = Model([noise, label], validity)\n",
    "        self.combined.compile(loss=self.wasserstein_loss, optimizer=optimizer)\n",
    "\n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return tf.reduce_mean(y_true * y_pred)\n",
    "\n",
    "    def build_generator(self):\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
    "        model_input = multiply([noise, label_embedding])\n",
    "\n",
    "        x = Dense(128)(model_input)\n",
    "        x = LeakyReLU(negative_slope=0.2)(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = Dense(256)(x)\n",
    "        x = LeakyReLU(negative_slope=0.2)(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        x = Dense(512)(x)\n",
    "        x = LeakyReLU(negative_slope=0.2)(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        output = Dense(self.data_dim, activation='linear')(x)\n",
    "\n",
    "        return Model([noise, label], output)\n",
    "\n",
    "    def build_critic(self):\n",
    "        data_input = Input(shape=(self.data_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.data_dim)(label))\n",
    "        model_input = multiply([data_input, label_embedding])\n",
    "\n",
    "        x = Dense(512)(model_input)\n",
    "        x = LeakyReLU(negative_slope=0.2)(x)\n",
    "        x = Dense(256)(x)\n",
    "        x = LeakyReLU(negative_slope=0.2)(x)\n",
    "        x = Dense(128)(x)\n",
    "        x = LeakyReLU(negative_slope=0.2)(x)\n",
    "        output = Dense(1, activation='linear')(x)\n",
    "\n",
    "        return Model([data_input, label], output)\n",
    "\n",
    "    def train(self, X_train, y_train, epochs=1000, batch_size=64):\n",
    "        valid = -np.ones((batch_size, 1))\n",
    "        fake = np.ones((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            real_samples, labels = X_train[idx], y_train.iloc[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_samples = self.generator.predict([noise, labels], verbose=0)\n",
    "\n",
    "            d_loss_real = self.critic.train_on_batch([real_samples, labels], valid)\n",
    "            d_loss_fake = self.critic.train_on_batch([gen_samples, labels], fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            g_loss = self.combined.train_on_batch([noise, labels], valid)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch [{epoch}/{epochs}] | D Loss: {d_loss:.4f} | G Loss: {g_loss:.4f}\")\n",
    "\n",
    "# Instantiate and train\n",
    "wcgan = ImprovedWCGANGP(data_dim=X_train.shape[1], num_classes=num_classes)\n",
    "wcgan.train(X_train, y_train, epochs=5000)\n",
    "\n",
    "# ---------------------- GENERATE SYNTHETIC DATA ----------------------\n",
    "def generate_samples(wcgan, n_samples):\n",
    "    noise = np.random.normal(0, 1, (n_samples, wcgan.latent_dim))\n",
    "    labels = np.random.randint(0, wcgan.num_classes, n_samples).reshape(-1, 1)\n",
    "    return wcgan.generator.predict([noise, labels], verbose=0), labels\n",
    "\n",
    "fake_samples, fake_labels = generate_samples(wcgan, X_train.shape[0])\n",
    "\n",
    "# ---------------------- EVALUATION ----------------------\n",
    "models = {\n",
    "    'LR': LogisticRegression(max_iter=200),\n",
    "    'MLP': MLPClassifier(max_iter=200),\n",
    "    'RF': RandomForestClassifier(),\n",
    "    'XGBT': XGBClassifier(eval_metric='logloss')\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(fake_samples, fake_labels.ravel())\n",
    "    preds = model.predict(X_test)\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_test, preds),\n",
    "        \"Precision\": precision_score(y_test, preds, average='macro', zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, preds, average='macro'),\n",
    "        \"F1 Score\": f1_score(y_test, preds, average='macro')\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n--- Structured Evaluation Results (Synthetic Data) ---\")\n",
    "print(results_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
