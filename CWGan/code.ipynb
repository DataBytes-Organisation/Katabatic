{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/50] Critic Loss: 1.3351 Generator Loss: -0.0850\n",
      "[Epoch 2/50] Critic Loss: -2.6747 Generator Loss: -0.3967\n",
      "[Epoch 3/50] Critic Loss: -3.0656 Generator Loss: -0.6185\n",
      "[Epoch 4/50] Critic Loss: -3.3641 Generator Loss: -0.8219\n",
      "[Epoch 5/50] Critic Loss: -3.0061 Generator Loss: -1.2652\n",
      "[Epoch 6/50] Critic Loss: -2.7717 Generator Loss: -1.6896\n",
      "[Epoch 7/50] Critic Loss: -2.4865 Generator Loss: -1.9120\n",
      "[Epoch 8/50] Critic Loss: -2.5064 Generator Loss: -1.9676\n",
      "[Epoch 9/50] Critic Loss: -2.2320 Generator Loss: -1.6823\n",
      "[Epoch 10/50] Critic Loss: -2.1994 Generator Loss: -1.1902\n",
      "[Epoch 11/50] Critic Loss: -1.9109 Generator Loss: -0.7154\n",
      "[Epoch 12/50] Critic Loss: -2.0394 Generator Loss: -0.3772\n",
      "[Epoch 13/50] Critic Loss: -2.2471 Generator Loss: -0.1546\n",
      "[Epoch 14/50] Critic Loss: -2.0377 Generator Loss: -0.3199\n",
      "[Epoch 15/50] Critic Loss: -1.9519 Generator Loss: 0.0023\n",
      "[Epoch 16/50] Critic Loss: -1.8826 Generator Loss: -0.0470\n",
      "[Epoch 17/50] Critic Loss: -2.0388 Generator Loss: -0.0342\n",
      "[Epoch 18/50] Critic Loss: -1.7803 Generator Loss: -0.0530\n",
      "[Epoch 19/50] Critic Loss: -1.8018 Generator Loss: -0.0768\n",
      "[Epoch 20/50] Critic Loss: -1.5385 Generator Loss: -0.0931\n",
      "[Epoch 21/50] Critic Loss: -1.8673 Generator Loss: -0.1234\n",
      "[Epoch 22/50] Critic Loss: -1.6142 Generator Loss: -0.1258\n",
      "[Epoch 23/50] Critic Loss: -1.6316 Generator Loss: -0.1037\n",
      "[Epoch 24/50] Critic Loss: -1.4063 Generator Loss: -0.1671\n",
      "[Epoch 25/50] Critic Loss: -1.3236 Generator Loss: -0.2086\n",
      "[Epoch 26/50] Critic Loss: -1.1320 Generator Loss: -0.4883\n",
      "[Epoch 27/50] Critic Loss: -1.3392 Generator Loss: -0.5307\n",
      "[Epoch 28/50] Critic Loss: -0.8591 Generator Loss: -0.3888\n",
      "[Epoch 29/50] Critic Loss: -0.8409 Generator Loss: -0.3875\n",
      "[Epoch 30/50] Critic Loss: -0.5851 Generator Loss: -0.7771\n",
      "[Epoch 31/50] Critic Loss: -0.8220 Generator Loss: -0.6315\n",
      "[Epoch 32/50] Critic Loss: -0.5272 Generator Loss: -0.5890\n",
      "[Epoch 33/50] Critic Loss: -0.4253 Generator Loss: -0.6394\n",
      "[Epoch 34/50] Critic Loss: -0.3289 Generator Loss: -0.3796\n",
      "[Epoch 35/50] Critic Loss: -0.6898 Generator Loss: 0.3159\n",
      "[Epoch 36/50] Critic Loss: -0.6272 Generator Loss: 0.1142\n",
      "[Epoch 37/50] Critic Loss: -0.3290 Generator Loss: 0.5755\n",
      "[Epoch 38/50] Critic Loss: -0.5812 Generator Loss: 1.7511\n",
      "[Epoch 39/50] Critic Loss: -0.4550 Generator Loss: 2.4872\n",
      "[Epoch 40/50] Critic Loss: -0.5668 Generator Loss: 2.4163\n",
      "[Epoch 41/50] Critic Loss: -0.8889 Generator Loss: 1.6612\n",
      "[Epoch 42/50] Critic Loss: -0.5169 Generator Loss: 2.4694\n",
      "[Epoch 43/50] Critic Loss: -0.2988 Generator Loss: 2.0531\n",
      "[Epoch 44/50] Critic Loss: -0.6653 Generator Loss: 2.4418\n",
      "[Epoch 45/50] Critic Loss: -0.5395 Generator Loss: 2.5992\n",
      "[Epoch 46/50] Critic Loss: -0.4531 Generator Loss: 1.7468\n",
      "[Epoch 47/50] Critic Loss: -0.3617 Generator Loss: 1.9500\n",
      "[Epoch 48/50] Critic Loss: -0.5207 Generator Loss: 1.4280\n",
      "[Epoch 49/50] Critic Loss: -0.1059 Generator Loss: 0.8099\n",
      "[Epoch 50/50] Critic Loss: 0.2515 Generator Loss: 0.8099\n",
      "\n",
      "Generated nursery records for class 'recommend':\n",
      "    parents  has_nurs      form  children   housing   finance    social  \\\n",
      "0  0.930544  2.192229  2.615144 -0.242560  0.846757  0.853387  0.232867   \n",
      "1  1.516846  1.522317  2.214849  2.036173  1.939848  0.041576  0.647442   \n",
      "2  1.375268  1.295876  1.493320  0.632434  1.087532  0.623040  0.494012   \n",
      "3  0.942236  3.233055  1.245525  0.209702  0.743847  1.358838 -0.171046   \n",
      "4  0.920635  4.235880  2.715041  1.133975  1.528550  0.542989  0.710185   \n",
      "\n",
      "     health  \n",
      "0  0.928186  \n",
      "1  0.014998  \n",
      "2  0.775870  \n",
      "3  0.470782  \n",
      "4  0.247423  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"nursery.csv\")\n",
    "\n",
    "# Encode categorical features\n",
    "df_encoded = df.copy()\n",
    "encoders = {}\n",
    "for col in df_encoded.columns:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "    encoders[col] = le\n",
    "\n",
    "X = df_encoded.drop(columns=[\"Target\"])\n",
    "y = df_encoded[\"Target\"]\n",
    "num_classes = y.nunique()\n",
    "input_dim = X.shape[1]\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lambda_gp = 10\n",
    "epochs = 50\n",
    "\n",
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):  # Fixed: Changed init to _init_\n",
    "        super().__init__()  # Fixed: Changed super().init() to super()._init_()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + num_classes, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat((z, c), dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "# Critic\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):  # Fixed: Changed init to _init_\n",
    "        super().__init__()  # Fixed: Changed super().init() to super()._init_()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim + num_classes, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        d_in = torch.cat((x, c), dim=1)\n",
    "        return self.model(d_in)\n",
    "\n",
    "# Gradient penalty\n",
    "def compute_gp(critic, real_samples, fake_samples, labels):\n",
    "    alpha = torch.rand(real_samples.size(0), 1).to(device)\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolates = critic(interpolates, labels)\n",
    "    fake = torch.ones_like(d_interpolates)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    return ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator().to(device)\n",
    "critic = Critic().to(device)\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "optimizer_C = torch.optim.Adam(critic.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i, (real_samples, labels) in enumerate(loader):\n",
    "        real_samples = real_samples.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Train Critic\n",
    "        optimizer_C.zero_grad()\n",
    "        z = torch.randn(real_samples.size(0), latent_dim).to(device)\n",
    "        fake_samples = generator(z, labels)\n",
    "        real_validity = critic(real_samples, labels)\n",
    "        fake_validity = critic(fake_samples.detach(), labels)\n",
    "        gp = compute_gp(critic, real_samples.data, fake_samples.data, labels)\n",
    "        c_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gp\n",
    "        c_loss.backward()\n",
    "        optimizer_C.step()\n",
    "\n",
    "        # Train Generator every 5 steps\n",
    "        if i % 5 == 0:\n",
    "            optimizer_G.zero_grad()\n",
    "            gen_samples = generator(z, labels)\n",
    "            g_loss = -torch.mean(critic(gen_samples, labels))\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{epochs}] Critic Loss: {c_loss.item():.4f} Generator Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "# Example: generate 5 samples for class \"recommend\"\n",
    "label_value = encoders['Target'].transform(['recommend'])[0]\n",
    "z = torch.randn(5, latent_dim).to(device)\n",
    "labels = torch.full((5,), label_value, dtype=torch.long).to(device)\n",
    "generated = generator(z, labels).detach().cpu().numpy()\n",
    "\n",
    "# Convert the numeric values back to categorical\n",
    "generated_df = pd.DataFrame(generated, columns=X.columns)\n",
    "print(\"\\nGenerated nursery records for class 'recommend':\")\n",
    "print(generated_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoded generated records:\n",
      "       parents     has_nurs        form children    housing     finance  \\\n",
      "0  pretentious  less_proper  incomplete        1   critical      inconv   \n",
      "1        usual  less_proper      foster        3  less_conv  convenient   \n",
      "2  pretentious     improper   completed        2   critical      inconv   \n",
      "3  pretentious       proper   completed        1   critical      inconv   \n",
      "4  pretentious    very_crit  incomplete        2  less_conv      inconv   \n",
      "\n",
      "        social     health  \n",
      "0      nonprob   priority  \n",
      "1  problematic  not_recom  \n",
      "2      nonprob   priority  \n",
      "3      nonprob  not_recom  \n",
      "4  problematic  not_recom  \n"
     ]
    }
   ],
   "source": [
    "decoded_df = generated_df.copy()\n",
    "for col in decoded_df.columns:\n",
    "    # Round to nearest valid index for the encoder\n",
    "    valid_indices = np.arange(len(encoders[col].classes_))\n",
    "    rounded_values = np.round(decoded_df[col]).clip(min(valid_indices), max(valid_indices)).astype(int)\n",
    "    decoded_df[col] = encoders[col].inverse_transform(rounded_values)\n",
    "print(\"\\nDecoded generated records:\")\n",
    "print(decoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[letter-recognition-2.csv] Epoch 1/30 | Critic Loss: -7.6260 | Generator Loss: 0.0395\n",
      "[letter-recognition-2.csv] Epoch 2/30 | Critic Loss: -30.3727 | Generator Loss: -0.3738\n",
      "[letter-recognition-2.csv] Epoch 3/30 | Critic Loss: -36.1018 | Generator Loss: -0.6214\n",
      "[letter-recognition-2.csv] Epoch 4/30 | Critic Loss: -34.2304 | Generator Loss: -0.4963\n",
      "[letter-recognition-2.csv] Epoch 5/30 | Critic Loss: -33.0588 | Generator Loss: -2.4799\n",
      "[letter-recognition-2.csv] Epoch 6/30 | Critic Loss: -30.8468 | Generator Loss: -6.5201\n",
      "[letter-recognition-2.csv] Epoch 7/30 | Critic Loss: -28.8966 | Generator Loss: -10.5449\n",
      "[letter-recognition-2.csv] Epoch 8/30 | Critic Loss: -26.3214 | Generator Loss: -13.2584\n",
      "[letter-recognition-2.csv] Epoch 9/30 | Critic Loss: -23.3543 | Generator Loss: -15.0538\n",
      "[letter-recognition-2.csv] Epoch 10/30 | Critic Loss: -20.4524 | Generator Loss: -17.6229\n",
      "[letter-recognition-2.csv] Epoch 11/30 | Critic Loss: -16.1049 | Generator Loss: -18.1881\n",
      "[letter-recognition-2.csv] Epoch 12/30 | Critic Loss: -12.7245 | Generator Loss: -18.7232\n",
      "[letter-recognition-2.csv] Epoch 13/30 | Critic Loss: -9.6327 | Generator Loss: -15.6191\n",
      "[letter-recognition-2.csv] Epoch 14/30 | Critic Loss: -7.5550 | Generator Loss: -9.3661\n",
      "[letter-recognition-2.csv] Epoch 15/30 | Critic Loss: -7.5189 | Generator Loss: -0.7525\n",
      "[letter-recognition-2.csv] Epoch 16/30 | Critic Loss: -6.2878 | Generator Loss: 1.0732\n",
      "[letter-recognition-2.csv] Epoch 17/30 | Critic Loss: -6.5437 | Generator Loss: 1.8235\n",
      "[letter-recognition-2.csv] Epoch 18/30 | Critic Loss: -6.8122 | Generator Loss: 1.4839\n",
      "[letter-recognition-2.csv] Epoch 19/30 | Critic Loss: -7.3557 | Generator Loss: 1.9409\n",
      "[letter-recognition-2.csv] Epoch 20/30 | Critic Loss: -8.0245 | Generator Loss: 1.3676\n",
      "[letter-recognition-2.csv] Epoch 21/30 | Critic Loss: -7.6405 | Generator Loss: 1.0907\n",
      "[letter-recognition-2.csv] Epoch 22/30 | Critic Loss: -7.7600 | Generator Loss: 1.6206\n",
      "[letter-recognition-2.csv] Epoch 23/30 | Critic Loss: -6.5792 | Generator Loss: 0.6708\n",
      "[letter-recognition-2.csv] Epoch 24/30 | Critic Loss: -7.0472 | Generator Loss: 0.8637\n",
      "[letter-recognition-2.csv] Epoch 25/30 | Critic Loss: -6.9074 | Generator Loss: 0.7456\n",
      "[letter-recognition-2.csv] Epoch 26/30 | Critic Loss: -6.0973 | Generator Loss: 1.5221\n",
      "[letter-recognition-2.csv] Epoch 27/30 | Critic Loss: -7.0200 | Generator Loss: 3.0655\n",
      "[letter-recognition-2.csv] Epoch 28/30 | Critic Loss: -6.7777 | Generator Loss: 2.4234\n",
      "[letter-recognition-2.csv] Epoch 29/30 | Critic Loss: -6.3510 | Generator Loss: 4.0879\n",
      "[letter-recognition-2.csv] Epoch 30/30 | Critic Loss: -6.2837 | Generator Loss: 3.2818\n",
      "RandomForest: Accuracy = 0.6375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaychoudhari/Downloads/Katabatic_partB/Katabatic_partB/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/dhananjaychoudhari/Downloads/Katabatic_partB/Katabatic_partB/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dhananjaychoudhari/Downloads/Katabatic_partB/Katabatic_partB/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [11:39:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier: Accuracy = 0.8250\n",
      "LogisticRegression: Accuracy = 0.9050\n",
      "XGBoost: Accuracy = 0.6525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'RandomForest': 0.6375,\n",
       " 'MLPClassifier': 0.825,\n",
       " 'LogisticRegression': 0.905,\n",
       " 'XGBoost': 0.6525}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_cwgan_for_numerical_features(\"letter-recognition-2.csv\", \"letter\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Katabatic_partB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
