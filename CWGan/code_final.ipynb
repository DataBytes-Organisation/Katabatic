{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d824f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Repeat 1/3\n",
      "\n",
      "🔄 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.6497\n",
      "MLP: 0.6627\n",
      "RF: 0.6850\n",
      "XGBT: 0.6276\n",
      "\n",
      "🔬 JSD: 0.4973 | WD: 1.1072\n",
      "\n",
      "🔄 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.7597\n",
      "MLP: 0.7631\n",
      "RF: 0.7752\n",
      "XGBT: 0.7635\n",
      "\n",
      "🔬 JSD: 0.5044 | WD: 1.4226\n",
      "\n",
      "🔁 Repeat 2/3\n",
      "\n",
      "🔄 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.6664\n",
      "MLP: 0.6609\n",
      "RF: 0.7410\n",
      "XGBT: 0.7204\n",
      "\n",
      "🔬 JSD: 0.4996 | WD: 1.0518\n",
      "\n",
      "🔄 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.6411\n",
      "MLP: 0.6382\n",
      "RF: 0.7165\n",
      "XGBT: 0.6962\n",
      "\n",
      "🔬 JSD: 0.4903 | WD: 0.8455\n",
      "\n",
      "🔁 Repeat 3/3\n",
      "\n",
      "🔄 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.7030\n",
      "MLP: 0.7116\n",
      "RF: 0.7642\n",
      "XGBT: 0.7181\n",
      "\n",
      "🔬 JSD: 0.4930 | WD: 0.9391\n",
      "\n",
      "🔄 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.6567\n",
      "MLP: 0.6698\n",
      "RF: 0.7659\n",
      "XGBT: 0.7097\n",
      "\n",
      "🔬 JSD: 0.4929 | WD: 0.9036\n",
      "\n",
      "📈 FINAL AVERAGE RESULTS ACROSS 3x2 CV:\n",
      "LogReg TSTR Accuracy: 0.6794\n",
      "MLP TSTR Accuracy: 0.6844\n",
      "RF TSTR Accuracy: 0.7413\n",
      "XGBT TSTR Accuracy: 0.7059\n",
      "\n",
      "JSD: 0.4963\n",
      "Wasserstein Distance: 1.0450\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import entropy, wasserstein_distance\n",
    "from scipy.io import arff\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# ===== Load + Clean ARFF =====\n",
    "data, meta = arff.loadarff(\"adult 1.arff\")\n",
    "df = pd.DataFrame(data)\n",
    "for col in df.select_dtypes([object]).columns:\n",
    "    df[col] = df[col].str.decode(\"utf-8\").str.replace(r\"[\\\\'\\\"]\", \"\", regex=True)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# ===== Encode Categorical =====\n",
    "encoders = {}\n",
    "for col in df.columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    encoders[col] = le\n",
    "\n",
    "X = df.drop(columns=[\"class\"])\n",
    "y = df[\"class\"]\n",
    "input_dim = X.shape[1]\n",
    "num_classes = y.nunique()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ===== Metrics =====\n",
    "def compute_jsd(p, q):\n",
    "    p, q = np.array(p) + 1e-10, np.array(q) + 1e-10\n",
    "    p, q = p / p.sum(), q / q.sum()\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * (entropy(p, m) + entropy(q, m))\n",
    "\n",
    "def evaluate_jsd_wd(real_df, synth_df):\n",
    "    jsd_scores, wd_scores = [], []\n",
    "    for col in real_df.columns:\n",
    "        real, synth = real_df[col].values, synth_df[col].values\n",
    "        jsd = compute_jsd(np.histogram(real, bins=20)[0], np.histogram(synth, bins=20)[0])\n",
    "        wd = wasserstein_distance(real, synth)\n",
    "        jsd_scores.append(jsd)\n",
    "        wd_scores.append(wd)\n",
    "    return np.mean(jsd_scores), np.mean(wd_scores)\n",
    "\n",
    "# ===== CW-GAN Models =====\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(32 + num_classes, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "    def forward(self, z, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat((z, c), dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim + num_classes, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        d_in = torch.cat((x, c), dim=1)\n",
    "        return self.model(d_in)\n",
    "\n",
    "def compute_gp(critic, real_samples, fake_samples, labels, device):\n",
    "    alpha = torch.rand(real_samples.size(0), 1).to(device)\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolates = critic(interpolates, labels)\n",
    "    fake = torch.ones_like(d_interpolates)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates, inputs=interpolates, grad_outputs=fake,\n",
    "        create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    return ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "# ===== Experiment Setup =====\n",
    "skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "all_results = defaultdict(list)\n",
    "\n",
    "for repeat in range(3):\n",
    "    print(f\"\\n🔁 Repeat {repeat+1}/3\")\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"\\n🔄 Fold {fold+1}/2\")\n",
    "\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        X_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "        loader = DataLoader(TensorDataset(X_tensor, y_tensor), batch_size=128, shuffle=True)\n",
    "\n",
    "        generator = Generator().to(device)\n",
    "        critic = Critic().to(device)\n",
    "        opt_G = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "        opt_C = torch.optim.Adam(critic.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "        for epoch in range(100):\n",
    "            for i, (real_samples, labels) in enumerate(loader):\n",
    "                real_samples, labels = real_samples.to(device), labels.to(device)\n",
    "                opt_C.zero_grad()\n",
    "                z = torch.randn(real_samples.size(0), 32).to(device)\n",
    "                fake_samples = generator(z, labels)\n",
    "                real_validity = critic(real_samples, labels)\n",
    "                fake_validity = critic(fake_samples.detach(), labels)\n",
    "                gp = compute_gp(critic, real_samples, fake_samples, labels, device)\n",
    "                c_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + 10 * gp\n",
    "                c_loss.backward()\n",
    "                opt_C.step()\n",
    "                if i % 5 == 0:\n",
    "                    opt_G.zero_grad()\n",
    "                    gen_samples = generator(z, labels)\n",
    "                    g_loss = -torch.mean(critic(gen_samples, labels))\n",
    "                    g_loss.backward()\n",
    "                    opt_G.step()\n",
    "\n",
    "        # Generate synthetic data\n",
    "        synth_size = len(X_train) // 2\n",
    "        z = torch.randn(synth_size, 32).to(device)\n",
    "        synth_labels = torch.randint(0, num_classes, (synth_size,), dtype=torch.long).to(device)\n",
    "        gen_data = generator(z, synth_labels).detach().cpu().numpy()\n",
    "        synth_df = pd.DataFrame(gen_data, columns=X.columns)\n",
    "        synth_df[\"class\"] = synth_labels.cpu().numpy()\n",
    "\n",
    "        # Classifier Evaluation (TSTR)\n",
    "        print(\"\\n📊 TSTR Accuracy:\")\n",
    "        models = {\n",
    "            \"LogReg\": LogisticRegression(max_iter=300),\n",
    "            \"MLP\": MLPClassifier(max_iter=300),\n",
    "            \"RF\": RandomForestClassifier(),\n",
    "            \"XGBT\": XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "        }\n",
    "\n",
    "        for name, model in models.items():\n",
    "            model.fit(synth_df.drop(columns=[\"class\"]), synth_df[\"class\"])\n",
    "            y_pred = model.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            all_results[f\"{name}_acc\"].append(acc)\n",
    "            print(f\"{name}: {acc:.4f}\")\n",
    "\n",
    "        # JSD + WD\n",
    "        jsd, wd = evaluate_jsd_wd(X_train, synth_df.drop(columns=[\"class\"]))\n",
    "        all_results[\"jsd\"].append(jsd)\n",
    "        all_results[\"wd\"].append(wd)\n",
    "        print(f\"\\n🔬 JSD: {jsd:.4f} | WD: {wd:.4f}\")\n",
    "\n",
    "# ===== Final Average Summary =====\n",
    "print(\"\\n📈 FINAL AVERAGE RESULTS ACROSS 3x2 CV:\")\n",
    "for name in [\"LogReg\", \"MLP\", \"RF\", \"XGBT\"]:\n",
    "    avg = np.mean(all_results[f\"{name}_acc\"])\n",
    "    print(f\"{name} TSTR Accuracy: {avg:.4f}\")\n",
    "\n",
    "print(f\"\\nJSD: {np.mean(all_results['jsd']):.4f}\")\n",
    "print(f\"Wasserstein Distance: {np.mean(all_results['wd']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6102ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Repeat 1/3\n",
      "\n",
      "🔄 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.7636\n",
      "MLP: 0.7734\n",
      "RF: 0.7829\n",
      "XGBT: 0.7581\n",
      "\n",
      "🔬 JSD: 0.4944 | WD: 1.1796\n",
      "\n",
      "🔄 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.6364\n",
      "MLP: 0.6224\n",
      "RF: 0.7422\n",
      "XGBT: 0.6911\n",
      "\n",
      "🔬 JSD: 0.4998 | WD: 1.1183\n",
      "\n",
      "🔁 Repeat 2/3\n",
      "\n",
      "🔄 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.6774\n",
      "MLP: 0.6521\n",
      "RF: 0.7806\n",
      "XGBT: 0.7205\n",
      "\n",
      "🔬 JSD: 0.4994 | WD: 1.3236\n",
      "\n",
      "🔄 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.7582\n",
      "MLP: 0.7705\n",
      "RF: 0.7609\n",
      "XGBT: 0.7627\n",
      "\n",
      "🔬 JSD: 0.5011 | WD: 1.4967\n",
      "\n",
      "🔁 Repeat 3/3\n",
      "\n",
      "🔄 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.7392\n",
      "MLP: 0.6616\n",
      "RF: 0.7188\n",
      "XGBT: 0.6166\n",
      "\n",
      "🔬 JSD: 0.4988 | WD: 1.1839\n",
      "\n",
      "🔄 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.6254\n",
      "MLP: 0.6511\n",
      "RF: 0.7006\n",
      "XGBT: 0.6392\n",
      "\n",
      "🔬 JSD: 0.4990 | WD: 1.1816\n",
      "\n",
      "📈 FINAL AVERAGE RESULTS ACROSS 3x2 CV:\n",
      "LogReg TSTR Accuracy: 0.7000\n",
      "MLP TSTR Accuracy: 0.6885\n",
      "RF TSTR Accuracy: 0.7477\n",
      "XGBT TSTR Accuracy: 0.6980\n",
      "\n",
      "JSD: 0.4987\n",
      "Wasserstein Distance: 1.2473\n",
      "\n",
      "🚀 Training generator on full dataset for final synthetic data...\n",
      "\n",
      "💾 Generating final synthetic dataset (50% size of original)\n",
      "✅ Saved synthetic dataset as 'synthetic_dataset_half.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import entropy, wasserstein_distance\n",
    "from scipy.io import arff\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# ===== Load + Clean ARFF =====\n",
    "data, meta = arff.loadarff(\"adult 1.arff\")\n",
    "df = pd.DataFrame(data)\n",
    "for col in df.select_dtypes([object]).columns:\n",
    "    df[col] = df[col].str.decode(\"utf-8\").str.replace(r\"[\\\\'\\\"]\", \"\", regex=True)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# ===== Encode Categorical =====\n",
    "encoders = {}\n",
    "for col in df.columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    encoders[col] = le\n",
    "\n",
    "X = df.drop(columns=[\"class\"])\n",
    "y = df[\"class\"]\n",
    "input_dim = X.shape[1]\n",
    "num_classes = y.nunique()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ===== Metrics =====\n",
    "def compute_jsd(p, q):\n",
    "    p, q = np.array(p) + 1e-10, np.array(q) + 1e-10\n",
    "    p, q = p / p.sum(), q / q.sum()\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * (entropy(p, m) + entropy(q, m))\n",
    "\n",
    "def evaluate_jsd_wd(real_df, synth_df):\n",
    "    jsd_scores, wd_scores = [], []\n",
    "    for col in real_df.columns:\n",
    "        real, synth = real_df[col].values, synth_df[col].values\n",
    "        jsd = compute_jsd(np.histogram(real, bins=20)[0], np.histogram(synth, bins=20)[0])\n",
    "        wd = wasserstein_distance(real, synth)\n",
    "        jsd_scores.append(jsd)\n",
    "        wd_scores.append(wd)\n",
    "    return np.mean(jsd_scores), np.mean(wd_scores)\n",
    "\n",
    "# ===== CW-GAN Models =====\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(32 + num_classes, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "    def forward(self, z, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat((z, c), dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim + num_classes, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        d_in = torch.cat((x, c), dim=1)\n",
    "        return self.model(d_in)\n",
    "\n",
    "def compute_gp(critic, real_samples, fake_samples, labels, device):\n",
    "    alpha = torch.rand(real_samples.size(0), 1).to(device)\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolates = critic(interpolates, labels)\n",
    "    fake = torch.ones_like(d_interpolates)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates, inputs=interpolates, grad_outputs=fake,\n",
    "        create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    return ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "# ===== Experiment Setup =====\n",
    "skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "all_results = defaultdict(list)\n",
    "\n",
    "for repeat in range(3):\n",
    "    print(f\"\\n🔁 Repeat {repeat+1}/3\")\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"\\n🔄 Fold {fold+1}/2\")\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        X_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "        loader = DataLoader(TensorDataset(X_tensor, y_tensor), batch_size=128, shuffle=True)\n",
    "\n",
    "        generator = Generator().to(device)\n",
    "        critic = Critic().to(device)\n",
    "        opt_G = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "        opt_C = torch.optim.Adam(critic.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "        for epoch in range(100):\n",
    "            for i, (real_samples, labels) in enumerate(loader):\n",
    "                real_samples, labels = real_samples.to(device), labels.to(device)\n",
    "                opt_C.zero_grad()\n",
    "                z = torch.randn(real_samples.size(0), 32).to(device)\n",
    "                fake_samples = generator(z, labels)\n",
    "                real_validity = critic(real_samples, labels)\n",
    "                fake_validity = critic(fake_samples.detach(), labels)\n",
    "                gp = compute_gp(critic, real_samples, fake_samples, labels, device)\n",
    "                c_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + 10 * gp\n",
    "                c_loss.backward()\n",
    "                opt_C.step()\n",
    "\n",
    "                if i % 5 == 0:\n",
    "                    opt_G.zero_grad()\n",
    "                    gen_samples = generator(z, labels)\n",
    "                    g_loss = -torch.mean(critic(gen_samples, labels))\n",
    "                    g_loss.backward()\n",
    "                    opt_G.step()\n",
    "\n",
    "        synth_size = len(X_train) // 2\n",
    "        z = torch.randn(synth_size, 32).to(device)\n",
    "        synth_labels = torch.randint(0, num_classes, (synth_size,), dtype=torch.long).to(device)\n",
    "        gen_data = generator(z, synth_labels).detach().cpu().numpy()\n",
    "        synth_df = pd.DataFrame(gen_data, columns=X.columns)\n",
    "        synth_df[\"class\"] = synth_labels.cpu().numpy()\n",
    "\n",
    "        print(\"\\n📊 TSTR Accuracy:\")\n",
    "        models = {\n",
    "            \"LogReg\": LogisticRegression(max_iter=300),\n",
    "            \"MLP\": MLPClassifier(max_iter=300),\n",
    "            \"RF\": RandomForestClassifier(),\n",
    "            \"XGBT\": XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "        }\n",
    "        for name, model in models.items():\n",
    "            model.fit(synth_df.drop(columns=[\"class\"]), synth_df[\"class\"])\n",
    "            y_pred = model.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            all_results[f\"{name}_acc\"].append(acc)\n",
    "            print(f\"{name}: {acc:.4f}\")\n",
    "\n",
    "        jsd, wd = evaluate_jsd_wd(X_train, synth_df.drop(columns=[\"class\"]))\n",
    "        all_results[\"jsd\"].append(jsd)\n",
    "        all_results[\"wd\"].append(wd)\n",
    "        print(f\"\\n🔬 JSD: {jsd:.4f} | WD: {wd:.4f}\")\n",
    "\n",
    "# ===== Final Average Summary =====\n",
    "print(\"\\n📈 FINAL AVERAGE RESULTS ACROSS 3x2 CV:\")\n",
    "for name in [\"LogReg\", \"MLP\", \"RF\", \"XGBT\"]:\n",
    "    avg = np.mean(all_results[f\"{name}_acc\"])\n",
    "    print(f\"{name} TSTR Accuracy: {avg:.4f}\")\n",
    "print(f\"\\nJSD: {np.mean(all_results['jsd']):.4f}\")\n",
    "print(f\"Wasserstein Distance: {np.mean(all_results['wd']):.4f}\")\n",
    "\n",
    "# ===== Final Generator Training on Full Dataset =====\n",
    "print(\"\\n🚀 Training generator on full dataset for final synthetic data...\")\n",
    "X_tensor_full = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_tensor_full = torch.tensor(y.values, dtype=torch.long)\n",
    "loader_full = DataLoader(TensorDataset(X_tensor_full, y_tensor_full), batch_size=128, shuffle=True)\n",
    "\n",
    "generator = Generator().to(device)\n",
    "critic = Critic().to(device)\n",
    "opt_G = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "opt_C = torch.optim.Adam(critic.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "for epoch in range(100):\n",
    "    for i, (real_samples, labels) in enumerate(loader_full):\n",
    "        real_samples, labels = real_samples.to(device), labels.to(device)\n",
    "        opt_C.zero_grad()\n",
    "        z = torch.randn(real_samples.size(0), 32).to(device)\n",
    "        fake_samples = generator(z, labels)\n",
    "        real_validity = critic(real_samples, labels)\n",
    "        fake_validity = critic(fake_samples.detach(), labels)\n",
    "        gp = compute_gp(critic, real_samples, fake_samples, labels, device)\n",
    "        c_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + 10 * gp\n",
    "        c_loss.backward()\n",
    "        opt_C.step()\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            opt_G.zero_grad()\n",
    "            gen_samples = generator(z, labels)\n",
    "            g_loss = -torch.mean(critic(gen_samples, labels))\n",
    "            g_loss.backward()\n",
    "            opt_G.step()\n",
    "\n",
    "# ===== Generate and Save Final Synthetic Dataset =====\n",
    "print(\"\\n💾 Generating final synthetic dataset (50% size of original)\")\n",
    "synth_size = len(X) // 2\n",
    "z = torch.randn(synth_size, 32).to(device)\n",
    "synth_labels = torch.randint(0, num_classes, (synth_size,), dtype=torch.long).to(device)\n",
    "gen_data = generator(z, synth_labels).detach().cpu().numpy()\n",
    "synth_df_final = pd.DataFrame(gen_data, columns=X.columns)\n",
    "synth_df_final[\"class\"] = synth_labels.cpu().numpy()\n",
    "\n",
    "# ===== Postprocess Categorical + Clamp Continuous Columns =====\n",
    "categorical_columns = [\n",
    "    \"workclass\", \"education\", \"marital-status\", \"occupation\",\n",
    "    \"relationship\", \"race\", \"sex\", \"native-country\"\n",
    "]\n",
    "\n",
    "for col in categorical_columns:\n",
    "    synth_df_final[col] = synth_df_final[col].round().astype(int)\n",
    "\n",
    "# ✅ ADD THESE RIGHT HERE\n",
    "for col in categorical_columns:\n",
    "    synth_df_final[col] = synth_df_final[col].clip(0, df[col].max())\n",
    "    \n",
    "# Clamp continuous columns\n",
    "for col in [\"fnlwgt\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]:\n",
    "    min_val = df[col].min()\n",
    "    max_val = df[col].max()\n",
    "    synth_df_final[col] = synth_df_final[col].clip(min_val, max_val)\n",
    "\n",
    "\n",
    "    synth_df_final.to_csv(\"synthetic_dataset_half.csv\", index=False)\n",
    "print(\"✅ Saved synthetic dataset as 'synthetic_dataset_half.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bfbe1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69466d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved synthetic dataset as 'synthetic_dataset_half.csv'\n"
     ]
    }
   ],
   "source": [
    "# ===== Postprocess Categorical + Clamp Continuous Columns =====\n",
    "categorical_columns = [\n",
    "    \"workclass\", \"education\", \"marital-status\", \"occupation\",\n",
    "    \"relationship\", \"race\", \"sex\", \"native-country\"\n",
    "]\n",
    "\n",
    "for col in categorical_columns:\n",
    "    synth_df_final[col] = synth_df_final[col].round().astype(int)\n",
    "\n",
    "# ✅ ADD THESE RIGHT HERE\n",
    "for col in categorical_columns:\n",
    "    synth_df_final[col] = synth_df_final[col].clip(0, df[col].max())\n",
    "    \n",
    "# Clamp continuous columns\n",
    "for col in [\"fnlwgt\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]:\n",
    "    min_val = df[col].min()\n",
    "    max_val = df[col].max()\n",
    "    synth_df_final[col] = synth_df_final[col].clip(min_val, max_val)\n",
    "\n",
    "\n",
    "    synth_df_final.to_csv(\"synthetic_dataset_half.csv\", index=False)\n",
    "print(\"✅ Saved synthetic dataset as 'synthetic_dataset_half.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e722f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Repeat 1/3\n",
      "\n",
      "🔄 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.3634\n",
      "MLP: 0.2928\n",
      "RF: 0.4271\n",
      "XGBT: 0.3299\n",
      "\n",
      "🔬 JSD: 0.3243 | WD: 0.7552\n",
      "\n",
      "🔄 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.3125\n",
      "MLP: 0.2535\n",
      "RF: 0.2836\n",
      "XGBT: 0.2627\n",
      "\n",
      "🔬 JSD: 0.3755 | WD: 0.8764\n",
      "\n",
      "🔁 Repeat 2/3\n",
      "\n",
      "🔄 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.1412\n",
      "MLP: 0.1308\n",
      "RF: 0.0949\n",
      "XGBT: 0.1597\n",
      "\n",
      "🔬 JSD: 0.2786 | WD: 0.7483\n",
      "\n",
      "🔄 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.3252\n",
      "MLP: 0.3252\n",
      "RF: 0.2523\n",
      "XGBT: 0.3438\n",
      "\n",
      "🔬 JSD: 0.3940 | WD: 0.7359\n",
      "\n",
      "🔁 Repeat 3/3\n",
      "\n",
      "🔄 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.3958\n",
      "MLP: 0.3854\n",
      "RF: 0.3553\n",
      "XGBT: 0.3345\n",
      "\n",
      "🔬 JSD: 0.2860 | WD: 0.6831\n",
      "\n",
      "🔄 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.3218\n",
      "MLP: 0.1968\n",
      "RF: 0.2558\n",
      "XGBT: 0.2431\n",
      "\n",
      "🔬 JSD: 0.2457 | WD: 0.7884\n",
      "\n",
      "📈 FINAL AVERAGE RESULTS ACROSS 3x2 CV:\n",
      "LogReg TSTR Accuracy: 0.3100\n",
      "MLP TSTR Accuracy: 0.2641\n",
      "RF TSTR Accuracy: 0.2782\n",
      "XGBT TSTR Accuracy: 0.2789\n",
      "\n",
      "JSD: 0.3173\n",
      "Wasserstein Distance: 0.7645\n",
      "\n",
      "🚀 Training generator on full dataset for final synthetic data...\n",
      "\n",
      "💾 Generating final synthetic dataset (50% size of original)\n",
      "✅ Saved synthetic car dataset as 'synthetic_car_dataset_half.csv'\n"
     ]
    }
   ],
   "source": [
    "# ===== Define Critic for CAR Dataset =====\n",
    "class CarCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes_car, num_classes_car)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim_car + num_classes_car, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        d_in = torch.cat((x, c), dim=1)\n",
    "        return self.model(d_in)\n",
    "\n",
    "# ===== Reusable Metrics =====\n",
    "def compute_jsd(p, q):\n",
    "    p, q = np.array(p) + 1e-10, np.array(q) + 1e-10\n",
    "    p, q = p / p.sum(), q / q.sum()\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * (entropy(p, m) + entropy(q, m))\n",
    "\n",
    "def evaluate_jsd_wd(real_df, synth_df):\n",
    "    jsd_scores, wd_scores = [], []\n",
    "    for col in real_df.columns:\n",
    "        real, synth = real_df[col].values, synth_df[col].values\n",
    "        jsd = compute_jsd(np.histogram(real, bins=20)[0], np.histogram(synth, bins=20)[0])\n",
    "        wd = wasserstein_distance(real, synth)\n",
    "        jsd_scores.append(jsd)\n",
    "        wd_scores.append(wd)\n",
    "    return np.mean(jsd_scores), np.mean(wd_scores)\n",
    "\n",
    "# ===== 3x2 CV + Evaluation =====\n",
    "skf_car = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "all_results_car = defaultdict(list)\n",
    "\n",
    "for repeat in range(3):\n",
    "    print(f\"\\n🔁 Repeat {repeat+1}/3\")\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf_car.split(X_car, y_car)):\n",
    "        print(f\"\\n🔄 Fold {fold+1}/2\")\n",
    "\n",
    "        X_train, X_test = X_car.iloc[train_idx], X_car.iloc[test_idx]\n",
    "        y_train, y_test = y_car.iloc[train_idx], y_car.iloc[test_idx]\n",
    "\n",
    "        X_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "        loader = DataLoader(TensorDataset(X_tensor, y_tensor), batch_size=128, shuffle=True)\n",
    "\n",
    "        generator_car = CarGenerator().to(device)\n",
    "        critic_car = CarCritic().to(device)\n",
    "        opt_G = torch.optim.Adam(generator_car.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "        opt_C = torch.optim.Adam(critic_car.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "        for epoch in range(100):\n",
    "            for i, (real_samples, labels) in enumerate(loader):\n",
    "                real_samples, labels = real_samples.to(device), labels.to(device)\n",
    "                opt_C.zero_grad()\n",
    "                z = torch.randn(real_samples.size(0), 32).to(device)\n",
    "                fake_samples = generator_car(z, labels)\n",
    "                real_validity = critic_car(real_samples, labels)\n",
    "                fake_validity = critic_car(fake_samples.detach(), labels)\n",
    "                gp = compute_gp(critic_car, real_samples, fake_samples, labels, device)\n",
    "                c_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + 10 * gp\n",
    "                c_loss.backward()\n",
    "                opt_C.step()\n",
    "\n",
    "                if i % 5 == 0:\n",
    "                    opt_G.zero_grad()\n",
    "                    gen_samples = generator_car(z, labels)\n",
    "                    g_loss = -torch.mean(critic_car(gen_samples, labels))\n",
    "                    g_loss.backward()\n",
    "                    opt_G.step()\n",
    "\n",
    "        # ===== Generate Synthetic Data =====\n",
    "        synth_size = len(X_train) // 2\n",
    "        z = torch.randn(synth_size, 32).to(device)\n",
    "        synth_labels = torch.randint(0, num_classes_car, (synth_size,), dtype=torch.long).to(device)\n",
    "        gen_data = generator_car(z, synth_labels).detach().cpu().numpy()\n",
    "\n",
    "        synth_df = pd.DataFrame(gen_data, columns=X_car.columns)\n",
    "        synth_df[\"class\"] = synth_labels.cpu().numpy()\n",
    "\n",
    "        # Postprocess\n",
    "        for col in synth_df.columns:\n",
    "            synth_df[col] = synth_df[col].round().astype(int)\n",
    "            synth_df[col] = synth_df[col].clip(0, car_df[col].max())\n",
    "\n",
    "        # ===== TSTR Evaluation =====\n",
    "        print(\"\\n📊 TSTR Accuracy:\")\n",
    "        models = {\n",
    "            \"LogReg\": LogisticRegression(max_iter=300),\n",
    "            \"MLP\": MLPClassifier(max_iter=300),\n",
    "            \"RF\": RandomForestClassifier(),\n",
    "            \"XGBT\": XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "        }\n",
    "\n",
    "        for name, model in models.items():\n",
    "            model.fit(synth_df.drop(columns=[\"class\"]), synth_df[\"class\"])\n",
    "            y_pred = model.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            all_results_car[f\"{name}_acc\"].append(acc)\n",
    "            print(f\"{name}: {acc:.4f}\")\n",
    "\n",
    "        # ===== JSD + WD\n",
    "        jsd, wd = evaluate_jsd_wd(X_train, synth_df.drop(columns=[\"class\"]))\n",
    "        all_results_car[\"jsd\"].append(jsd)\n",
    "        all_results_car[\"wd\"].append(wd)\n",
    "        print(f\"\\n🔬 JSD: {jsd:.4f} | WD: {wd:.4f}\")\n",
    "\n",
    "# ===== Final Summary =====\n",
    "print(\"\\n📈 FINAL AVERAGE RESULTS ACROSS 3x2 CV:\")\n",
    "for name in [\"LogReg\", \"MLP\", \"RF\", \"XGBT\"]:\n",
    "    avg = np.mean(all_results_car[f\"{name}_acc\"])\n",
    "    print(f\"{name} TSTR Accuracy: {avg:.4f}\")\n",
    "print(f\"\\nJSD: {np.mean(all_results_car['jsd']):.4f}\")\n",
    "print(f\"Wasserstein Distance: {np.mean(all_results_car['wd']):.4f}\")\n",
    "\n",
    "# ===== Final Training + Save CSV =====\n",
    "print(\"\\n🚀 Training generator on full dataset for final synthetic data...\")\n",
    "X_tensor_full_car = torch.tensor(X_car.values, dtype=torch.float32)\n",
    "y_tensor_full_car = torch.tensor(y_car.values, dtype=torch.long)\n",
    "loader_full_car = DataLoader(TensorDataset(X_tensor_full_car, y_tensor_full_car), batch_size=128, shuffle=True)\n",
    "\n",
    "generator_car = CarGenerator().to(device)\n",
    "critic_car = CarCritic().to(device)\n",
    "opt_G = torch.optim.Adam(generator_car.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "opt_C = torch.optim.Adam(critic_car.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "for epoch in range(100):\n",
    "    for i, (real_samples, labels) in enumerate(loader_full_car):\n",
    "        real_samples, labels = real_samples.to(device), labels.to(device)\n",
    "        opt_C.zero_grad()\n",
    "        z = torch.randn(real_samples.size(0), 32).to(device)\n",
    "        fake_samples = generator_car(z, labels)\n",
    "        real_validity = critic_car(real_samples, labels)\n",
    "        fake_validity = critic_car(fake_samples.detach(), labels)\n",
    "        gp = compute_gp(critic_car, real_samples, fake_samples, labels, device)\n",
    "        c_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + 10 * gp\n",
    "        c_loss.backward()\n",
    "        opt_C.step()\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            opt_G.zero_grad()\n",
    "            gen_samples = generator_car(z, labels)\n",
    "            g_loss = -torch.mean(critic_car(gen_samples, labels))\n",
    "            g_loss.backward()\n",
    "            opt_G.step()\n",
    "\n",
    "# ===== Generate Final Synthetic Data =====\n",
    "print(\"\\n💾 Generating final synthetic dataset (50% size of original)\")\n",
    "synth_size_final = len(X_car) // 2\n",
    "z = torch.randn(synth_size_final, 32).to(device)\n",
    "synth_labels_final = torch.randint(0, num_classes_car, (synth_size_final,), dtype=torch.long).to(device)\n",
    "gen_data_final = generator_car(z, synth_labels_final).detach().cpu().numpy()\n",
    "\n",
    "synth_df_final = pd.DataFrame(gen_data_final, columns=X_car.columns)\n",
    "synth_df_final[\"class\"] = synth_labels_final.cpu().numpy()\n",
    "\n",
    "# Postprocess\n",
    "for col in synth_df_final.columns:\n",
    "    synth_df_final[col] = synth_df_final[col].round().astype(int)\n",
    "    synth_df_final[col] = synth_df_final[col].clip(0, car_df[col].max())\n",
    "\n",
    "synth_df_final.to_csv(\"synthetic_car_dataset_half.csv\", index=False)\n",
    "print(\"✅ Saved synthetic car dataset as 'synthetic_car_dataset_half.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a93fe53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CREDIT dataset ready — encoded and split.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ===== Load and Clean CREDIT Dataset =====\n",
    "credit_data, credit_meta = arff.loadarff(\"credit-a.arff\")\n",
    "credit_df = pd.DataFrame(credit_data)\n",
    "\n",
    "# Decode byte strings to normal strings\n",
    "for col in credit_df.select_dtypes([object]).columns:\n",
    "    credit_df[col] = credit_df[col].str.decode(\"utf-8\").str.replace(r\"[\\\\'\\\"]\", \"\", regex=True)\n",
    "\n",
    "# Encode all categorical columns\n",
    "encoders_credit = {}\n",
    "for col in credit_df.columns:\n",
    "    le = LabelEncoder()\n",
    "    credit_df[col] = le.fit_transform(credit_df[col])\n",
    "    encoders_credit[col] = le\n",
    "\n",
    "# Define input features and target\n",
    "X_credit = credit_df.drop(columns=[\"class\"])\n",
    "y_credit = credit_df[\"class\"]\n",
    "\n",
    "# Dimensions for GAN\n",
    "input_dim_credit = X_credit.shape[1]\n",
    "num_classes_credit = y_credit.nunique()\n",
    "\n",
    "print(\"✅ CREDIT dataset ready — encoded and split.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89c8e2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Repeat 1/3\n",
      "\n",
      "🔄 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.5507\n",
      "MLP: 0.5507\n",
      "RF: 0.5101\n",
      "XGBT: 0.4000\n",
      "\n",
      "🔬 JSD: 0.4958 | WD: 1.2855\n",
      "\n",
      "🔄 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.4725\n",
      "MLP: 0.4812\n",
      "RF: 0.5217\n",
      "XGBT: 0.5188\n",
      "\n",
      "🔬 JSD: 0.4970 | WD: 1.2699\n",
      "\n",
      "🔁 Repeat 2/3\n",
      "\n",
      "🔄 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.4464\n",
      "MLP: 0.4464\n",
      "RF: 0.5043\n",
      "XGBT: 0.5275\n",
      "\n",
      "🔬 JSD: 0.3654 | WD: 1.2408\n",
      "\n",
      "🔄 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.5565\n",
      "MLP: 0.5536\n",
      "RF: 0.5565\n",
      "XGBT: 0.5565\n",
      "\n",
      "🔬 JSD: 0.4796 | WD: 1.2458\n",
      "\n",
      "🔁 Repeat 3/3\n",
      "\n",
      "🔄 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.5565\n",
      "MLP: 0.5623\n",
      "RF: 0.5594\n",
      "XGBT: 0.2899\n",
      "\n",
      "🔬 JSD: 0.3906 | WD: 1.2084\n",
      "\n",
      "🔄 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.4435\n",
      "MLP: 0.4435\n",
      "RF: 0.4406\n",
      "XGBT: 0.4261\n",
      "\n",
      "🔬 JSD: 0.3817 | WD: 1.2346\n",
      "\n",
      "📈 FINAL AVERAGE RESULTS ACROSS 3x2 CV:\n",
      "LogReg TSTR Accuracy: 0.5043\n",
      "MLP TSTR Accuracy: 0.5063\n",
      "RF TSTR Accuracy: 0.5155\n",
      "XGBT TSTR Accuracy: 0.4531\n",
      "\n",
      "JSD: 0.4350\n",
      "Wasserstein Distance: 1.2475\n",
      "\n",
      "🚀 Training generator on full dataset for final synthetic data...\n",
      "\n",
      "💾 Generating final synthetic dataset (50% size of original)\n",
      "✅ Saved synthetic credit dataset as 'synthetic_credit_dataset_half.csv'\n"
     ]
    }
   ],
   "source": [
    "# ===== Define Critic for CREDIT Dataset =====\n",
    "class CreditCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes_credit, num_classes_credit)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim_credit + num_classes_credit, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        d_in = torch.cat((x, c), dim=1)\n",
    "        return self.model(d_in)\n",
    "\n",
    "# ===== Reusable Metrics =====\n",
    "def compute_jsd(p, q):\n",
    "    p, q = np.array(p) + 1e-10, np.array(q) + 1e-10\n",
    "    p, q = p / p.sum(), q / q.sum()\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * (entropy(p, m) + entropy(q, m))\n",
    "\n",
    "def evaluate_jsd_wd(real_df, synth_df):\n",
    "    jsd_scores, wd_scores = [], []\n",
    "    for col in real_df.columns:\n",
    "        real, synth = real_df[col].values, synth_df[col].values\n",
    "        jsd = compute_jsd(np.histogram(real, bins=20)[0], np.histogram(synth, bins=20)[0])\n",
    "        wd = wasserstein_distance(real, synth)\n",
    "        jsd_scores.append(jsd)\n",
    "        wd_scores.append(wd)\n",
    "    return np.mean(jsd_scores), np.mean(wd_scores)\n",
    "\n",
    "# ===== 3x2 CV + Evaluation =====\n",
    "skf_credit = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "all_results_credit = defaultdict(list)\n",
    "\n",
    "for repeat in range(3):\n",
    "    print(f\"\\n🔁 Repeat {repeat+1}/3\")\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf_credit.split(X_credit, y_credit)):\n",
    "        print(f\"\\n🔄 Fold {fold+1}/2\")\n",
    "\n",
    "        X_train, X_test = X_credit.iloc[train_idx], X_credit.iloc[test_idx]\n",
    "        y_train, y_test = y_credit.iloc[train_idx], y_credit.iloc[test_idx]\n",
    "\n",
    "        X_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "        loader = DataLoader(TensorDataset(X_tensor, y_tensor), batch_size=128, shuffle=True)\n",
    "\n",
    "        generator_credit = CreditGenerator().to(device)\n",
    "        critic_credit = CreditCritic().to(device)\n",
    "        opt_G = torch.optim.Adam(generator_credit.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "        opt_C = torch.optim.Adam(critic_credit.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "        for epoch in range(100):\n",
    "            for i, (real_samples, labels) in enumerate(loader):\n",
    "                real_samples, labels = real_samples.to(device), labels.to(device)\n",
    "                opt_C.zero_grad()\n",
    "                z = torch.randn(real_samples.size(0), 32).to(device)\n",
    "                fake_samples = generator_credit(z, labels)\n",
    "                real_validity = critic_credit(real_samples, labels)\n",
    "                fake_validity = critic_credit(fake_samples.detach(), labels)\n",
    "                gp = compute_gp(critic_credit, real_samples, fake_samples, labels, device)\n",
    "                c_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + 10 * gp\n",
    "                c_loss.backward()\n",
    "                opt_C.step()\n",
    "\n",
    "                if i % 5 == 0:\n",
    "                    opt_G.zero_grad()\n",
    "                    gen_samples = generator_credit(z, labels)\n",
    "                    g_loss = -torch.mean(critic_credit(gen_samples, labels))\n",
    "                    g_loss.backward()\n",
    "                    opt_G.step()\n",
    "\n",
    "        # ===== Generate Synthetic Data =====\n",
    "        synth_size = len(X_train) // 2\n",
    "        z = torch.randn(synth_size, 32).to(device)\n",
    "        synth_labels = torch.randint(0, num_classes_credit, (synth_size,), dtype=torch.long).to(device)\n",
    "        gen_data = generator_credit(z, synth_labels).detach().cpu().numpy()\n",
    "\n",
    "        synth_df = pd.DataFrame(gen_data, columns=X_credit.columns)\n",
    "        synth_df[\"class\"] = synth_labels.cpu().numpy()\n",
    "\n",
    "        # Postprocess\n",
    "        for col in synth_df.columns:\n",
    "            synth_df[col] = synth_df[col].round().astype(int)\n",
    "            synth_df[col] = synth_df[col].clip(0, credit_df[col].max())\n",
    "\n",
    "        # ===== TSTR Evaluation =====\n",
    "        print(\"\\n📊 TSTR Accuracy:\")\n",
    "        models = {\n",
    "            \"LogReg\": LogisticRegression(max_iter=300),\n",
    "            \"MLP\": MLPClassifier(max_iter=300),\n",
    "            \"RF\": RandomForestClassifier(),\n",
    "            \"XGBT\": XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "        }\n",
    "\n",
    "        for name, model in models.items():\n",
    "            model.fit(synth_df.drop(columns=[\"class\"]), synth_df[\"class\"])\n",
    "            y_pred = model.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            all_results_credit[f\"{name}_acc\"].append(acc)\n",
    "            print(f\"{name}: {acc:.4f}\")\n",
    "\n",
    "        # ===== JSD + WD\n",
    "        jsd, wd = evaluate_jsd_wd(X_train, synth_df.drop(columns=[\"class\"]))\n",
    "        all_results_credit[\"jsd\"].append(jsd)\n",
    "        all_results_credit[\"wd\"].append(wd)\n",
    "        print(f\"\\n🔬 JSD: {jsd:.4f} | WD: {wd:.4f}\")\n",
    "\n",
    "# ===== Final Summary =====\n",
    "print(\"\\n📈 FINAL AVERAGE RESULTS ACROSS 3x2 CV:\")\n",
    "for name in [\"LogReg\", \"MLP\", \"RF\", \"XGBT\"]:\n",
    "    avg = np.mean(all_results_credit[f\"{name}_acc\"])\n",
    "    print(f\"{name} TSTR Accuracy: {avg:.4f}\")\n",
    "print(f\"\\nJSD: {np.mean(all_results_credit['jsd']):.4f}\")\n",
    "print(f\"Wasserstein Distance: {np.mean(all_results_credit['wd']):.4f}\")\n",
    "\n",
    "# ===== Final Training + Save CSV =====\n",
    "print(\"\\n🚀 Training generator on full dataset for final synthetic data...\")\n",
    "X_tensor_full_credit = torch.tensor(X_credit.values, dtype=torch.float32)\n",
    "y_tensor_full_credit = torch.tensor(y_credit.values, dtype=torch.long)\n",
    "loader_full_credit = DataLoader(TensorDataset(X_tensor_full_credit, y_tensor_full_credit), batch_size=128, shuffle=True)\n",
    "\n",
    "generator_credit = CreditGenerator().to(device)\n",
    "critic_credit = CreditCritic().to(device)\n",
    "opt_G = torch.optim.Adam(generator_credit.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "opt_C = torch.optim.Adam(critic_credit.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "for epoch in range(100):\n",
    "    for i, (real_samples, labels) in enumerate(loader_full_credit):\n",
    "        real_samples, labels = real_samples.to(device), labels.to(device)\n",
    "        opt_C.zero_grad()\n",
    "        z = torch.randn(real_samples.size(0), 32).to(device)\n",
    "        fake_samples = generator_credit(z, labels)\n",
    "        real_validity = critic_credit(real_samples, labels)\n",
    "        fake_validity = critic_credit(fake_samples.detach(), labels)\n",
    "        gp = compute_gp(critic_credit, real_samples, fake_samples, labels, device)\n",
    "        c_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + 10 * gp\n",
    "        c_loss.backward()\n",
    "        opt_C.step()\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            opt_G.zero_grad()\n",
    "            gen_samples = generator_credit(z, labels)\n",
    "            g_loss = -torch.mean(critic_credit(gen_samples, labels))\n",
    "            g_loss.backward()\n",
    "            opt_G.step()\n",
    "\n",
    "# ===== Generate Final Synthetic Data =====\n",
    "print(\"\\n💾 Generating final synthetic dataset (50% size of original)\")\n",
    "synth_size_final = len(X_credit) // 2\n",
    "z = torch.randn(synth_size_final, 32).to(device)\n",
    "synth_labels_final = torch.randint(0, num_classes_credit, (synth_size_final,), dtype=torch.long).to(device)\n",
    "gen_data_final = generator_credit(z, synth_labels_final).detach().cpu().numpy()\n",
    "\n",
    "synth_df_final = pd.DataFrame(gen_data_final, columns=X_credit.columns)\n",
    "synth_df_final[\"class\"] = synth_labels_final.cpu().numpy()\n",
    "\n",
    "# Postprocess\n",
    "for col in synth_df_final.columns:\n",
    "    synth_df_final[col] = synth_df_final[col].round().astype(int)\n",
    "    synth_df_final[col] = synth_df_final[col].clip(0, credit_df[col].max())\n",
    "\n",
    "synth_df_final.to_csv(\"synthetic_credit_dataset_half.csv\", index=False)\n",
    "print(\"✅ Saved synthetic credit dataset as 'synthetic_credit_dataset_half.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ec71e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Repeat 1/3\n",
      "\n",
      "🔄 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.1554\n",
      "MLP: 0.1574\n",
      "RF: 0.1627\n",
      "XGBT: 0.1326\n",
      "\n",
      "🔬 JSD: 0.1773 | WD: 0.5672\n",
      "\n",
      "🔄 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.1843\n",
      "MLP: 0.1705\n",
      "RF: 0.1959\n",
      "XGBT: 0.1752\n",
      "\n",
      "🔬 JSD: 0.1673 | WD: 0.5455\n",
      "\n",
      "🔁 Repeat 2/3\n",
      "\n",
      "🔄 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.2058\n",
      "MLP: 0.2175\n",
      "RF: 0.2149\n",
      "XGBT: 0.2229\n",
      "\n",
      "🔬 JSD: 0.1510 | WD: 0.5500\n",
      "\n",
      "🔄 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.2184\n",
      "MLP: 0.2162\n",
      "RF: 0.1912\n",
      "XGBT: 0.1965\n",
      "\n",
      "🔬 JSD: 0.2151 | WD: 0.5812\n",
      "\n",
      "🔁 Repeat 3/3\n",
      "\n",
      "🔄 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.2047\n",
      "MLP: 0.2273\n",
      "RF: 0.1953\n",
      "XGBT: 0.2087\n",
      "\n",
      "🔬 JSD: 0.2071 | WD: 0.5677\n",
      "\n",
      "🔄 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.2000\n",
      "MLP: 0.1995\n",
      "RF: 0.2173\n",
      "XGBT: 0.2036\n",
      "\n",
      "🔬 JSD: 0.1713 | WD: 0.5397\n",
      "\n",
      "📈 FINAL AVERAGE RESULTS ACROSS 3x2 CV:\n",
      "LogReg TSTR Accuracy: 0.1948\n",
      "MLP TSTR Accuracy: 0.1981\n",
      "RF TSTR Accuracy: 0.1962\n",
      "XGBT TSTR Accuracy: 0.1899\n",
      "\n",
      "JSD: 0.1815\n",
      "Wasserstein Distance: 0.5586\n",
      "\n",
      "🚀 Training on full dataset for final synthetic sample...\n",
      "\n",
      "💾 Generating and saving final dataset\n",
      "✅ Saved synthetic letter dataset as 'synthetic_letter_dataset_half.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import entropy, wasserstein_distance\n",
    "from scipy.io import arff\n",
    "from collections import defaultdict\n",
    "\n",
    "# ===== Load and Encode Letter Dataset =====\n",
    "data, meta = arff.loadarff(\"letter-recog.arff\")\n",
    "letter_df = pd.DataFrame(data)\n",
    "for col in letter_df.select_dtypes([object]).columns:\n",
    "    letter_df[col] = letter_df[col].str.decode(\"utf-8\")\n",
    "encoders = {}\n",
    "for col in letter_df.columns:\n",
    "    le = LabelEncoder()\n",
    "    letter_df[col] = le.fit_transform(letter_df[col])\n",
    "    encoders[col] = le\n",
    "\n",
    "X_letter = letter_df.drop(columns=[\"class\"])\n",
    "y_letter = letter_df[\"class\"]\n",
    "input_dim_letter = X_letter.shape[1]\n",
    "num_classes_letter = y_letter.nunique()\n",
    "device = torch.device(\"cpu\")  # Force CPU use\n",
    "\n",
    "# ===== Define Generator and Critic =====\n",
    "class LetterGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes_letter, num_classes_letter)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(32 + num_classes_letter, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim_letter)\n",
    "        )\n",
    "    def forward(self, z, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat((z, c), dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "class LetterCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes_letter, num_classes_letter)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim_letter + num_classes_letter, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        d_in = torch.cat((x, c), dim=1)\n",
    "        return self.model(d_in)\n",
    "\n",
    "def compute_gp(critic, real_samples, fake_samples, labels, device):\n",
    "    alpha = torch.rand(real_samples.size(0), 1).to(device)\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolates = critic(interpolates, labels)\n",
    "    fake = torch.ones_like(d_interpolates)\n",
    "    gradients = torch.autograd.grad(outputs=d_interpolates, inputs=interpolates,\n",
    "                                    grad_outputs=fake, create_graph=True,\n",
    "                                    retain_graph=True, only_inputs=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    return ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "def compute_jsd(p, q):\n",
    "    p, q = np.array(p) + 1e-10, np.array(q) + 1e-10\n",
    "    p, q = p / p.sum(), q / p.sum()\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * (entropy(p, m) + entropy(q, m))\n",
    "\n",
    "def evaluate_jsd_wd(real_df, synth_df):\n",
    "    jsd_scores, wd_scores = [], []\n",
    "    for col in real_df.columns:\n",
    "        real, synth = real_df[col].values, synth_df[col].values\n",
    "        jsd = compute_jsd(np.histogram(real, bins=20)[0], np.histogram(synth, bins=20)[0])\n",
    "        wd = wasserstein_distance(real, synth)\n",
    "        jsd_scores.append(jsd)\n",
    "        wd_scores.append(wd)\n",
    "    return np.mean(jsd_scores), np.mean(wd_scores)\n",
    "\n",
    "# ===== 3x2 CV + Evaluation =====\n",
    "skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "all_results_letter = defaultdict(list)\n",
    "\n",
    "for repeat in range(3):\n",
    "    print(f\"\\n🔁 Repeat {repeat+1}/3\")\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X_letter, y_letter)):\n",
    "        print(f\"\\n🔄 Fold {fold+1}/2\")\n",
    "        X_train, X_test = X_letter.iloc[train_idx], X_letter.iloc[test_idx]\n",
    "        y_train, y_test = y_letter.iloc[train_idx], y_letter.iloc[test_idx]\n",
    "\n",
    "        X_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "        loader = DataLoader(TensorDataset(X_tensor, y_tensor), batch_size=128, shuffle=True)\n",
    "\n",
    "        generator = LetterGenerator().to(device)\n",
    "        critic = LetterCritic().to(device)\n",
    "        opt_G = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "        opt_C = torch.optim.Adam(critic.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "        for epoch in range(100):\n",
    "            for i, (real_samples, labels) in enumerate(loader):\n",
    "                real_samples, labels = real_samples.to(device), labels.to(device)\n",
    "                opt_C.zero_grad()\n",
    "                z = torch.randn(real_samples.size(0), 32).to(device)\n",
    "                fake_samples = generator(z, labels)\n",
    "                real_validity = critic(real_samples, labels)\n",
    "                fake_validity = critic(fake_samples.detach(), labels)\n",
    "                gp = compute_gp(critic, real_samples, fake_samples, labels, device)\n",
    "                c_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + 10 * gp\n",
    "                c_loss.backward()\n",
    "                opt_C.step()\n",
    "                if i % 5 == 0:\n",
    "                    opt_G.zero_grad()\n",
    "                    gen_samples = generator(z, labels)\n",
    "                    g_loss = -torch.mean(critic(gen_samples, labels))\n",
    "                    g_loss.backward()\n",
    "                    opt_G.step()\n",
    "\n",
    "        # Generate and evaluate synthetic data\n",
    "        synth_size = len(X_train) // 2\n",
    "        z = torch.randn(synth_size, 32).to(device)\n",
    "        synth_labels = torch.randint(0, num_classes_letter, (synth_size,), dtype=torch.long).to(device)\n",
    "        gen_data = generator(z, synth_labels).detach().cpu().numpy()\n",
    "        synth_df = pd.DataFrame(gen_data, columns=X_letter.columns)\n",
    "        synth_df[\"class\"] = synth_labels.cpu().numpy()\n",
    "        for col in synth_df.columns:\n",
    "            synth_df[col] = synth_df[col].round().astype(int)\n",
    "            synth_df[col] = synth_df[col].clip(0, letter_df[col].max())\n",
    "\n",
    "        print(\"\\n📊 TSTR Accuracy:\")\n",
    "        models = {\n",
    "            \"LogReg\": LogisticRegression(max_iter=300),\n",
    "            \"MLP\": MLPClassifier(max_iter=300),\n",
    "            \"RF\": RandomForestClassifier(),\n",
    "            \"XGBT\": XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "        }\n",
    "        for name, model in models.items():\n",
    "            model.fit(synth_df.drop(columns=[\"class\"]), synth_df[\"class\"])\n",
    "            y_pred = model.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            all_results_letter[f\"{name}_acc\"].append(acc)\n",
    "            print(f\"{name}: {acc:.4f}\")\n",
    "\n",
    "        jsd, wd = evaluate_jsd_wd(X_train, synth_df.drop(columns=[\"class\"]))\n",
    "        all_results_letter[\"jsd\"].append(jsd)\n",
    "        all_results_letter[\"wd\"].append(wd)\n",
    "        print(f\"\\n🔬 JSD: {jsd:.4f} | WD: {wd:.4f}\")\n",
    "\n",
    "# ===== Final Report + Save Dataset =====\n",
    "print(\"\\n📈 FINAL AVERAGE RESULTS ACROSS 3x2 CV:\")\n",
    "for name in [\"LogReg\", \"MLP\", \"RF\", \"XGBT\"]:\n",
    "    print(f\"{name} TSTR Accuracy: {np.mean(all_results_letter[f'{name}_acc']):.4f}\")\n",
    "print(f\"\\nJSD: {np.mean(all_results_letter['jsd']):.4f}\")\n",
    "print(f\"Wasserstein Distance: {np.mean(all_results_letter['wd']):.4f}\")\n",
    "\n",
    "print(\"\\n🚀 Training on full dataset for final synthetic sample...\")\n",
    "X_tensor_full = torch.tensor(X_letter.values, dtype=torch.float32)\n",
    "y_tensor_full = torch.tensor(y_letter.values, dtype=torch.long)\n",
    "loader_full = DataLoader(TensorDataset(X_tensor_full, y_tensor_full), batch_size=128, shuffle=True)\n",
    "\n",
    "generator = LetterGenerator().to(device)\n",
    "critic = LetterCritic().to(device)\n",
    "opt_G = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "opt_C = torch.optim.Adam(critic.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "for epoch in range(100):\n",
    "    for i, (real_samples, labels) in enumerate(loader_full):\n",
    "        real_samples, labels = real_samples.to(device), labels.to(device)\n",
    "        opt_C.zero_grad()\n",
    "        z = torch.randn(real_samples.size(0), 32).to(device)\n",
    "        fake_samples = generator(z, labels)\n",
    "        real_validity = critic(real_samples, labels)\n",
    "        fake_validity = critic(fake_samples.detach(), labels)\n",
    "        gp = compute_gp(critic, real_samples, fake_samples, labels, device)\n",
    "        c_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + 10 * gp\n",
    "        c_loss.backward()\n",
    "        opt_C.step()\n",
    "        if i % 5 == 0:\n",
    "            opt_G.zero_grad()\n",
    "            gen_samples = generator(z, labels)\n",
    "            g_loss = -torch.mean(critic(gen_samples, labels))\n",
    "            g_loss.backward()\n",
    "            opt_G.step()\n",
    "\n",
    "print(\"\\n💾 Generating and saving final dataset\")\n",
    "synth_size_final = len(X_letter) // 2\n",
    "z = torch.randn(synth_size_final, 32).to(device)\n",
    "synth_labels_final = torch.randint(0, num_classes_letter, (synth_size_final,), dtype=torch.long).to(device)\n",
    "gen_data_final = generator(z, synth_labels_final).detach().cpu().numpy()\n",
    "\n",
    "synth_df_final = pd.DataFrame(gen_data_final, columns=X_letter.columns)\n",
    "synth_df_final[\"class\"] = synth_labels_final.cpu().numpy()\n",
    "for col in synth_df_final.columns:\n",
    "    synth_df_final[col] = synth_df_final[col].round().astype(int)\n",
    "    synth_df_final[col] = synth_df_final[col].clip(0, letter_df[col].max())\n",
    "\n",
    "synth_df_final.to_csv(\"synthetic_letter_dataset_half.csv\", index=False)\n",
    "print(\"✅ Saved synthetic letter dataset as 'synthetic_letter_dataset_half.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a5e3429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Repeat 1/3\n",
      "\n",
      "🔄 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.7652\n",
      "MLP: 0.7765\n",
      "RF: 0.7774\n",
      "XGBT: 0.7643\n",
      "\n",
      "🔬 JSD: 0.1400 | WD: 0.9349\n",
      "\n",
      "🔄 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.7182\n",
      "MLP: 0.7059\n",
      "RF: 0.7252\n",
      "XGBT: 0.6474\n",
      "\n",
      "🔬 JSD: 0.1436 | WD: 1.0236\n",
      "\n",
      "🔁 Repeat 2/3\n",
      "\n",
      "🔄 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.6544\n",
      "MLP: 0.5350\n",
      "RF: 0.6144\n",
      "XGBT: 0.5811\n",
      "\n",
      "🔬 JSD: 0.1339 | WD: 1.0025\n",
      "\n",
      "🔄 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.7621\n",
      "MLP: 0.7423\n",
      "RF: 0.7682\n",
      "XGBT: 0.7460\n",
      "\n",
      "🔬 JSD: 0.1324 | WD: 0.9667\n",
      "\n",
      "🔁 Repeat 3/3\n",
      "\n",
      "🔄 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.7498\n",
      "MLP: 0.7283\n",
      "RF: 0.7666\n",
      "XGBT: 0.7282\n",
      "\n",
      "🔬 JSD: 0.1395 | WD: 0.9409\n",
      "\n",
      "🔄 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.7385\n",
      "MLP: 0.7408\n",
      "RF: 0.7334\n",
      "XGBT: 0.6936\n",
      "\n",
      "🔬 JSD: 0.1402 | WD: 0.8349\n",
      "\n",
      "📈 FINAL AVERAGE RESULTS ACROSS 3x2 CV:\n",
      "LogReg TSTR Accuracy: 0.7314\n",
      "MLP TSTR Accuracy: 0.7048\n",
      "RF TSTR Accuracy: 0.7309\n",
      "XGBT TSTR Accuracy: 0.6934\n",
      "\n",
      "JSD: 0.1382\n",
      "Wasserstein Distance: 0.9506\n",
      "\n",
      "🚀 Training generator on full CHESS dataset...\n",
      "\n",
      "💾 Generating final synthetic dataset (50% size of original)\n",
      "✅ Saved synthetic chess dataset as 'synthetic_chess_dataset_half.csv'\n"
     ]
    }
   ],
   "source": [
    "# ===== Encode and Prepare CHESS Dataset =====\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "chess_df = df.copy()\n",
    "encoders = {}\n",
    "for col in chess_df.columns:\n",
    "    le = LabelEncoder()\n",
    "    chess_df[col] = le.fit_transform(chess_df[col])\n",
    "    encoders[col] = le\n",
    "\n",
    "X_chess = chess_df.drop(columns=[\"class\"])\n",
    "y_chess = chess_df[\"class\"]\n",
    "input_dim_chess = X_chess.shape[1]\n",
    "num_classes_chess = y_chess.nunique()\n",
    "\n",
    "# ===== Generator for CHESS =====\n",
    "class ChessGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes_chess, num_classes_chess)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(32 + num_classes_chess, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim_chess)\n",
    "        )\n",
    "    def forward(self, z, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat((z, c), dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "# ===== Critic for CHESS =====\n",
    "class ChessCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes_chess, num_classes_chess)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim_chess + num_classes_chess, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        d_in = torch.cat((x, c), dim=1)\n",
    "        return self.model(d_in)\n",
    "\n",
    "# ===== 3x2 Stratified Evaluation =====\n",
    "skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "all_results_chess = defaultdict(list)\n",
    "\n",
    "for repeat in range(3):\n",
    "    print(f\"\\n🔁 Repeat {repeat+1}/3\")\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X_chess, y_chess)):\n",
    "        print(f\"\\n🔄 Fold {fold+1}/2\")\n",
    "        X_train, X_test = X_chess.iloc[train_idx], X_chess.iloc[test_idx]\n",
    "        y_train, y_test = y_chess.iloc[train_idx], y_chess.iloc[test_idx]\n",
    "\n",
    "        X_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "        loader = DataLoader(TensorDataset(X_tensor, y_tensor), batch_size=128, shuffle=True)\n",
    "\n",
    "        generator = ChessGenerator().to(device)\n",
    "        critic = ChessCritic().to(device)\n",
    "        opt_G = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "        opt_C = torch.optim.Adam(critic.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "        for epoch in range(100):\n",
    "            for i, (real_samples, labels) in enumerate(loader):\n",
    "                real_samples, labels = real_samples.to(device), labels.to(device)\n",
    "                opt_C.zero_grad()\n",
    "                z = torch.randn(real_samples.size(0), 32).to(device)\n",
    "                fake_samples = generator(z, labels)\n",
    "                real_validity = critic(real_samples, labels)\n",
    "                fake_validity = critic(fake_samples.detach(), labels)\n",
    "                gp = compute_gp(critic, real_samples, fake_samples, labels, device)\n",
    "                c_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + 10 * gp\n",
    "                c_loss.backward()\n",
    "                opt_C.step()\n",
    "\n",
    "                if i % 5 == 0:\n",
    "                    opt_G.zero_grad()\n",
    "                    gen_samples = generator(z, labels)\n",
    "                    g_loss = -torch.mean(critic(gen_samples, labels))\n",
    "                    g_loss.backward()\n",
    "                    opt_G.step()\n",
    "\n",
    "        # ===== Generate Synthetic Data =====\n",
    "        synth_size = len(X_train) // 2\n",
    "        z = torch.randn(synth_size, 32).to(device)\n",
    "        synth_labels = torch.randint(0, num_classes_chess, (synth_size,), dtype=torch.long).to(device)\n",
    "        gen_data = generator(z, synth_labels).detach().cpu().numpy()\n",
    "\n",
    "        synth_df = pd.DataFrame(gen_data, columns=X_chess.columns)\n",
    "        synth_df[\"class\"] = synth_labels.cpu().numpy()\n",
    "\n",
    "        for col in synth_df.columns:\n",
    "            synth_df[col] = synth_df[col].round().astype(int)\n",
    "            synth_df[col] = synth_df[col].clip(0, chess_df[col].max())\n",
    "\n",
    "        # ===== TSTR Evaluation =====\n",
    "        print(\"\\n📊 TSTR Accuracy:\")\n",
    "        models = {\n",
    "            \"LogReg\": LogisticRegression(max_iter=300),\n",
    "            \"MLP\": MLPClassifier(max_iter=300),\n",
    "            \"RF\": RandomForestClassifier(),\n",
    "            \"XGBT\": XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "        }\n",
    "        for name, model in models.items():\n",
    "            model.fit(synth_df.drop(columns=[\"class\"]), synth_df[\"class\"])\n",
    "            y_pred = model.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            all_results_chess[f\"{name}_acc\"].append(acc)\n",
    "            print(f\"{name}: {acc:.4f}\")\n",
    "\n",
    "        jsd, wd = evaluate_jsd_wd(X_train, synth_df.drop(columns=[\"class\"]))\n",
    "        all_results_chess[\"jsd\"].append(jsd)\n",
    "        all_results_chess[\"wd\"].append(wd)\n",
    "        print(f\"\\n🔬 JSD: {jsd:.4f} | WD: {wd:.4f}\")\n",
    "\n",
    "# ===== Final Evaluation =====\n",
    "print(\"\\n📈 FINAL AVERAGE RESULTS ACROSS 3x2 CV:\")\n",
    "for name in [\"LogReg\", \"MLP\", \"RF\", \"XGBT\"]:\n",
    "    print(f\"{name} TSTR Accuracy: {np.mean(all_results_chess[f'{name}_acc']):.4f}\")\n",
    "print(f\"\\nJSD: {np.mean(all_results_chess['jsd']):.4f}\")\n",
    "print(f\"Wasserstein Distance: {np.mean(all_results_chess['wd']):.4f}\")\n",
    "\n",
    "# ===== Final Generator Training =====\n",
    "print(\"\\n🚀 Training generator on full CHESS dataset...\")\n",
    "X_tensor_full = torch.tensor(X_chess.values, dtype=torch.float32)\n",
    "y_tensor_full = torch.tensor(y_chess.values, dtype=torch.long)\n",
    "loader_full = DataLoader(TensorDataset(X_tensor_full, y_tensor_full), batch_size=128, shuffle=True)\n",
    "\n",
    "generator = ChessGenerator().to(device)\n",
    "critic = ChessCritic().to(device)\n",
    "opt_G = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "opt_C = torch.optim.Adam(critic.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "for epoch in range(100):\n",
    "    for i, (real_samples, labels) in enumerate(loader_full):\n",
    "        real_samples, labels = real_samples.to(device), labels.to(device)\n",
    "        opt_C.zero_grad()\n",
    "        z = torch.randn(real_samples.size(0), 32).to(device)\n",
    "        fake_samples = generator(z, labels)\n",
    "        real_validity = critic(real_samples, labels)\n",
    "        fake_validity = critic(fake_samples.detach(), labels)\n",
    "        gp = compute_gp(critic, real_samples, fake_samples, labels, device)\n",
    "        c_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + 10 * gp\n",
    "        c_loss.backward()\n",
    "        opt_C.step()\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            opt_G.zero_grad()\n",
    "            gen_samples = generator(z, labels)\n",
    "            g_loss = -torch.mean(critic(gen_samples, labels))\n",
    "            g_loss.backward()\n",
    "            opt_G.step()\n",
    "\n",
    "# ===== Save Final CHESS Dataset =====\n",
    "print(\"\\n💾 Generating final synthetic dataset (50% size of original)\")\n",
    "synth_size_final = len(X_chess) // 2\n",
    "z = torch.randn(synth_size_final, 32).to(device)\n",
    "synth_labels_final = torch.randint(0, num_classes_chess, (synth_size_final,), dtype=torch.long).to(device)\n",
    "gen_data_final = generator(z, synth_labels_final).detach().cpu().numpy()\n",
    "\n",
    "synth_df_final = pd.DataFrame(gen_data_final, columns=X_chess.columns)\n",
    "synth_df_final[\"class\"] = synth_labels_final.cpu().numpy()\n",
    "\n",
    "for col in synth_df_final.columns:\n",
    "    synth_df_final[col] = synth_df_final[col].round().astype(int)\n",
    "    synth_df_final[col] = synth_df_final[col].clip(0, chess_df[col].max())\n",
    "\n",
    "synth_df_final.to_csv(\"synthetic_chess_dataset_half.csv\", index=False)\n",
    "print(\"✅ Saved synthetic chess dataset as 'synthetic_chess_dataset_half.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d68250f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Repeat 1/3\n",
      "\n",
      "🔀 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.7568\n",
      "MLP: 0.7724\n",
      "RF: 0.7680\n",
      "XGBT: 0.7322\n",
      "\n",
      "🔬 JSD: 0.1257 | WD: 0.8123\n",
      "\n",
      "🔀 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.7127\n",
      "MLP: 0.6508\n",
      "RF: 0.7115\n",
      "XGBT: 0.6971\n",
      "\n",
      "🔬 JSD: 0.1179 | WD: 0.7782\n",
      "\n",
      "🔁 Repeat 2/3\n",
      "\n",
      "🔀 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.7662\n",
      "MLP: 0.7755\n",
      "RF: 0.7727\n",
      "XGBT: 0.7390\n",
      "\n",
      "🔬 JSD: 0.1330 | WD: 0.8792\n",
      "\n",
      "🔀 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.7644\n",
      "MLP: 0.7493\n",
      "RF: 0.7749\n",
      "XGBT: 0.7378\n",
      "\n",
      "🔬 JSD: 0.1393 | WD: 1.1428\n",
      "\n",
      "🔁 Repeat 3/3\n",
      "\n",
      "🔀 Fold 1/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.7057\n",
      "MLP: 0.7046\n",
      "RF: 0.7218\n",
      "XGBT: 0.6736\n",
      "\n",
      "🔬 JSD: 0.1360 | WD: 0.8897\n",
      "\n",
      "🔀 Fold 2/2\n",
      "\n",
      "📊 TSTR Accuracy:\n",
      "LogReg: 0.5085\n",
      "MLP: 0.4768\n",
      "RF: 0.4845\n",
      "XGBT: 0.4519\n",
      "\n",
      "🔬 JSD: 0.1324 | WD: 0.7952\n",
      "\n",
      "📈 FINAL AVERAGE RESULTS ACROSS 3x2 CV:\n",
      "LogReg TSTR Accuracy: 0.7024\n",
      "MLP TSTR Accuracy: 0.6882\n",
      "RF TSTR Accuracy: 0.7056\n",
      "XGBT TSTR Accuracy: 0.6719\n",
      "\n",
      "JSD: 0.1307\n",
      "Wasserstein Distance: 0.8829\n",
      "\n",
      "🚀 Training final generator on full nursery dataset...\n",
      "\n",
      "📏 Generating final synthetic nursery dataset...\n",
      "✅ Saved synthetic nursery dataset as 'synthetic_nursery_dataset_half.csv'\n"
     ]
    }
   ],
   "source": [
    "# ===== Encode and Prepare NURSERY Dataset =====\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "nursery_df = df.copy()\n",
    "nursery_encoders = {}\n",
    "for col in nursery_df.columns:\n",
    "    le = LabelEncoder()\n",
    "    nursery_df[col] = le.fit_transform(nursery_df[col])\n",
    "    nursery_encoders[col] = le\n",
    "\n",
    "X_nursery = nursery_df.drop(columns=[\"class\"])\n",
    "y_nursery = nursery_df[\"class\"]\n",
    "input_dim_nursery = X_nursery.shape[1]\n",
    "num_classes_nursery = y_nursery.nunique()\n",
    "\n",
    "# ===== Generator for NURSERY =====\n",
    "class NurseryGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes_nursery, num_classes_nursery)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(32 + num_classes_nursery, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim_nursery)\n",
    "        )\n",
    "    def forward(self, z, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat((z, c), dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "# ===== Critic for NURSERY =====\n",
    "class NurseryCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes_nursery, num_classes_nursery)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim_nursery + num_classes_nursery, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        d_in = torch.cat((x, c), dim=1)\n",
    "        return self.model(d_in)\n",
    "\n",
    "# ===== 3x2 Stratified Evaluation =====\n",
    "skf_nursery = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "all_results_nursery = defaultdict(list)\n",
    "\n",
    "for repeat in range(3):\n",
    "    print(f\"\\n🔁 Repeat {repeat+1}/3\")\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf_nursery.split(X_nursery, y_nursery)):\n",
    "        print(f\"\\n🔀 Fold {fold+1}/2\")\n",
    "        X_train, X_test = X_nursery.iloc[train_idx], X_nursery.iloc[test_idx]\n",
    "        y_train, y_test = y_nursery.iloc[train_idx], y_nursery.iloc[test_idx]\n",
    "\n",
    "        X_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "        loader = DataLoader(TensorDataset(X_tensor, y_tensor), batch_size=128, shuffle=True)\n",
    "\n",
    "        generator = NurseryGenerator().to(device)\n",
    "        critic = NurseryCritic().to(device)\n",
    "        opt_G = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "        opt_C = torch.optim.Adam(critic.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "        for epoch in range(100):\n",
    "            for i, (real_samples, labels) in enumerate(loader):\n",
    "                real_samples, labels = real_samples.to(device), labels.to(device)\n",
    "                opt_C.zero_grad()\n",
    "                z = torch.randn(real_samples.size(0), 32).to(device)\n",
    "                fake_samples = generator(z, labels)\n",
    "                real_validity = critic(real_samples, labels)\n",
    "                fake_validity = critic(fake_samples.detach(), labels)\n",
    "                gp = compute_gp(critic, real_samples, fake_samples, labels, device)\n",
    "                c_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + 10 * gp\n",
    "                c_loss.backward()\n",
    "                opt_C.step()\n",
    "\n",
    "                if i % 5 == 0:\n",
    "                    opt_G.zero_grad()\n",
    "                    gen_samples = generator(z, labels)\n",
    "                    g_loss = -torch.mean(critic(gen_samples, labels))\n",
    "                    g_loss.backward()\n",
    "                    opt_G.step()\n",
    "\n",
    "        synth_size = len(X_train) // 2\n",
    "        z = torch.randn(synth_size, 32).to(device)\n",
    "        synth_labels = torch.randint(0, num_classes_nursery, (synth_size,), dtype=torch.long).to(device)\n",
    "        gen_data = generator(z, synth_labels).detach().cpu().numpy()\n",
    "\n",
    "        synth_df = pd.DataFrame(gen_data, columns=X_nursery.columns)\n",
    "        synth_df[\"class\"] = synth_labels.cpu().numpy()\n",
    "\n",
    "        for col in synth_df.columns:\n",
    "            synth_df[col] = synth_df[col].round().astype(int)\n",
    "            synth_df[col] = synth_df[col].clip(0, nursery_df[col].max())\n",
    "\n",
    "        print(\"\\n📊 TSTR Accuracy:\")\n",
    "        models = {\n",
    "            \"LogReg\": LogisticRegression(max_iter=300),\n",
    "            \"MLP\": MLPClassifier(max_iter=300),\n",
    "            \"RF\": RandomForestClassifier(),\n",
    "            \"XGBT\": XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "        }\n",
    "        for name, model in models.items():\n",
    "            model.fit(synth_df.drop(columns=[\"class\"]), synth_df[\"class\"])\n",
    "            y_pred = model.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            all_results_nursery[f\"{name}_acc\"].append(acc)\n",
    "            print(f\"{name}: {acc:.4f}\")\n",
    "\n",
    "        jsd, wd = evaluate_jsd_wd(X_train, synth_df.drop(columns=[\"class\"]))\n",
    "        all_results_nursery[\"jsd\"].append(jsd)\n",
    "        all_results_nursery[\"wd\"].append(wd)\n",
    "        print(f\"\\n🔬 JSD: {jsd:.4f} | WD: {wd:.4f}\")\n",
    "\n",
    "print(\"\\n📈 FINAL AVERAGE RESULTS ACROSS 3x2 CV:\")\n",
    "for name in [\"LogReg\", \"MLP\", \"RF\", \"XGBT\"]:\n",
    "    print(f\"{name} TSTR Accuracy: {np.mean(all_results_nursery[f'{name}_acc']):.4f}\")\n",
    "print(f\"\\nJSD: {np.mean(all_results_nursery['jsd']):.4f}\")\n",
    "print(f\"Wasserstein Distance: {np.mean(all_results_nursery['wd']):.4f}\")\n",
    "\n",
    "print(\"\\n🚀 Training final generator on full nursery dataset...\")\n",
    "X_tensor_full = torch.tensor(X_nursery.values, dtype=torch.float32)\n",
    "y_tensor_full = torch.tensor(y_nursery.values, dtype=torch.long)\n",
    "loader_full = DataLoader(TensorDataset(X_tensor_full, y_tensor_full), batch_size=128, shuffle=True)\n",
    "\n",
    "generator = NurseryGenerator().to(device)\n",
    "critic = NurseryCritic().to(device)\n",
    "opt_G = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "opt_C = torch.optim.Adam(critic.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "for epoch in range(100):\n",
    "    for i, (real_samples, labels) in enumerate(loader_full):\n",
    "        real_samples, labels = real_samples.to(device), labels.to(device)\n",
    "        opt_C.zero_grad()\n",
    "        z = torch.randn(real_samples.size(0), 32).to(device)\n",
    "        fake_samples = generator(z, labels)\n",
    "        real_validity = critic(real_samples, labels)\n",
    "        fake_validity = critic(fake_samples.detach(), labels)\n",
    "        gp = compute_gp(critic, real_samples, fake_samples, labels, device)\n",
    "        c_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + 10 * gp\n",
    "        c_loss.backward()\n",
    "        opt_C.step()\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            opt_G.zero_grad()\n",
    "            gen_samples = generator(z, labels)\n",
    "            g_loss = -torch.mean(critic(gen_samples, labels))\n",
    "            g_loss.backward()\n",
    "            opt_G.step()\n",
    "\n",
    "print(\"\\n📏 Generating final synthetic nursery dataset...\")\n",
    "synth_size_final = len(X_nursery) // 2\n",
    "z = torch.randn(synth_size_final, 32).to(device)\n",
    "synth_labels_final = torch.randint(0, num_classes_nursery, (synth_size_final,), dtype=torch.long).to(device)\n",
    "gen_data_final = generator(z, synth_labels_final).detach().cpu().numpy()\n",
    "\n",
    "synth_df_final = pd.DataFrame(gen_data_final, columns=X_nursery.columns)\n",
    "synth_df_final[\"class\"] = synth_labels_final.cpu().numpy()\n",
    "\n",
    "for col in synth_df_final.columns:\n",
    "    synth_df_final[col] = synth_df_final[col].round().astype(int)\n",
    "    synth_df_final[col] = synth_df_final[col].clip(0, nursery_df[col].max())\n",
    "\n",
    "synth_df_final.to_csv(\"synthetic_nursery_dataset_half.csv\", index=False)\n",
    "print(\"✅ Saved synthetic nursery dataset as 'synthetic_nursery_dataset_half.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
